19:50:49	Running experiment with ID base_naive_22127_195049
19:50:49	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  y: Float64 0.9
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 4638

19:50:49	will explore for first 100 timesteps
19:50:49	will estimate based on feedback from teacher 3 with beta 50.0
19:50:49	generated 27 utilities (each length 3 items)
19:50:50	generated 216 arm distribution sets (each shape 3 arms x 3 items)
19:50:50	generated 1 beta value sets (each length 3 teachers)
19:50:50	generated 5832 states
19:50:50	generated 6 actions
19:50:50	generated reward function
19:50:50	generated 21 observations
19:50:50	generated observation function
19:50:50	true state State([10.0, 0.0, 10.0], Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]], [0.0, 0.01, 50.0])
19:50:50	logging naive policy simulation 1 to ./sims/base_naive_22127_195049_run1.txt
19:50:50	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.4444444444444444, 0.0, 0.5555555555555556], [0.4117647058823529, 0.5882352941176471, 0.0], [0.42857142857142855, 0.5714285714285714, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 4.285714285714286
19:50:51	logging naive policy simulation 2 to ./sims/base_naive_22127_195049_run2.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.4411764705882353, 0.0, 0.5588235294117647], [0.40625, 0.59375, 0.0], [0.5862068965517241, 0.41379310344827586, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.862068965517241
19:50:51	logging naive policy simulation 3 to ./sims/base_naive_22127_195049_run3.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.49056603773584906, 0.0, 0.5094339622641509], [0.4444444444444444, 0.5555555555555556, 0.0], [0.5116279069767442, 0.4883720930232558, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.116279069767442
19:50:51	logging naive policy simulation 4 to ./sims/base_naive_22127_195049_run4.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.4626865671641791, 0.0, 0.5373134328358209], [0.43283582089552236, 0.5671641791044776, 0.0], [0.4696969696969697, 0.5303030303030303, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 4.696969696969697
19:50:51	logging naive policy simulation 5 to ./sims/base_naive_22127_195049_run5.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.4823529411764706, 0.0, 0.5176470588235295], [0.46153846153846156, 0.5384615384615384, 0.0], [0.4819277108433735, 0.5180722891566265, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 4.8192771084337345
19:50:51	logging naive policy simulation 6 to ./sims/base_naive_22127_195049_run6.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.5, 0.0, 0.5], [0.46464646464646464, 0.5353535353535354, 0.0], [0.46236559139784944, 0.5376344086021505, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 2 with reward 4.646464646464646
19:50:51	logging naive policy simulation 7 to ./sims/base_naive_22127_195049_run7.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.49557522123893805, 0.0, 0.504424778761062], [0.4864864864864865, 0.5135135135135135, 0.0], [0.4636363636363636, 0.5363636363636364, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 2 with reward 4.864864864864865
19:50:51	logging naive policy simulation 8 to ./sims/base_naive_22127_195049_run8.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.49230769230769234, 0.0, 0.5076923076923077], [0.4881889763779528, 0.5118110236220472, 0.0], [0.47619047619047616, 0.5238095238095238, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 2 with reward 4.881889763779528
19:50:51	logging naive policy simulation 9 to ./sims/base_naive_22127_195049_run9.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.5138888888888888, 0.0, 0.4861111111111111], [0.4966887417218543, 0.5033112582781457, 0.0], [0.5034013605442177, 0.4965986394557823, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.034013605442177
19:50:51	logging naive policy simulation 10 to ./sims/base_naive_22127_195049_run10.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.5256410256410257, 0.0, 0.47435897435897434], [0.5, 0.5, 0.0], [0.5059523809523809, 0.49404761904761907, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.059523809523809
19:50:51	logging naive policy simulation 11 to ./sims/base_naive_22127_195049_run11.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.5144508670520231, 0.0, 0.48554913294797686], [0.5, 0.5, 0.0], [0.5300546448087432, 0.46994535519125685, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.300546448087432
19:50:51	logging naive policy simulation 12 to ./sims/base_naive_22127_195049_run12.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.5238095238095238, 0.0, 0.47619047619047616], [0.5167464114832536, 0.48325358851674644, 0.0], [0.5204081632653061, 0.47959183673469385, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.204081632653061
19:50:51	logging naive policy simulation 13 to ./sims/base_naive_22127_195049_run13.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.5165876777251185, 0.0, 0.4834123222748815], [0.5132743362831859, 0.48672566371681414, 0.0], [0.5233644859813084, 0.4766355140186916, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.233644859813084
19:50:51	logging naive policy simulation 14 to ./sims/base_naive_22127_195049_run14.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.5130434782608696, 0.0, 0.48695652173913045], [0.5084033613445378, 0.49159663865546216, 0.0], [0.5344827586206896, 0.46551724137931033, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.344827586206896
19:50:51	logging naive policy simulation 15 to ./sims/base_naive_22127_195049_run15.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.5141700404858299, 0.0, 0.48582995951417], [0.5099601593625498, 0.4900398406374502, 0.0], [0.5282258064516129, 0.4717741935483871, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.282258064516129
19:50:51	logging naive policy simulation 16 to ./sims/base_naive_22127_195049_run16.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.5076923076923077, 0.0, 0.49230769230769234], [0.5185185185185185, 0.48148148148148145, 0.0], [0.5315985130111525, 0.4684014869888476, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.315985130111525
19:50:51	logging naive policy simulation 17 to ./sims/base_naive_22127_195049_run17.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.5197132616487455, 0.0, 0.48028673835125446], [0.5344827586206896, 0.46551724137931033, 0.0], [0.527972027972028, 0.47202797202797203, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 2 with reward 5.344827586206896
19:50:51	logging naive policy simulation 18 to ./sims/base_naive_22127_195049_run18.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.5033783783783784, 0.0, 0.4966216216216216], [0.5224358974358975, 0.4775641025641026, 0.0], [0.5167785234899329, 0.48322147651006714, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 2 with reward 5.2243589743589745
19:50:51	logging naive policy simulation 19 to ./sims/base_naive_22127_195049_run19.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.5016181229773463, 0.0, 0.49838187702265374], [0.5209580838323353, 0.47904191616766467, 0.0], [0.5180327868852459, 0.4819672131147541, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 2 with reward 5.209580838323353
19:50:51	logging naive policy simulation 20 to ./sims/base_naive_22127_195049_run20.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.5045871559633027, 0.0, 0.4954128440366973], [0.5228571428571429, 0.47714285714285715, 0.0], [0.525, 0.475, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.25
19:50:51	logging naive policy simulation 21 to ./sims/base_naive_22127_195049_run21.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.49853372434017595, 0.0, 0.501466275659824], [0.5203252032520326, 0.4796747967479675, 0.0], [0.5295857988165681, 0.47041420118343197, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.295857988165681
19:50:51	logging naive policy simulation 22 to ./sims/base_naive_22127_195049_run22.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.49859943977591037, 0.0, 0.5014005602240896], [0.5230769230769231, 0.47692307692307695, 0.0], [0.5397727272727273, 0.4602272727272727, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.397727272727273
19:50:51	logging naive policy simulation 23 to ./sims/base_naive_22127_195049_run23.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.5108108108108108, 0.0, 0.4891891891891892], [0.5185185185185185, 0.48148148148148145, 0.0], [0.5444743935309974, 0.4555256064690027, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.444743935309973
19:50:51	logging naive policy simulation 24 to ./sims/base_naive_22127_195049_run24.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.5, 0.0, 0.5], [0.5105882352941177, 0.4894117647058824, 0.0], [0.5440414507772021, 0.45595854922279794, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.440414507772021
19:50:51	logging naive policy simulation 25 to ./sims/base_naive_22127_195049_run25.txt
19:50:51	estimating U using teacher 3 with beta 50.0
19:50:51	Estimated U: [10.0, -0.0, -10.0]
19:50:51	Estimated D: Any[[0.4975369458128079, 0.0, 0.5024630541871922], [0.5067873303167421, 0.49321266968325794, 0.0], [0.5365239294710328, 0.4634760705289673, 0.0]]
19:50:51	given U and D estimates, highest-reward arm is arm 3 with reward 5.365239294710328
19:50:51	ran 25 naive policy rollouts for 1000 timesteps each
19:50:51	Naive R: [4800.0, 4845.0, 4870.0, 4820.0, 4820.0, 4785.0, 4795.0, 4830.0, 4865.0, 4820.0, 4835.0, 4830.0, 4895.0, 4840.0, 4815.0, 4830.0, 4875.0, 4840.0, 4775.0, 4835.0, 4825.0, 4835.0, 4800.0, 4835.0, 4840.0]
