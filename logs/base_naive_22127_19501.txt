19:50:01	Running experiment with ID base_naive_22127_19501
19:50:01	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  y: Float64 0.9
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 4586

19:50:01	will explore for first 100 timesteps
19:50:01	will estimate based on feedback from teacher 3 with beta 50.0
19:50:02	generated 27 utilities (each length 3 items)
19:50:02	generated 216 arm distribution sets (each shape 3 arms x 3 items)
19:50:02	generated 1 beta value sets (each length 3 teachers)
19:50:02	generated 5832 states
19:50:02	generated 6 actions
19:50:02	generated reward function
19:50:02	generated 21 observations
19:50:02	generated observation function
19:50:02	true state State([5.0, 5.0, 10.0], Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]], [0.0, 0.01, 50.0])
19:50:02	logging naive policy simulation 1 to ./sims/base_naive_22127_19501_run1.txt
19:50:02	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.6666666666666666, 0.3333333333333333], [0.5333333333333333, 0.4666666666666667, 0.0], [0.4166666666666667, 0.5833333333333334, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 5.333333333333333
19:50:03	logging naive policy simulation 2 to ./sims/base_naive_22127_19501_run2.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.6666666666666666, 0.3333333333333333], [0.43333333333333335, 0.5666666666666667, 0.0], [0.4444444444444444, 0.5555555555555556, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 3 with reward 4.444444444444445
19:50:03	logging naive policy simulation 3 to ./sims/base_naive_22127_19501_run3.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.6904761904761905, 0.30952380952380953], [0.40816326530612246, 0.5918367346938775, 0.0], [0.47058823529411764, 0.5294117647058824, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 3 with reward 4.705882352941177
19:50:03	logging naive policy simulation 4 to ./sims/base_naive_22127_19501_run4.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.6666666666666666, 0.3333333333333333], [0.4696969696969697, 0.5303030303030303, 0.0], [0.47619047619047616, 0.5238095238095238, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 3 with reward 4.761904761904762
19:50:03	logging naive policy simulation 5 to ./sims/base_naive_22127_19501_run5.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.6419753086419753, 0.35802469135802467], [0.4375, 0.5625, 0.0], [0.4533333333333333, 0.5466666666666666, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 3 with reward 4.533333333333333
19:50:03	logging naive policy simulation 6 to ./sims/base_naive_22127_19501_run6.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.6326530612244898, 0.3673469387755102], [0.47058823529411764, 0.5294117647058824, 0.0], [0.47058823529411764, 0.5294117647058824, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 4.705882352941177
19:50:03	logging naive policy simulation 7 to ./sims/base_naive_22127_19501_run7.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.6206896551724138, 0.3793103448275862], [0.46218487394957986, 0.5378151260504201, 0.0], [0.4423076923076923, 0.5576923076923077, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 4.621848739495799
19:50:03	logging naive policy simulation 8 to ./sims/base_naive_22127_19501_run8.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.6212121212121212, 0.3787878787878788], [0.5074626865671642, 0.4925373134328358, 0.0], [0.45161290322580644, 0.5483870967741935, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 5.074626865671642
19:50:03	logging naive policy simulation 9 to ./sims/base_naive_22127_19501_run9.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.6232876712328768, 0.3767123287671233], [0.5032258064516129, 0.4967741935483871, 0.0], [0.4244604316546763, 0.5755395683453237, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 5.032258064516128
19:50:03	logging naive policy simulation 10 to ./sims/base_naive_22127_19501_run10.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.6060606060606061, 0.3939393939393939], [0.4970414201183432, 0.5029585798816568, 0.0], [0.4620253164556962, 0.5379746835443038, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 4.970414201183432
19:50:03	logging naive policy simulation 11 to ./sims/base_naive_22127_19501_run11.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.5977653631284916, 0.4022346368715084], [0.4973821989528796, 0.5026178010471204, 0.0], [0.46745562130177515, 0.5325443786982249, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 4.973821989528796
19:50:03	logging naive policy simulation 12 to ./sims/base_naive_22127_19501_run12.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.5721649484536082, 0.42783505154639173], [0.49523809523809526, 0.5047619047619047, 0.0], [0.4702702702702703, 0.5297297297297298, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 4.9523809523809526
19:50:03	logging naive policy simulation 13 to ./sims/base_naive_22127_19501_run13.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.569377990430622, 0.430622009569378], [0.4977578475336323, 0.5022421524663677, 0.0], [0.4504950495049505, 0.5495049504950495, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 4.977578475336323
19:50:03	logging naive policy simulation 14 to ./sims/base_naive_22127_19501_run14.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.5688073394495413, 0.43119266055045874], [0.4773662551440329, 0.522633744855967, 0.0], [0.44036697247706424, 0.5596330275229358, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 4.773662551440329
19:50:03	logging naive policy simulation 15 to ./sims/base_naive_22127_19501_run15.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.5584415584415584, 0.44155844155844154], [0.48091603053435117, 0.5190839694656488, 0.0], [0.45726495726495725, 0.5427350427350427, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 4.809160305343512
19:50:03	logging naive policy simulation 16 to ./sims/base_naive_22127_19501_run16.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.5503875968992248, 0.4496124031007752], [0.4946236559139785, 0.5053763440860215, 0.0], [0.4628099173553719, 0.5371900826446281, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 4.946236559139785
19:50:03	logging naive policy simulation 17 to ./sims/base_naive_22127_19501_run17.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.5370370370370371, 0.46296296296296297], [0.4913494809688581, 0.5086505190311419, 0.0], [0.47104247104247104, 0.528957528957529, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 4.913494809688581
19:50:03	logging naive policy simulation 18 to ./sims/base_naive_22127_19501_run18.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.5342465753424658, 0.4657534246575342], [0.4967532467532468, 0.5032467532467533, 0.0], [0.4645390070921986, 0.5354609929078015, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 4.967532467532468
19:50:03	logging naive policy simulation 19 to ./sims/base_naive_22127_19501_run19.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.5364238410596026, 0.46357615894039733], [0.5015197568389058, 0.49848024316109424, 0.0], [0.47315436241610737, 0.5268456375838926, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 5.015197568389058
19:50:03	logging naive policy simulation 20 to ./sims/base_naive_22127_19501_run20.txt
19:50:03	estimating U using teacher 3 with beta 50.0
19:50:03	Estimated U: [10.0, -0.0, -10.0]
19:50:03	Estimated D: Any[[0.0, 0.5356037151702786, 0.46439628482972134], [0.5028901734104047, 0.49710982658959535, 0.0], [0.4810126582278481, 0.5189873417721519, 0.0]]
19:50:03	given U and D estimates, highest-reward arm is arm 2 with reward 5.028901734104046
19:50:04	logging naive policy simulation 21 to ./sims/base_naive_22127_19501_run21.txt
19:50:04	estimating U using teacher 3 with beta 50.0
19:50:04	Estimated U: [10.0, -0.0, -10.0]
19:50:04	Estimated D: Any[[0.0, 0.5406976744186046, 0.45930232558139533], [0.5013927576601671, 0.4986072423398329, 0.0], [0.4894894894894895, 0.5105105105105106, 0.0]]
19:50:04	given U and D estimates, highest-reward arm is arm 2 with reward 5.013927576601671
19:50:04	logging naive policy simulation 22 to ./sims/base_naive_22127_19501_run22.txt
19:50:04	estimating U using teacher 3 with beta 50.0
19:50:04	Estimated U: [10.0, -0.0, -10.0]
19:50:04	Estimated D: Any[[0.0, 0.5363128491620112, 0.46368715083798884], [0.5026455026455027, 0.4973544973544973, 0.0], [0.4813753581661891, 0.5186246418338109, 0.0]]
19:50:04	given U and D estimates, highest-reward arm is arm 2 with reward 5.026455026455027
19:50:04	logging naive policy simulation 23 to ./sims/base_naive_22127_19501_run23.txt
19:50:04	estimating U using teacher 3 with beta 50.0
19:50:04	Estimated U: [10.0, -0.0, -10.0]
19:50:04	Estimated D: Any[[0.0, 0.5319148936170213, 0.46808510638297873], [0.4975124378109453, 0.5024875621890548, 0.0], [0.475, 0.525, 0.0]]
19:50:04	given U and D estimates, highest-reward arm is arm 2 with reward 4.975124378109453
19:50:04	logging naive policy simulation 24 to ./sims/base_naive_22127_19501_run24.txt
19:50:04	estimating U using teacher 3 with beta 50.0
19:50:04	Estimated U: [10.0, -0.0, -10.0]
19:50:04	Estimated D: Any[[0.0, 0.5279187817258884, 0.4720812182741117], [0.4951219512195122, 0.5048780487804878, 0.0], [0.46965699208443273, 0.5303430079155673, 0.0]]
19:50:04	given U and D estimates, highest-reward arm is arm 2 with reward 4.951219512195122
19:50:04	logging naive policy simulation 25 to ./sims/base_naive_22127_19501_run25.txt
19:50:04	estimating U using teacher 3 with beta 50.0
19:50:04	Estimated U: [10.0, -0.0, -10.0]
19:50:04	Estimated D: Any[[0.0, 0.5265700483091788, 0.47342995169082125], [0.49414519906323184, 0.5058548009367682, 0.0], [0.4681933842239186, 0.5318066157760815, 0.0]]
19:50:04	given U and D estimates, highest-reward arm is arm 2 with reward 4.941451990632318
19:50:04	ran 25 naive policy rollouts for 1000 timesteps each
19:50:04	Naive R: [4770.0, 4785.0, 4760.0, 4780.0, 4787.5, 4787.5, 4815.0, 4795.0, 4785.0, 4807.5, 4770.0, 4787.5, 4762.5, 4747.5, 4772.5, 4827.5, 4725.0, 4875.0, 4760.0, 4832.5, 4807.5, 4780.0, 4810.0, 4770.0, 4805.0]
