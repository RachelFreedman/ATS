19:49:29	Running experiment with ID base_naive_22127_194929
19:49:29	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  y: Float64 0.9
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 1235

19:49:29	will explore for first 100 timesteps
19:49:29	will estimate based on feedback from teacher 3 with beta 50.0
19:49:30	generated 27 utilities (each length 3 items)
19:49:30	generated 216 arm distribution sets (each shape 3 arms x 3 items)
19:49:30	generated 1 beta value sets (each length 3 teachers)
19:49:30	generated 5832 states
19:49:30	generated 6 actions
19:49:30	generated reward function
19:49:30	generated 21 observations
19:49:30	generated observation function
19:49:30	true state State([5.0, 0.0, 10.0], Array{Float64}[[0.5, 0.0, 0.5], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]], [0.0, 0.01, 50.0])
19:49:30	logging naive policy simulation 1 to ./sims/base_naive_22127_194929_run1.txt
19:49:30	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.6086956521739131, 0.0, 0.391304347826087], [0.0, 0.6666666666666666, 0.3333333333333333], [0.0, 0.6666666666666666, 0.3333333333333333]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward 2.173913043478261
19:49:31	logging naive policy simulation 2 to ./sims/base_naive_22127_194929_run2.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.5135135135135135, 0.0, 0.4864864864864865], [0.0, 0.6, 0.4], [0.0, 0.56, 0.44]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward 0.27027027027026995
19:49:31	logging naive policy simulation 3 to ./sims/base_naive_22127_194929_run3.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.4897959183673469, 0.0, 0.5102040816326531], [0.0, 0.6078431372549019, 0.39215686274509803], [0.0, 0.4909090909090909, 0.509090909090909]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.20408163265306123
19:49:31	logging naive policy simulation 4 to ./sims/base_naive_22127_194929_run4.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.48333333333333334, 0.0, 0.5166666666666667], [0.0, 0.5074626865671642, 0.4925373134328358], [0.0, 0.43243243243243246, 0.5675675675675675]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.33333333333333415
19:49:31	logging naive policy simulation 5 to ./sims/base_naive_22127_194929_run5.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.5064935064935064, 0.0, 0.4935064935064935], [0.0, 0.5232558139534884, 0.47674418604651164], [0.0, 0.4222222222222222, 0.5777777777777777]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward 0.12987012987012958
19:49:31	logging naive policy simulation 6 to ./sims/base_naive_22127_194929_run6.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.5053763440860215, 0.0, 0.4946236559139785], [0.0, 0.5233644859813084, 0.4766355140186916], [0.0, 0.4380952380952381, 0.5619047619047619]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward 0.10752688172043001
19:49:31	logging naive policy simulation 7 to ./sims/base_naive_22127_194929_run7.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.4954954954954955, 0.0, 0.5045045045045045], [0.0, 0.512396694214876, 0.48760330578512395], [0.0, 0.432, 0.568]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.09009009009008961
19:49:31	logging naive policy simulation 8 to ./sims/base_naive_22127_194929_run8.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.488, 0.0, 0.512], [0.0, 0.524822695035461, 0.475177304964539], [0.0, 0.4676258992805755, 0.5323741007194245]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.2400000000000002
19:49:31	logging naive policy simulation 9 to ./sims/base_naive_22127_194929_run9.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.45517241379310347, 0.0, 0.5448275862068965], [0.0, 0.5298013245033113, 0.47019867549668876], [0.0, 0.45751633986928103, 0.5424836601307189]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.8965517241379306
19:49:31	logging naive policy simulation 10 to ./sims/base_naive_22127_194929_run10.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.4666666666666667, 0.0, 0.5333333333333333], [0.0, 0.5297619047619048, 0.47023809523809523], [0.0, 0.4578313253012048, 0.5421686746987951]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.6666666666666663
19:49:31	logging naive policy simulation 11 to ./sims/base_naive_22127_194929_run11.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.4602272727272727, 0.0, 0.5397727272727273], [0.0, 0.5469613259668509, 0.4530386740331492], [0.0, 0.47282608695652173, 0.5271739130434783]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.7954545454545463
19:49:31	logging naive policy simulation 12 to ./sims/base_naive_22127_194929_run12.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.46875, 0.0, 0.53125], [0.0, 0.555, 0.445], [0.0, 0.48258706467661694, 0.5174129353233831]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.625
19:49:31	logging naive policy simulation 13 to ./sims/base_naive_22127_194929_run13.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.46153846153846156, 0.0, 0.5384615384615384], [0.0, 0.5504587155963303, 0.44954128440366975], [0.0, 0.481981981981982, 0.5180180180180181]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.7692307692307685
19:49:31	logging naive policy simulation 14 to ./sims/base_naive_22127_194929_run14.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.47161572052401746, 0.0, 0.5283842794759825], [0.0, 0.5526315789473685, 0.4473684210526316], [0.0, 0.4791666666666667, 0.5208333333333334]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.5676855895196513
19:49:31	logging naive policy simulation 15 to ./sims/base_naive_22127_194929_run15.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.47346938775510206, 0.0, 0.5265306122448979], [0.0, 0.5481171548117155, 0.45188284518828453], [0.0, 0.4827586206896552, 0.5172413793103449]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.5306122448979591
19:49:31	logging naive policy simulation 16 to ./sims/base_naive_22127_194929_run16.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.47692307692307695, 0.0, 0.5230769230769231], [0.0, 0.5341365461847389, 0.46586345381526106], [0.0, 0.4712230215827338, 0.5287769784172662]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.4615384615384619
19:49:31	logging naive policy simulation 17 to ./sims/base_naive_22127_194929_run17.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.48, 0.0, 0.52], [0.0, 0.5267175572519084, 0.4732824427480916], [0.0, 0.48135593220338985, 0.5186440677966102]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.40000000000000036
19:49:31	logging naive policy simulation 18 to ./sims/base_naive_22127_194929_run18.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.47038327526132406, 0.0, 0.5296167247386759], [0.0, 0.525, 0.475], [0.0, 0.48220064724919093, 0.517799352750809]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.592334494773519
19:49:31	logging naive policy simulation 19 to ./sims/base_naive_22127_194929_run19.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.4758842443729904, 0.0, 0.5241157556270096], [0.0, 0.5272108843537415, 0.47278911564625853], [0.0, 0.4813664596273292, 0.5186335403726708]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.4823151125401923
19:49:31	logging naive policy simulation 20 to ./sims/base_naive_22127_194929_run20.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.4695121951219512, 0.0, 0.5304878048780488], [0.0, 0.5172413793103449, 0.4827586206896552], [0.0, 0.4746268656716418, 0.5253731343283582]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.6097560975609757
19:49:31	logging naive policy simulation 21 to ./sims/base_naive_22127_194929_run21.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.4595375722543353, 0.0, 0.5404624277456648], [0.0, 0.5103857566765578, 0.4896142433234421], [0.0, 0.4699140401146132, 0.5300859598853869]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.8092485549132948
19:49:31	logging naive policy simulation 22 to ./sims/base_naive_22127_194929_run22.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.45454545454545453, 0.0, 0.5454545454545454], [0.0, 0.5056179775280899, 0.4943820224719101], [0.0, 0.4644808743169399, 0.5355191256830601]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.9090909090909092
19:49:31	logging naive policy simulation 23 to ./sims/base_naive_22127_194929_run23.txt
19:49:31	estimating U using teacher 3 with beta 50.0
19:49:31	Estimated U: [10.0, -0.0, -10.0]
19:49:31	Estimated D: Any[[0.46174142480211083, 0.0, 0.5382585751978892], [0.0, 0.514745308310992, 0.48525469168900803], [0.0, 0.4649350649350649, 0.535064935064935]]
19:49:31	given U and D estimates, highest-reward arm is arm 1 with reward -0.7651715039577835
19:49:32	logging naive policy simulation 24 to ./sims/base_naive_22127_194929_run24.txt
19:49:32	estimating U using teacher 3 with beta 50.0
19:49:32	Estimated U: [10.0, -0.0, -10.0]
19:49:32	Estimated D: Any[[0.4578005115089514, 0.0, 0.5421994884910486], [0.0, 0.5180412371134021, 0.48195876288659795], [0.0, 0.46851385390428213, 0.5314861460957179]]
19:49:32	given U and D estimates, highest-reward arm is arm 1 with reward -0.843989769820972
19:49:32	logging naive policy simulation 25 to ./sims/base_naive_22127_194929_run25.txt
19:49:32	estimating U using teacher 3 with beta 50.0
19:49:32	Estimated U: [10.0, -0.0, -10.0]
19:49:32	Estimated D: Any[[0.4651741293532338, 0.0, 0.5348258706467661], [0.0, 0.525, 0.475], [0.0, 0.47255369928400953, 0.5274463007159904]]
19:49:32	given U and D estimates, highest-reward arm is arm 1 with reward -0.6965174129353229
19:49:32	ran 25 naive policy rollouts for 1000 timesteps each
19:49:32	Naive R: [7057.5, 7020.0, 7070.0, 7007.5, 7052.5, 7050.0, 7055.0, 7025.0, 7020.0, 7050.0, 6987.5, 7050.0, 7065.0, 7047.5, 7030.0, 6997.5, 7012.5, 7000.0, 7065.0, 7067.5, 7045.0, 7057.5, 7050.0, 6975.0, 7002.5]
