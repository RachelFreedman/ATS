19:53:11	Running experiment with ID base_naive_22127_195311
19:53:12	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  y: Float64 0.9
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 3526

19:53:12	will explore for first 100 timesteps
19:53:12	will estimate based on feedback from teacher 3 with beta 50.0
19:53:12	generated 27 utilities (each length 3 items)
19:53:12	generated 216 arm distribution sets (each shape 3 arms x 3 items)
19:53:12	generated 1 beta value sets (each length 3 teachers)
19:53:12	generated 5832 states
19:53:12	generated 6 actions
19:53:12	generated reward function
19:53:12	generated 21 observations
19:53:12	generated observation function
19:53:12	true state State([0.0, 10.0, 5.0], Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.5, 0.0, 0.5]], [0.0, 0.01, 50.0])
19:53:12	logging naive policy simulation 1 to ./sims/base_naive_22127_195311_run1.txt
19:53:12	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.52, 0.48, 0.0], [0.42857142857142855, 0.0, 0.5714285714285714], [0.5833333333333334, 0.0, 0.4166666666666667]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 5.2
19:53:13	logging naive policy simulation 2 to ./sims/base_naive_22127_195311_run2.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.4888888888888889, 0.5111111111111111, 0.0], [0.4782608695652174, 0.0, 0.5217391304347826], [0.5, 0.0, 0.5]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.888888888888888
19:53:13	logging naive policy simulation 3 to ./sims/base_naive_22127_195311_run3.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.5, 0.5, 0.0], [0.48717948717948717, 0.0, 0.5128205128205128], [0.5897435897435898, 0.0, 0.41025641025641024]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 5.0
19:53:13	logging naive policy simulation 4 to ./sims/base_naive_22127_195311_run4.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.4878048780487805, 0.5121951219512195, 0.0], [0.4426229508196721, 0.0, 0.5573770491803278], [0.5625, 0.0, 0.4375]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.878048780487805
19:53:13	logging naive policy simulation 5 to ./sims/base_naive_22127_195311_run5.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.46938775510204084, 0.5306122448979592, 0.0], [0.4342105263157895, 0.0, 0.5657894736842105], [0.6065573770491803, 0.0, 0.39344262295081966]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.6938775510204085
19:53:13	logging naive policy simulation 6 to ./sims/base_naive_22127_195311_run6.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.44036697247706424, 0.5596330275229358, 0.0], [0.4329896907216495, 0.0, 0.5670103092783505], [0.5641025641025641, 0.0, 0.4358974358974359]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.4036697247706424
19:53:13	logging naive policy simulation 7 to ./sims/base_naive_22127_195311_run7.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.4566929133858268, 0.5433070866141733, 0.0], [0.45871559633027525, 0.0, 0.5412844036697247], [0.5670103092783505, 0.0, 0.4329896907216495]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.566929133858268
19:53:13	logging naive policy simulation 8 to ./sims/base_naive_22127_195311_run8.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.4594594594594595, 0.5405405405405406, 0.0], [0.4435483870967742, 0.0, 0.5564516129032258], [0.5470085470085471, 0.0, 0.452991452991453]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.594594594594595
19:53:13	logging naive policy simulation 9 to ./sims/base_naive_22127_195311_run9.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.47619047619047616, 0.5238095238095238, 0.0], [0.45323741007194246, 0.0, 0.5467625899280576], [0.5642857142857143, 0.0, 0.4357142857142857]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.761904761904762
19:53:13	logging naive policy simulation 10 to ./sims/base_naive_22127_195311_run10.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.47643979057591623, 0.5235602094240838, 0.0], [0.4342105263157895, 0.0, 0.5657894736842105], [0.5370370370370371, 0.0, 0.46296296296296297]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.7643979057591626
19:53:13	logging naive policy simulation 11 to ./sims/base_naive_22127_195311_run11.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.48148148148148145, 0.5185185185185185, 0.0], [0.4457831325301205, 0.0, 0.5542168674698795], [0.5314285714285715, 0.0, 0.4685714285714286]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.814814814814815
19:53:13	logging naive policy simulation 12 to ./sims/base_naive_22127_195311_run12.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.4675324675324675, 0.5324675324675324, 0.0], [0.44324324324324327, 0.0, 0.5567567567567567], [0.5287958115183246, 0.0, 0.4712041884816754]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.675324675324675
19:53:13	logging naive policy simulation 13 to ./sims/base_naive_22127_195311_run13.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.4669260700389105, 0.5330739299610895, 0.0], [0.457286432160804, 0.0, 0.542713567839196], [0.5346534653465347, 0.0, 0.46534653465346537]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.669260700389105
19:53:13	logging naive policy simulation 14 to ./sims/base_naive_22127_195311_run14.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.4727272727272727, 0.5272727272727272, 0.0], [0.45539906103286387, 0.0, 0.5446009389671361], [0.536697247706422, 0.0, 0.463302752293578]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.7272727272727275
19:53:13	logging naive policy simulation 15 to ./sims/base_naive_22127_195311_run15.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.46996466431095407, 0.5300353356890459, 0.0], [0.4652173913043478, 0.0, 0.5347826086956522], [0.5504201680672269, 0.0, 0.4495798319327731]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.69964664310954
19:53:13	logging naive policy simulation 16 to ./sims/base_naive_22127_195311_run16.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.4633333333333333, 0.5366666666666666, 0.0], [0.46887966804979253, 0.0, 0.5311203319502075], [0.5533596837944664, 0.0, 0.44664031620553357]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.633333333333333
19:53:13	logging naive policy simulation 17 to ./sims/base_naive_22127_195311_run17.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.4716981132075472, 0.5283018867924528, 0.0], [0.46484375, 0.0, 0.53515625], [0.5413533834586466, 0.0, 0.45864661654135336]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.716981132075472
19:53:13	logging naive policy simulation 18 to ./sims/base_naive_22127_195311_run18.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.4868035190615836, 0.5131964809384164, 0.0], [0.46886446886446886, 0.0, 0.5311355311355311], [0.5448028673835126, 0.0, 0.4551971326164875]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.868035190615836
19:53:13	logging naive policy simulation 19 to ./sims/base_naive_22127_195311_run19.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.48314606741573035, 0.5168539325842697, 0.0], [0.4671280276816609, 0.0, 0.532871972318339], [0.5313531353135313, 0.0, 0.46864686468646866]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.831460674157303
19:53:13	logging naive policy simulation 20 to ./sims/base_naive_22127_195311_run20.txt
19:53:13	estimating U using teacher 3 with beta 50.0
19:53:13	Estimated U: [10.0, -0.0, -10.0]
19:53:13	Estimated D: Any[[0.4797843665768194, 0.5202156334231806, 0.0], [0.4790996784565916, 0.0, 0.5209003215434084], [0.5189873417721519, 0.0, 0.4810126582278481]]
19:53:13	given U and D estimates, highest-reward arm is arm 1 with reward 4.797843665768194
19:53:13	logging naive policy simulation 21 to ./sims/base_naive_22127_195311_run21.txt
19:53:14	estimating U using teacher 3 with beta 50.0
19:53:14	Estimated U: [10.0, -0.0, -10.0]
19:53:14	Estimated D: Any[[0.4883720930232558, 0.5116279069767442, 0.0], [0.4892966360856269, 0.0, 0.5107033639143731], [0.5258358662613982, 0.0, 0.47416413373860183]]
19:53:14	given U and D estimates, highest-reward arm is arm 1 with reward 4.883720930232558
19:53:14	logging naive policy simulation 22 to ./sims/base_naive_22127_195311_run22.txt
19:53:14	estimating U using teacher 3 with beta 50.0
19:53:14	Estimated U: [10.0, -0.0, -10.0]
19:53:14	Estimated D: Any[[0.49627791563275436, 0.5037220843672456, 0.0], [0.48546511627906974, 0.0, 0.5145348837209303], [0.52046783625731, 0.0, 0.47953216374269003]]
19:53:14	given U and D estimates, highest-reward arm is arm 1 with reward 4.962779156327544
19:53:14	logging naive policy simulation 23 to ./sims/base_naive_22127_195311_run23.txt
19:53:14	estimating U using teacher 3 with beta 50.0
19:53:14	Estimated U: [10.0, -0.0, -10.0]
19:53:14	Estimated D: Any[[0.495260663507109, 0.504739336492891, 0.0], [0.49025069637883006, 0.0, 0.5097493036211699], [0.5235457063711911, 0.0, 0.47645429362880887]]
19:53:14	given U and D estimates, highest-reward arm is arm 1 with reward 4.95260663507109
19:53:14	logging naive policy simulation 24 to ./sims/base_naive_22127_195311_run24.txt
19:53:14	estimating U using teacher 3 with beta 50.0
19:53:14	Estimated U: [10.0, -0.0, -10.0]
19:53:14	Estimated D: Any[[0.5, 0.5, 0.0], [0.4881889763779528, 0.0, 0.5118110236220472], [0.5144356955380578, 0.0, 0.48556430446194226]]
19:53:14	given U and D estimates, highest-reward arm is arm 1 with reward 5.0
19:53:14	logging naive policy simulation 25 to ./sims/base_naive_22127_195311_run25.txt
19:53:14	estimating U using teacher 3 with beta 50.0
19:53:14	Estimated U: [10.0, -0.0, -10.0]
19:53:14	Estimated D: Any[[0.5010893246187363, 0.4989106753812636, 0.0], [0.47869674185463656, 0.0, 0.5213032581453634], [0.5152284263959391, 0.0, 0.4847715736040609]]
19:53:14	given U and D estimates, highest-reward arm is arm 1 with reward 5.010893246187363
19:53:14	ran 25 naive policy rollouts for 1000 timesteps each
19:53:14	Naive R: [4690.0, 4662.5, 4702.5, 4627.5, 4650.0, 4650.0, 4667.5, 4692.5, 4695.0, 4702.5, 4692.5, 4662.5, 4692.5, 4665.0, 4632.5, 4650.0, 4660.0, 4690.0, 4675.0, 4662.5, 4652.5, 4655.0, 4680.0, 4675.0, 4692.5]
