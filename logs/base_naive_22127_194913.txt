19:49:13	Running experiment with ID base_naive_22127_194913
19:49:13	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  y: Float64 0.9
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 1226

19:49:13	will explore for first 100 timesteps
19:49:13	will estimate based on feedback from teacher 3 with beta 50.0
19:49:14	generated 27 utilities (each length 3 items)
19:49:14	generated 216 arm distribution sets (each shape 3 arms x 3 items)
19:49:14	generated 1 beta value sets (each length 3 teachers)
19:49:14	generated 5832 states
19:49:14	generated 6 actions
19:49:14	generated reward function
19:49:14	generated 21 observations
19:49:14	generated observation function
19:49:14	true state State([5.0, 0.0, 5.0], Array{Float64}[[0.5, 0.0, 0.5], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]], [0.0, 0.01, 50.0])
19:49:14	logging naive policy simulation 1 to ./sims/base_naive_22127_194913_run1.txt
19:49:14	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.4117647058823529, 0.0, 0.5882352941176471], [0.0, 0.5, 0.5], [0.0, 0.6111111111111112, 0.3888888888888889]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward -1.7647058823529418
19:49:15	logging naive policy simulation 2 to ./sims/base_naive_22127_194913_run2.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.42857142857142855, 0.0, 0.5714285714285714], [0.0, 0.4230769230769231, 0.5769230769230769], [0.0, 0.5882352941176471, 0.4117647058823529]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward -1.4285714285714284
19:49:15	logging naive policy simulation 3 to ./sims/base_naive_22127_194913_run3.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.46296296296296297, 0.0, 0.5370370370370371], [0.0, 0.41025641025641024, 0.5897435897435898], [0.0, 0.5964912280701754, 0.40350877192982454]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward -0.7407407407407411
19:49:15	logging naive policy simulation 4 to ./sims/base_naive_22127_194913_run4.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.4788732394366197, 0.0, 0.5211267605633803], [0.0, 0.44642857142857145, 0.5535714285714286], [0.0, 0.5588235294117647, 0.4411764705882353]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward -0.4225352112676055
19:49:15	logging naive policy simulation 5 to ./sims/base_naive_22127_194913_run5.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.46511627906976744, 0.0, 0.5348837209302325], [0.0, 0.47058823529411764, 0.5294117647058824], [0.0, 0.5280898876404494, 0.47191011235955055]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward -0.6976744186046504
19:49:15	logging naive policy simulation 6 to ./sims/base_naive_22127_194913_run6.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.44, 0.0, 0.56], [0.0, 0.46987951807228917, 0.5301204819277109], [0.0, 0.5462962962962963, 0.4537037037037037]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward -1.2000000000000002
19:49:15	logging naive policy simulation 7 to ./sims/base_naive_22127_194913_run7.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.4406779661016949, 0.0, 0.559322033898305], [0.0, 0.45263157894736844, 0.5473684210526316], [0.0, 0.56, 0.44]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward -1.1864406779661014
19:49:15	logging naive policy simulation 8 to ./sims/base_naive_22127_194913_run8.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.46564885496183206, 0.0, 0.5343511450381679], [0.0, 0.46017699115044247, 0.5398230088495575], [0.0, 0.5586206896551724, 0.4413793103448276]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward -0.6870229007633593
19:49:15	logging naive policy simulation 9 to ./sims/base_naive_22127_194913_run9.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.47019867549668876, 0.0, 0.5298013245033113], [0.0, 0.4444444444444444, 0.5555555555555556], [0.0, 0.551948051948052, 0.44805194805194803]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward -0.5960264900662251
19:49:15	logging naive policy simulation 10 to ./sims/base_naive_22127_194913_run10.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.4691358024691358, 0.0, 0.5308641975308642], [0.0, 0.4444444444444444, 0.5555555555555556], [0.0, 0.5371428571428571, 0.46285714285714286]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward -0.6172839506172847
19:49:15	logging naive policy simulation 11 to ./sims/base_naive_22127_194913_run11.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.48863636363636365, 0.0, 0.5113636363636364], [0.0, 0.4573170731707317, 0.5426829268292683], [0.0, 0.5230769230769231, 0.47692307692307695]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward -0.22727272727272685
19:49:15	logging naive policy simulation 12 to ./sims/base_naive_22127_194913_run12.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.49473684210526314, 0.0, 0.5052631578947369], [0.0, 0.44808743169398907, 0.5519125683060109], [0.0, 0.5233644859813084, 0.4766355140186916]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward -0.10526315789473673
19:49:15	logging naive policy simulation 13 to ./sims/base_naive_22127_194913_run13.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.4878048780487805, 0.0, 0.5121951219512195], [0.0, 0.43781094527363185, 0.5621890547263682], [0.0, 0.5238095238095238, 0.47619047619047616]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward -0.24390243902439046
19:49:15	logging naive policy simulation 14 to ./sims/base_naive_22127_194913_run14.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.5088495575221239, 0.0, 0.4911504424778761], [0.0, 0.4330357142857143, 0.5669642857142857], [0.0, 0.5145228215767634, 0.4854771784232365]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward 0.1769911504424786
19:49:15	logging naive policy simulation 15 to ./sims/base_naive_22127_194913_run15.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.5185185185185185, 0.0, 0.48148148148148145], [0.0, 0.4396551724137931, 0.5603448275862069], [0.0, 0.4980694980694981, 0.5019305019305019]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward 0.37037037037037057
19:49:15	logging naive policy simulation 16 to ./sims/base_naive_22127_194913_run16.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.5114503816793893, 0.0, 0.48854961832061067], [0.0, 0.4375, 0.5625], [0.0, 0.486013986013986, 0.513986013986014]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward 0.2290076335877863
19:49:15	logging naive policy simulation 17 to ./sims/base_naive_22127_194913_run17.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.5126353790613718, 0.0, 0.48736462093862815], [0.0, 0.4367816091954023, 0.5632183908045977], [0.0, 0.48184818481848185, 0.5181518151815182]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward 0.252707581227437
19:49:15	logging naive policy simulation 18 to ./sims/base_naive_22127_194913_run18.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.5249169435215947, 0.0, 0.4750830564784053], [0.0, 0.4416058394160584, 0.5583941605839416], [0.0, 0.47003154574132494, 0.5299684542586751]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward 0.49833887043189384
19:49:15	logging naive policy simulation 19 to ./sims/base_naive_22127_194913_run19.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.5303514376996805, 0.0, 0.4696485623003195], [0.0, 0.45517241379310347, 0.5448275862068965], [0.0, 0.4759036144578313, 0.5240963855421686]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward 0.6070287539936103
19:49:15	logging naive policy simulation 20 to ./sims/base_naive_22127_194913_run20.txt
19:49:15	estimating U using teacher 3 with beta 50.0
19:49:15	Estimated U: [10.0, -0.0, -10.0]
19:49:15	Estimated D: Any[[0.5274390243902439, 0.0, 0.4725609756097561], [0.0, 0.461038961038961, 0.538961038961039], [0.0, 0.48295454545454547, 0.5170454545454546]]
19:49:15	given U and D estimates, highest-reward arm is arm 1 with reward 0.5487804878048783
19:49:16	logging naive policy simulation 21 to ./sims/base_naive_22127_194913_run21.txt
19:49:16	estimating U using teacher 3 with beta 50.0
19:49:16	Estimated U: [10.0, -0.0, -10.0]
19:49:16	Estimated D: Any[[0.5273775216138329, 0.0, 0.47262247838616717], [0.0, 0.46153846153846156, 0.5384615384615384], [0.0, 0.48509485094850946, 0.5149051490514905]]
19:49:16	given U and D estimates, highest-reward arm is arm 1 with reward 0.5475504322766575
19:49:16	logging naive policy simulation 22 to ./sims/base_naive_22127_194913_run22.txt
19:49:16	estimating U using teacher 3 with beta 50.0
19:49:16	Estimated U: [10.0, -0.0, -10.0]
19:49:16	Estimated D: Any[[0.5318559556786704, 0.0, 0.46814404432132967], [0.0, 0.46920821114369504, 0.530791788856305], [0.0, 0.4869791666666667, 0.5130208333333334]]
19:49:16	given U and D estimates, highest-reward arm is arm 1 with reward 0.6371191135734077
19:49:16	logging naive policy simulation 23 to ./sims/base_naive_22127_194913_run23.txt
19:49:16	estimating U using teacher 3 with beta 50.0
19:49:16	Estimated U: [10.0, -0.0, -10.0]
19:49:16	Estimated D: Any[[0.53315649867374, 0.0, 0.46684350132625996], [0.0, 0.4619718309859155, 0.5380281690140845], [0.0, 0.48009950248756217, 0.5199004975124378]]
19:49:16	given U and D estimates, highest-reward arm is arm 1 with reward 0.6631299734748008
19:49:16	logging naive policy simulation 24 to ./sims/base_naive_22127_194913_run24.txt
19:49:16	estimating U using teacher 3 with beta 50.0
19:49:16	Estimated U: [10.0, -0.0, -10.0]
19:49:16	Estimated D: Any[[0.5406091370558376, 0.0, 0.4593908629441624], [0.0, 0.46112600536193027, 0.5388739946380697], [0.0, 0.47743467933491684, 0.5225653206650831]]
19:49:16	given U and D estimates, highest-reward arm is arm 1 with reward 0.8121827411167517
19:49:16	logging naive policy simulation 25 to ./sims/base_naive_22127_194913_run25.txt
19:49:16	estimating U using teacher 3 with beta 50.0
19:49:16	Estimated U: [10.0, -0.0, -10.0]
19:49:16	Estimated D: Any[[0.5365853658536586, 0.0, 0.4634146341463415], [0.0, 0.45918367346938777, 0.5408163265306123], [0.0, 0.47368421052631576, 0.5263157894736842]]
19:49:16	given U and D estimates, highest-reward arm is arm 1 with reward 0.7317073170731708
19:49:16	ran 25 naive policy rollouts for 1000 timesteps each
19:49:16	Naive R: [4655.0, 4670.0, 4685.0, 4655.0, 4657.5, 4655.0, 4662.5, 4660.0, 4677.5, 4630.0, 4670.0, 4665.0, 4662.5, 4687.5, 4650.0, 4682.5, 4670.0, 4687.5, 4637.5, 4670.0, 4680.0, 4647.5, 4660.0, 4677.5, 4667.5]
