19:51:21	Running experiment with ID base_naive_22127_195120
19:51:21	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  y: Float64 0.9
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 1572

19:51:21	will explore for first 100 timesteps
19:51:21	will estimate based on feedback from teacher 3 with beta 50.0
19:51:21	generated 27 utilities (each length 3 items)
19:51:21	generated 216 arm distribution sets (each shape 3 arms x 3 items)
19:51:21	generated 1 beta value sets (each length 3 teachers)
19:51:22	generated 5832 states
19:51:22	generated 6 actions
19:51:22	generated reward function
19:51:22	generated 21 observations
19:51:22	generated observation function
19:51:22	true state State([10.0, 5.0, 0.0], Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]], [0.0, 0.01, 50.0])
19:51:22	logging naive policy simulation 1 to ./sims/base_naive_22127_195120_run1.txt
19:51:22	estimating U using teacher 3 with beta 50.0
19:51:22	Estimated U: [10.0, -0.0, -10.0]
19:51:22	Estimated D: Any[[0.4666666666666667, 0.5333333333333333, 0.0], [0.5294117647058824, 0.0, 0.47058823529411764], [0.0, 0.6153846153846154, 0.38461538461538464]]
19:51:22	given U and D estimates, highest-reward arm is arm 1 with reward 4.666666666666667
19:51:22	logging naive policy simulation 2 to ./sims/base_naive_22127_195120_run2.txt
19:51:22	estimating U using teacher 3 with beta 50.0
19:51:22	Estimated U: [10.0, -0.0, -10.0]
19:51:22	Estimated D: Any[[0.4827586206896552, 0.5172413793103449, 0.0], [0.5, 0.0, 0.5], [0.0, 0.4864864864864865, 0.5135135135135135]]
19:51:22	given U and D estimates, highest-reward arm is arm 1 with reward 4.827586206896552
19:51:22	logging naive policy simulation 3 to ./sims/base_naive_22127_195120_run3.txt
19:51:22	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.47058823529411764, 0.5294117647058824, 0.0], [0.39215686274509803, 0.0, 0.6078431372549019], [0.0, 0.42, 0.58]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.705882352941177
19:51:23	logging naive policy simulation 4 to ./sims/base_naive_22127_195120_run4.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.484375, 0.515625, 0.0], [0.40298507462686567, 0.0, 0.5970149253731343], [0.0, 0.39705882352941174, 0.6029411764705882]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.84375
19:51:23	logging naive policy simulation 5 to ./sims/base_naive_22127_195120_run5.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.43529411764705883, 0.5647058823529412, 0.0], [0.4318181818181818, 0.0, 0.5681818181818182], [0.0, 0.43209876543209874, 0.5679012345679012]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.352941176470589
19:51:23	logging naive policy simulation 6 to ./sims/base_naive_22127_195120_run6.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.4563106796116505, 0.5436893203883495, 0.0], [0.42857142857142855, 0.0, 0.5714285714285714], [0.0, 0.4421052631578947, 0.5578947368421052]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.5631067961165055
19:51:23	logging naive policy simulation 7 to ./sims/base_naive_22127_195120_run7.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.4628099173553719, 0.5371900826446281, 0.0], [0.472, 0.0, 0.528], [0.0, 0.44144144144144143, 0.5585585585585585]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.62809917355372
19:51:23	logging naive policy simulation 8 to ./sims/base_naive_22127_195120_run8.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.44696969696969696, 0.553030303030303, 0.0], [0.47619047619047616, 0.0, 0.5238095238095238], [0.0, 0.44715447154471544, 0.5528455284552846]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.46969696969697
19:51:23	logging naive policy simulation 9 to ./sims/base_naive_22127_195120_run9.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.44594594594594594, 0.5540540540540541, 0.0], [0.49390243902439024, 0.0, 0.5060975609756098], [0.0, 0.45864661654135336, 0.5413533834586466]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.45945945945946
19:51:23	logging naive policy simulation 10 to ./sims/base_naive_22127_195120_run10.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.4601226993865031, 0.5398773006134969, 0.0], [0.5136612021857924, 0.0, 0.48633879781420764], [0.0, 0.461038961038961, 0.538961038961039]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.601226993865031
19:51:23	logging naive policy simulation 11 to ./sims/base_naive_22127_195120_run11.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.46994535519125685, 0.5300546448087432, 0.0], [0.51, 0.0, 0.49], [0.0, 0.47878787878787876, 0.5212121212121212]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.699453551912568
19:51:23	logging naive policy simulation 12 to ./sims/base_naive_22127_195120_run12.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.47474747474747475, 0.5252525252525253, 0.0], [0.509090909090909, 0.0, 0.4909090909090909], [0.0, 0.4748603351955307, 0.5251396648044693]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.747474747474747
19:51:23	logging naive policy simulation 13 to ./sims/base_naive_22127_195120_run13.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.4790697674418605, 0.5209302325581395, 0.0], [0.5042735042735043, 0.0, 0.49572649572649574], [0.0, 0.46632124352331605, 0.533678756476684]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.790697674418604
19:51:23	logging naive policy simulation 14 to ./sims/base_naive_22127_195120_run14.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.47478991596638653, 0.5252100840336135, 0.0], [0.5059288537549407, 0.0, 0.49407114624505927], [0.0, 0.46859903381642515, 0.5314009661835749]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.7478991596638656
19:51:23	logging naive policy simulation 15 to ./sims/base_naive_22127_195120_run15.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.466403162055336, 0.5335968379446641, 0.0], [0.5197132616487455, 0.0, 0.48028673835125446], [0.0, 0.47058823529411764, 0.5294117647058824]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.66403162055336
19:51:23	logging naive policy simulation 16 to ./sims/base_naive_22127_195120_run16.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.4612546125461255, 0.5387453874538746, 0.0], [0.5204081632653061, 0.0, 0.47959183673469385], [0.0, 0.4708333333333333, 0.5291666666666667]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.612546125461255
19:51:23	logging naive policy simulation 17 to ./sims/base_naive_22127_195120_run17.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.46503496503496505, 0.534965034965035, 0.0], [0.5163398692810458, 0.0, 0.48366013071895425], [0.0, 0.4826254826254826, 0.5173745173745173]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.650349650349651
19:51:23	logging naive policy simulation 18 to ./sims/base_naive_22127_195120_run18.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.45514950166112955, 0.5448504983388704, 0.0], [0.50920245398773, 0.0, 0.49079754601226994], [0.0, 0.48201438848920863, 0.5179856115107914]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.5514950166112955
19:51:23	logging naive policy simulation 19 to ./sims/base_naive_22127_195120_run19.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.4658385093167702, 0.5341614906832298, 0.0], [0.5057471264367817, 0.0, 0.4942528735632184], [0.0, 0.48109965635738833, 0.5189003436426117]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.658385093167702
19:51:23	logging naive policy simulation 20 to ./sims/base_naive_22127_195120_run20.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.4608695652173913, 0.5391304347826087, 0.0], [0.4959349593495935, 0.0, 0.5040650406504065], [0.0, 0.4934210526315789, 0.506578947368421]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.608695652173913
19:51:23	logging naive policy simulation 21 to ./sims/base_naive_22127_195120_run21.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.46321525885558584, 0.5367847411444142, 0.0], [0.4883116883116883, 0.0, 0.5116883116883116], [0.0, 0.496875, 0.503125]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.632152588555859
19:51:23	logging naive policy simulation 22 to ./sims/base_naive_22127_195120_run22.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.46475195822454307, 0.5352480417754569, 0.0], [0.4837905236907731, 0.0, 0.516209476309227], [0.0, 0.49240121580547114, 0.5075987841945289]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.6475195822454305
19:51:23	logging naive policy simulation 23 to ./sims/base_naive_22127_195120_run23.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.4648241206030151, 0.535175879396985, 0.0], [0.47721822541966424, 0.0, 0.5227817745803357], [0.0, 0.49117647058823527, 0.5088235294117647]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.648241206030151
19:51:23	logging naive policy simulation 24 to ./sims/base_naive_22127_195120_run24.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.4621026894865526, 0.5378973105134475, 0.0], [0.48484848484848486, 0.0, 0.5151515151515151], [0.0, 0.4821917808219178, 0.5178082191780822]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.621026894865526
19:51:23	logging naive policy simulation 25 to ./sims/base_naive_22127_195120_run25.txt
19:51:23	estimating U using teacher 3 with beta 50.0
19:51:23	Estimated U: [10.0, -0.0, -10.0]
19:51:23	Estimated D: Any[[0.45754716981132076, 0.5424528301886793, 0.0], [0.4820627802690583, 0.0, 0.5179372197309418], [0.0, 0.47883597883597884, 0.5211640211640212]]
19:51:23	given U and D estimates, highest-reward arm is arm 1 with reward 4.5754716981132075
19:51:23	ran 25 naive policy rollouts for 1000 timesteps each
19:51:23	Naive R: [6980.0, 7000.0, 7032.5, 6972.5, 7045.0, 7005.0, 7025.0, 6972.5, 6980.0, 7010.0, 7012.5, 6997.5, 6982.5, 7052.5, 7027.5, 7007.5, 6970.0, 7010.0, 7050.0, 7060.0, 7035.0, 6972.5, 6970.0, 6955.0, 6980.0]
