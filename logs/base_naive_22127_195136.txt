19:51:36	Running experiment with ID base_naive_22127_195136
19:51:37	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  y: Float64 0.9
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 1712

19:51:37	will explore for first 100 timesteps
19:51:37	will estimate based on feedback from teacher 3 with beta 50.0
19:51:37	generated 27 utilities (each length 3 items)
19:51:37	generated 216 arm distribution sets (each shape 3 arms x 3 items)
19:51:37	generated 1 beta value sets (each length 3 teachers)
19:51:37	generated 5832 states
19:51:37	generated 6 actions
19:51:37	generated reward function
19:51:37	generated 21 observations
19:51:38	generated observation function
19:51:38	true state State([5.0, 0.0, 5.0], Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.0, 0.5, 0.5]], [0.0, 0.01, 50.0])
19:51:38	logging naive policy simulation 1 to ./sims/base_naive_22127_195136_run1.txt
19:51:38	estimating U using teacher 3 with beta 50.0
19:51:38	Estimated U: [10.0, -0.0, -10.0]
19:51:38	Estimated D: Any[[0.3333333333333333, 0.0, 0.6666666666666666], [0.7142857142857143, 0.2857142857142857, 0.0], [0.0, 0.6153846153846154, 0.38461538461538464]]
19:51:38	given U and D estimates, highest-reward arm is arm 2 with reward 7.142857142857143
19:51:38	logging naive policy simulation 2 to ./sims/base_naive_22127_195136_run2.txt
19:51:38	estimating U using teacher 3 with beta 50.0
19:51:38	Estimated U: [10.0, -0.0, -10.0]
19:51:38	Estimated D: Any[[0.43243243243243246, 0.0, 0.5675675675675675], [0.5806451612903226, 0.41935483870967744, 0.0], [0.0, 0.5357142857142857, 0.4642857142857143]]
19:51:38	given U and D estimates, highest-reward arm is arm 2 with reward 5.806451612903226
19:51:38	logging naive policy simulation 3 to ./sims/base_naive_22127_195136_run3.txt
19:51:38	estimating U using teacher 3 with beta 50.0
19:51:38	Estimated U: [10.0, -0.0, -10.0]
19:51:38	Estimated D: Any[[0.4423076923076923, 0.0, 0.5576923076923077], [0.5853658536585366, 0.4146341463414634, 0.0], [0.0, 0.5102040816326531, 0.4897959183673469]]
19:51:38	given U and D estimates, highest-reward arm is arm 2 with reward 5.853658536585366
19:51:38	logging naive policy simulation 4 to ./sims/base_naive_22127_195136_run4.txt
19:51:38	estimating U using teacher 3 with beta 50.0
19:51:38	Estimated U: [10.0, -0.0, -10.0]
19:51:38	Estimated D: Any[[0.453125, 0.0, 0.546875], [0.6206896551724138, 0.3793103448275862, 0.0], [0.0, 0.5166666666666667, 0.48333333333333334]]
19:51:38	given U and D estimates, highest-reward arm is arm 2 with reward 6.206896551724138
19:51:38	logging naive policy simulation 5 to ./sims/base_naive_22127_195136_run5.txt
19:51:38	estimating U using teacher 3 with beta 50.0
19:51:38	Estimated U: [10.0, -0.0, -10.0]
19:51:38	Estimated D: Any[[0.4605263157894737, 0.0, 0.5394736842105263], [0.6753246753246753, 0.3246753246753247, 0.0], [0.0, 0.4868421052631579, 0.5131578947368421]]
19:51:38	given U and D estimates, highest-reward arm is arm 2 with reward 6.753246753246753
19:51:38	logging naive policy simulation 6 to ./sims/base_naive_22127_195136_run6.txt
19:51:38	estimating U using teacher 3 with beta 50.0
19:51:38	Estimated U: [10.0, -0.0, -10.0]
19:51:38	Estimated D: Any[[0.4329896907216495, 0.0, 0.5670103092783505], [0.6597938144329897, 0.3402061855670103, 0.0], [0.0, 0.5051546391752577, 0.4948453608247423]]
19:51:38	given U and D estimates, highest-reward arm is arm 2 with reward 6.597938144329897
19:51:38	logging naive policy simulation 7 to ./sims/base_naive_22127_195136_run7.txt
19:51:38	estimating U using teacher 3 with beta 50.0
19:51:38	Estimated U: [10.0, -0.0, -10.0]
19:51:38	Estimated D: Any[[0.4424778761061947, 0.0, 0.5575221238938053], [0.6293103448275862, 0.3706896551724138, 0.0], [0.0, 0.5084745762711864, 0.4915254237288136]]
19:51:38	given U and D estimates, highest-reward arm is arm 2 with reward 6.293103448275862
19:51:38	logging naive policy simulation 8 to ./sims/base_naive_22127_195136_run8.txt
19:51:38	estimating U using teacher 3 with beta 50.0
19:51:38	Estimated U: [10.0, -0.0, -10.0]
19:51:38	Estimated D: Any[[0.4426229508196721, 0.0, 0.5573770491803278], [0.6142857142857143, 0.38571428571428573, 0.0], [0.0, 0.5294117647058824, 0.47058823529411764]]
19:51:38	given U and D estimates, highest-reward arm is arm 2 with reward 6.142857142857143
19:51:38	logging naive policy simulation 9 to ./sims/base_naive_22127_195136_run9.txt
19:51:38	estimating U using teacher 3 with beta 50.0
19:51:38	Estimated U: [10.0, -0.0, -10.0]
19:51:38	Estimated D: Any[[0.45, 0.0, 0.55], [0.618421052631579, 0.3815789473684211, 0.0], [0.0, 0.5364238410596026, 0.46357615894039733]]
19:51:38	given U and D estimates, highest-reward arm is arm 2 with reward 6.184210526315789
19:51:39	logging naive policy simulation 10 to ./sims/base_naive_22127_195136_run10.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.45161290322580644, 0.0, 0.5483870967741935], [0.5921787709497207, 0.40782122905027934, 0.0], [0.0, 0.54375, 0.45625]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.921787709497207
19:51:39	logging naive policy simulation 11 to ./sims/base_naive_22127_195136_run11.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.4685714285714286, 0.0, 0.5314285714285715], [0.5969387755102041, 0.4030612244897959, 0.0], [0.0, 0.5555555555555556, 0.4444444444444444]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.969387755102042
19:51:39	logging naive policy simulation 12 to ./sims/base_naive_22127_195136_run12.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.4734042553191489, 0.0, 0.526595744680851], [0.5787037037037037, 0.4212962962962963, 0.0], [0.0, 0.5510204081632653, 0.4489795918367347]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.787037037037037
19:51:39	logging naive policy simulation 13 to ./sims/base_naive_22127_195136_run13.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.46919431279620855, 0.0, 0.5308056872037915], [0.5811965811965812, 0.4188034188034188, 0.0], [0.0, 0.5343137254901961, 0.46568627450980393]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.811965811965813
19:51:39	logging naive policy simulation 14 to ./sims/base_naive_22127_195136_run14.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.47533632286995514, 0.0, 0.5246636771300448], [0.568, 0.432, 0.0], [0.0, 0.5398230088495575, 0.46017699115044247]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.68
19:51:39	logging naive policy simulation 15 to ./sims/base_naive_22127_195136_run15.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.4789915966386555, 0.0, 0.5210084033613446], [0.5708955223880597, 0.4291044776119403, 0.0], [0.0, 0.5495867768595041, 0.45041322314049587]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.708955223880597
19:51:39	logging naive policy simulation 16 to ./sims/base_naive_22127_195136_run16.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.47265625, 0.0, 0.52734375], [0.573943661971831, 0.426056338028169, 0.0], [0.0, 0.5543071161048689, 0.44569288389513106]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.73943661971831
19:51:39	logging naive policy simulation 17 to ./sims/base_naive_22127_195136_run17.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.4781021897810219, 0.0, 0.5218978102189781], [0.5714285714285714, 0.42857142857142855, 0.0], [0.0, 0.5598591549295775, 0.44014084507042256]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.7142857142857135
19:51:39	logging naive policy simulation 18 to ./sims/base_naive_22127_195136_run18.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.4713804713804714, 0.0, 0.5286195286195287], [0.5620915032679739, 0.43790849673202614, 0.0], [0.0, 0.5612244897959183, 0.4387755102040816]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.620915032679738
19:51:39	logging naive policy simulation 19 to ./sims/base_naive_22127_195136_run19.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.4585987261146497, 0.0, 0.5414012738853503], [0.5572755417956656, 0.44272445820433437, 0.0], [0.0, 0.564935064935065, 0.43506493506493504]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.572755417956657
19:51:39	logging naive policy simulation 20 to ./sims/base_naive_22127_195136_run20.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.46846846846846846, 0.0, 0.5315315315315315], [0.5552238805970149, 0.44477611940298506, 0.0], [0.0, 0.56, 0.44]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.552238805970148
19:51:39	logging naive policy simulation 21 to ./sims/base_naive_22127_195136_run21.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.47752808988764045, 0.0, 0.5224719101123596], [0.5637393767705382, 0.43626062322946174, 0.0], [0.0, 0.5548961424332344, 0.44510385756676557]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.637393767705382
19:51:39	logging naive policy simulation 22 to ./sims/base_naive_22127_195136_run22.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.4781420765027322, 0.0, 0.5218579234972678], [0.5613079019073569, 0.43869209809264303, 0.0], [0.0, 0.5454545454545454, 0.45454545454545453]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.613079019073569
19:51:39	logging naive policy simulation 23 to ./sims/base_naive_22127_195136_run23.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.4831168831168831, 0.0, 0.5168831168831168], [0.556135770234987, 0.44386422976501305, 0.0], [0.0, 0.5342465753424658, 0.4657534246575342]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.56135770234987
19:51:39	logging naive policy simulation 24 to ./sims/base_naive_22127_195136_run24.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.48, 0.0, 0.52], [0.547911547911548, 0.4520884520884521, 0.0], [0.0, 0.5329815303430079, 0.46701846965699206]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.47911547911548
19:51:39	logging naive policy simulation 25 to ./sims/base_naive_22127_195136_run25.txt
19:51:39	estimating U using teacher 3 with beta 50.0
19:51:39	Estimated U: [10.0, -0.0, -10.0]
19:51:39	Estimated D: Any[[0.47596153846153844, 0.0, 0.5240384615384616], [0.5495283018867925, 0.45047169811320753, 0.0], [0.0, 0.5303030303030303, 0.4696969696969697]]
19:51:39	given U and D estimates, highest-reward arm is arm 2 with reward 5.495283018867925
19:51:39	ran 25 naive policy rollouts for 1000 timesteps each
19:51:39	Naive R: [2392.5, 2440.0, 2402.5, 2380.0, 2397.5, 2457.5, 2430.0, 2400.0, 2407.5, 2415.0, 2442.5, 2405.0, 2430.0, 2405.0, 2410.0, 2442.5, 2407.5, 2420.0, 2412.5, 2417.5, 2440.0, 2372.5, 2417.5, 2420.0, 2415.0]
