19:51:05	Running experiment with ID base_naive_22127_19515
19:51:05	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  y: Float64 0.9
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 4641

19:51:05	will explore for first 100 timesteps
19:51:05	will estimate based on feedback from teacher 3 with beta 50.0
19:51:05	generated 27 utilities (each length 3 items)
19:51:05	generated 216 arm distribution sets (each shape 3 arms x 3 items)
19:51:05	generated 1 beta value sets (each length 3 teachers)
19:51:06	generated 5832 states
19:51:06	generated 6 actions
19:51:06	generated reward function
19:51:06	generated 21 observations
19:51:06	generated observation function
19:51:06	true state State([10.0, 5.0, 10.0], Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]], [0.0, 0.01, 50.0])
19:51:06	logging naive policy simulation 1 to ./sims/base_naive_22127_19515_run1.txt
19:51:06	estimating U using teacher 3 with beta 50.0
19:51:06	Estimated U: [10.0, -0.0, -10.0]
19:51:06	Estimated D: Any[[0.4375, 0.0, 0.5625], [0.35294117647058826, 0.6470588235294118, 0.0], [0.6923076923076923, 0.3076923076923077, 0.0]]
19:51:06	given U and D estimates, highest-reward arm is arm 3 with reward 6.923076923076923
19:51:07	logging naive policy simulation 2 to ./sims/base_naive_22127_19515_run2.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.46875, 0.0, 0.53125], [0.4722222222222222, 0.5277777777777778, 0.0], [0.5428571428571428, 0.45714285714285713, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 5.428571428571428
19:51:07	logging naive policy simulation 3 to ./sims/base_naive_22127_19515_run3.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.45454545454545453, 0.0, 0.5454545454545454], [0.4909090909090909, 0.509090909090909, 0.0], [0.4888888888888889, 0.5111111111111111, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 2 with reward 4.909090909090909
19:51:07	logging naive policy simulation 4 to ./sims/base_naive_22127_19515_run4.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.4225352112676056, 0.0, 0.5774647887323944], [0.47297297297297297, 0.527027027027027, 0.0], [0.4375, 0.5625, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 2 with reward 4.72972972972973
19:51:07	logging naive policy simulation 5 to ./sims/base_naive_22127_19515_run5.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.42857142857142855, 0.0, 0.5714285714285714], [0.44565217391304346, 0.5543478260869565, 0.0], [0.41333333333333333, 0.5866666666666667, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 2 with reward 4.456521739130435
19:51:07	logging naive policy simulation 6 to ./sims/base_naive_22127_19515_run6.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.41509433962264153, 0.0, 0.5849056603773585], [0.4473684210526316, 0.5526315789473685, 0.0], [0.4186046511627907, 0.5813953488372093, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 2 with reward 4.473684210526316
19:51:07	logging naive policy simulation 7 to ./sims/base_naive_22127_19515_run7.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.4296875, 0.0, 0.5703125], [0.4375, 0.5625, 0.0], [0.4954128440366973, 0.5045871559633027, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.954128440366973
19:51:07	logging naive policy simulation 8 to ./sims/base_naive_22127_19515_run8.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.4513888888888889, 0.0, 0.5486111111111112], [0.4276315789473684, 0.5723684210526315, 0.0], [0.484375, 0.515625, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.84375
19:51:07	logging naive policy simulation 9 to ./sims/base_naive_22127_19515_run9.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.4528301886792453, 0.0, 0.5471698113207547], [0.42168674698795183, 0.5783132530120482, 0.0], [0.4864864864864865, 0.5135135135135135, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.864864864864865
19:51:07	logging naive policy simulation 10 to ./sims/base_naive_22127_19515_run10.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.4742857142857143, 0.0, 0.5257142857142857], [0.4308510638297872, 0.5691489361702128, 0.0], [0.49710982658959535, 0.5028901734104047, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.971098265895954
19:51:07	logging naive policy simulation 11 to ./sims/base_naive_22127_19515_run11.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.47802197802197804, 0.0, 0.521978021978022], [0.4339622641509434, 0.5660377358490566, 0.0], [0.49206349206349204, 0.5079365079365079, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.92063492063492
19:51:07	logging naive policy simulation 12 to ./sims/base_naive_22127_19515_run12.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.484375, 0.0, 0.515625], [0.43555555555555553, 0.5644444444444444, 0.0], [0.49019607843137253, 0.5098039215686274, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.901960784313725
19:51:07	logging naive policy simulation 13 to ./sims/base_naive_22127_19515_run13.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.4830917874396135, 0.0, 0.5169082125603864], [0.45, 0.55, 0.0], [0.4930875576036866, 0.5069124423963134, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.9308755760368665
19:51:07	logging naive policy simulation 14 to ./sims/base_naive_22127_19515_run14.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.48214285714285715, 0.0, 0.5178571428571429], [0.45, 0.55, 0.0], [0.49356223175965663, 0.5064377682403434, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.935622317596566
19:51:07	logging naive policy simulation 15 to ./sims/base_naive_22127_19515_run15.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.4830508474576271, 0.0, 0.5169491525423728], [0.4635036496350365, 0.5364963503649635, 0.0], [0.5060240963855421, 0.4939759036144578, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 5.0602409638554215
19:51:07	logging naive policy simulation 16 to ./sims/base_naive_22127_19515_run16.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.4883720930232558, 0.0, 0.5116279069767442], [0.46959459459459457, 0.5304054054054054, 0.0], [0.5038167938931297, 0.4961832061068702, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 5.038167938931297
19:51:07	logging naive policy simulation 17 to ./sims/base_naive_22127_19515_run17.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.48717948717948717, 0.0, 0.5128205128205128], [0.46325878594249204, 0.536741214057508, 0.0], [0.498220640569395, 0.501779359430605, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.98220640569395
19:51:07	logging naive policy simulation 18 to ./sims/base_naive_22127_19515_run18.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.4897260273972603, 0.0, 0.5102739726027398], [0.45592705167173253, 0.5440729483282675, 0.0], [0.49328859060402686, 0.5067114093959731, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.932885906040268
19:51:07	logging naive policy simulation 19 to ./sims/base_naive_22127_19515_run19.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.48859934853420195, 0.0, 0.511400651465798], [0.4556213017751479, 0.5443786982248521, 0.0], [0.49038461538461536, 0.5096153846153846, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.903846153846153
19:51:07	logging naive policy simulation 20 to ./sims/base_naive_22127_19515_run20.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.48580441640378547, 0.0, 0.5141955835962145], [0.4576271186440678, 0.5423728813559322, 0.0], [0.48823529411764705, 0.5117647058823529, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.88235294117647
19:51:07	logging naive policy simulation 21 to ./sims/base_naive_22127_19515_run21.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.49107142857142855, 0.0, 0.5089285714285714], [0.4650537634408602, 0.5349462365591398, 0.0], [0.4858757062146893, 0.5141242937853108, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.858757062146893
19:51:07	logging naive policy simulation 22 to ./sims/base_naive_22127_19515_run22.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.48725212464589235, 0.0, 0.5127478753541076], [0.46981627296587924, 0.5301837270341208, 0.0], [0.4891304347826087, 0.5108695652173914, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.891304347826087
19:51:07	logging naive policy simulation 23 to ./sims/base_naive_22127_19515_run23.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.4838709677419355, 0.0, 0.5161290322580645], [0.4631043256997455, 0.5368956743002544, 0.0], [0.49869451697127937, 0.5013054830287206, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.986945169712794
19:51:07	logging naive policy simulation 24 to ./sims/base_naive_22127_19515_run24.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.4740932642487047, 0.0, 0.5259067357512953], [0.46568627450980393, 0.5343137254901961, 0.0], [0.5, 0.5, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 5.0
19:51:07	logging naive policy simulation 25 to ./sims/base_naive_22127_19515_run25.txt
19:51:07	estimating U using teacher 3 with beta 50.0
19:51:07	Estimated U: [10.0, -0.0, -10.0]
19:51:07	Estimated D: Any[[0.46601941747572817, 0.0, 0.5339805825242718], [0.4682352941176471, 0.5317647058823529, 0.0], [0.4975728155339806, 0.5024271844660194, 0.0]]
19:51:07	given U and D estimates, highest-reward arm is arm 3 with reward 4.975728155339806
19:51:07	ran 25 naive policy rollouts for 1000 timesteps each
19:51:07	Naive R: [7135.0, 7217.5, 7197.5, 7195.0, 7167.5, 7147.5, 7247.5, 7232.5, 7155.0, 7262.5, 7120.0, 7060.0, 7110.0, 7190.0, 7095.0, 7232.5, 7170.0, 7187.5, 7072.5, 7180.0, 7180.0, 7092.5, 7142.5, 7100.0, 7257.5]
