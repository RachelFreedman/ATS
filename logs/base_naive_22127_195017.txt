19:50:17	Running experiment with ID base_naive_22127_195017
19:50:18	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  y: Float64 0.9
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 4589

19:50:18	will explore for first 100 timesteps
19:50:18	will estimate based on feedback from teacher 3 with beta 50.0
19:50:18	generated 27 utilities (each length 3 items)
19:50:18	generated 216 arm distribution sets (each shape 3 arms x 3 items)
19:50:18	generated 1 beta value sets (each length 3 teachers)
19:50:18	generated 5832 states
19:50:18	generated 6 actions
19:50:18	generated reward function
19:50:18	generated 21 observations
19:50:18	generated observation function
19:50:18	true state State([5.0, 10.0, 10.0], Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]], [0.0, 0.01, 50.0])
19:50:18	logging naive policy simulation 1 to ./sims/base_naive_22127_195017_run1.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5, 0.5], [0.47058823529411764, 0.5294117647058824, 0.0], [0.3333333333333333, 0.6666666666666666, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 2 with reward 4.705882352941177
19:50:19	logging naive policy simulation 2 to ./sims/base_naive_22127_195017_run2.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5161290322580645, 0.4838709677419355], [0.5454545454545454, 0.45454545454545453, 0.0], [0.3333333333333333, 0.6666666666666666, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 2 with reward 5.454545454545454
19:50:19	logging naive policy simulation 3 to ./sims/base_naive_22127_195017_run3.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5, 0.5], [0.5576923076923077, 0.4423076923076923, 0.0], [0.30357142857142855, 0.6964285714285714, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 2 with reward 5.576923076923077
19:50:19	logging naive policy simulation 4 to ./sims/base_naive_22127_195017_run4.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5223880597014925, 0.47761194029850745], [0.5138888888888888, 0.4861111111111111, 0.0], [0.35714285714285715, 0.6428571428571429, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 2 with reward 5.138888888888888
19:50:19	logging naive policy simulation 5 to ./sims/base_naive_22127_195017_run5.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5301204819277109, 0.46987951807228917], [0.4946236559139785, 0.5053763440860215, 0.0], [0.38823529411764707, 0.611764705882353, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 2 with reward 4.946236559139785
19:50:19	logging naive policy simulation 6 to ./sims/base_naive_22127_195017_run6.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5, 0.5], [0.48148148148148145, 0.5185185185185185, 0.0], [0.37142857142857144, 0.6285714285714286, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 2 with reward 4.814814814814815
19:50:19	logging naive policy simulation 7 to ./sims/base_naive_22127_195017_run7.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5181818181818182, 0.4818181818181818], [0.4715447154471545, 0.5284552845528455, 0.0], [0.3949579831932773, 0.6050420168067226, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 2 with reward 4.715447154471545
19:50:19	logging naive policy simulation 8 to ./sims/base_naive_22127_195017_run8.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5193798449612403, 0.4806201550387597], [0.45652173913043476, 0.5434782608695652, 0.0], [0.4090909090909091, 0.5909090909090909, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 2 with reward 4.565217391304348
19:50:19	logging naive policy simulation 9 to ./sims/base_naive_22127_195017_run9.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5333333333333333, 0.4666666666666667], [0.44025157232704404, 0.559748427672956, 0.0], [0.42857142857142855, 0.5714285714285714, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 2 with reward 4.40251572327044
19:50:19	logging naive policy simulation 10 to ./sims/base_naive_22127_195017_run10.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5308641975308642, 0.4691358024691358], [0.43646408839779005, 0.56353591160221, 0.0], [0.4339622641509434, 0.5660377358490566, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 2 with reward 4.3646408839779
19:50:19	logging naive policy simulation 11 to ./sims/base_naive_22127_195017_run11.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5254237288135594, 0.4745762711864407], [0.4371859296482412, 0.5628140703517588, 0.0], [0.4602272727272727, 0.5397727272727273, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 3 with reward 4.602272727272727
19:50:19	logging naive policy simulation 12 to ./sims/base_naive_22127_195017_run12.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5291005291005291, 0.4708994708994709], [0.4423963133640553, 0.5576036866359447, 0.0], [0.4631578947368421, 0.5368421052631579, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 3 with reward 4.631578947368421
19:50:19	logging naive policy simulation 13 to ./sims/base_naive_22127_195017_run13.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5215311004784688, 0.4784688995215311], [0.4700854700854701, 0.5299145299145299, 0.0], [0.45893719806763283, 0.5410628019323671, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 2 with reward 4.700854700854701
19:50:19	logging naive policy simulation 14 to ./sims/base_naive_22127_195017_run14.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5155555555555555, 0.48444444444444446], [0.46987951807228917, 0.5301204819277109, 0.0], [0.4658119658119658, 0.5341880341880342, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 2 with reward 4.698795180722891
19:50:19	logging naive policy simulation 15 to ./sims/base_naive_22127_195017_run15.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5101214574898786, 0.4898785425101215], [0.46691176470588236, 0.5330882352941176, 0.0], [0.46774193548387094, 0.532258064516129, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 3 with reward 4.677419354838709
19:50:19	logging naive policy simulation 16 to ./sims/base_naive_22127_195017_run16.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5076923076923077, 0.49230769230769234], [0.4639175257731959, 0.5360824742268041, 0.0], [0.46946564885496184, 0.5305343511450382, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 3 with reward 4.694656488549619
19:50:19	logging naive policy simulation 17 to ./sims/base_naive_22127_195017_run17.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5054545454545455, 0.49454545454545457], [0.46905537459283386, 0.5309446254071661, 0.0], [0.4659498207885305, 0.5340501792114696, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 2 with reward 4.690553745928339
19:50:19	logging naive policy simulation 18 to ./sims/base_naive_22127_195017_run18.txt
19:50:19	estimating U using teacher 3 with beta 50.0
19:50:19	Estimated U: [10.0, -0.0, -10.0]
19:50:19	Estimated D: Any[[0.0, 0.5, 0.5], [0.4725609756097561, 0.5274390243902439, 0.0], [0.46621621621621623, 0.5337837837837838, 0.0]]
19:50:19	given U and D estimates, highest-reward arm is arm 2 with reward 4.725609756097561
19:50:20	logging naive policy simulation 19 to ./sims/base_naive_22127_195017_run19.txt
19:50:20	estimating U using teacher 3 with beta 50.0
19:50:20	Estimated U: [10.0, -0.0, -10.0]
19:50:20	Estimated D: Any[[0.0, 0.48562300319488816, 0.5143769968051118], [0.4649122807017544, 0.5350877192982456, 0.0], [0.4666666666666667, 0.5333333333333333, 0.0]]
19:50:20	given U and D estimates, highest-reward arm is arm 3 with reward 4.666666666666667
19:50:20	logging naive policy simulation 20 to ./sims/base_naive_22127_195017_run20.txt
19:50:20	estimating U using teacher 3 with beta 50.0
19:50:20	Estimated U: [10.0, -0.0, -10.0]
19:50:20	Estimated D: Any[[0.0, 0.48502994011976047, 0.5149700598802395], [0.4735376044568245, 0.5264623955431755, 0.0], [0.47416413373860183, 0.5258358662613982, 0.0]]
19:50:20	given U and D estimates, highest-reward arm is arm 3 with reward 4.741641337386018
19:50:20	logging naive policy simulation 21 to ./sims/base_naive_22127_195017_run21.txt
19:50:20	estimating U using teacher 3 with beta 50.0
19:50:20	Estimated U: [10.0, -0.0, -10.0]
19:50:20	Estimated D: Any[[0.0, 0.49712643678160917, 0.5028735632183908], [0.4745308310991957, 0.5254691689008043, 0.0], [0.46839080459770116, 0.5316091954022989, 0.0]]
19:50:20	given U and D estimates, highest-reward arm is arm 2 with reward 4.745308310991957
19:50:20	logging naive policy simulation 22 to ./sims/base_naive_22127_195017_run22.txt
19:50:20	estimating U using teacher 3 with beta 50.0
19:50:20	Estimated U: [10.0, -0.0, -10.0]
19:50:20	Estimated D: Any[[0.0, 0.4905149051490515, 0.5094850948509485], [0.48586118251928023, 0.5141388174807198, 0.0], [0.4742547425474255, 0.5257452574525745, 0.0]]
19:50:20	given U and D estimates, highest-reward arm is arm 2 with reward 4.858611825192803
19:50:20	logging naive policy simulation 23 to ./sims/base_naive_22127_195017_run23.txt
19:50:20	estimating U using teacher 3 with beta 50.0
19:50:20	Estimated U: [10.0, -0.0, -10.0]
19:50:20	Estimated D: Any[[0.0, 0.4935064935064935, 0.5064935064935064], [0.4838709677419355, 0.5161290322580645, 0.0], [0.4730077120822622, 0.5269922879177378, 0.0]]
19:50:20	given U and D estimates, highest-reward arm is arm 2 with reward 4.838709677419355
19:50:20	logging naive policy simulation 24 to ./sims/base_naive_22127_195017_run24.txt
19:50:20	estimating U using teacher 3 with beta 50.0
19:50:20	Estimated U: [10.0, -0.0, -10.0]
19:50:20	Estimated D: Any[[0.0, 0.485, 0.515], [0.4799054373522459, 0.5200945626477541, 0.0], [0.47160493827160493, 0.528395061728395, 0.0]]
19:50:20	given U and D estimates, highest-reward arm is arm 2 with reward 4.799054373522459
19:50:20	logging naive policy simulation 25 to ./sims/base_naive_22127_195017_run25.txt
19:50:20	estimating U using teacher 3 with beta 50.0
19:50:20	Estimated U: [10.0, -0.0, -10.0]
19:50:20	Estimated D: Any[[0.0, 0.475177304964539, 0.524822695035461], [0.4875283446712018, 0.5124716553287982, 0.0], [0.4665071770334928, 0.5334928229665071, 0.0]]
19:50:20	given U and D estimates, highest-reward arm is arm 2 with reward 4.875283446712018
19:50:20	ran 25 naive policy rollouts for 1000 timesteps each
19:50:20	Naive R: [7130.0, 7220.0, 7150.0, 7235.0, 7180.0, 7142.5, 7107.5, 7150.0, 7177.5, 7177.5, 7162.5, 7110.0, 7205.0, 7225.0, 7247.5, 7127.5, 7147.5, 7205.0, 7207.5, 7192.5, 7137.5, 7237.5, 7165.0, 7170.0, 7212.5]
