19:52:55	Running experiment with ID base_naive_22127_195255
19:52:56	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  y: Float64 0.9
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 3514

19:52:56	will explore for first 100 timesteps
19:52:56	will estimate based on feedback from teacher 3 with beta 50.0
19:52:56	generated 27 utilities (each length 3 items)
19:52:56	generated 216 arm distribution sets (each shape 3 arms x 3 items)
19:52:56	generated 1 beta value sets (each length 3 teachers)
19:52:56	generated 5832 states
19:52:56	generated 6 actions
19:52:56	generated reward function
19:52:56	generated 21 observations
19:52:56	generated observation function
19:52:57	true state State([0.0, 5.0, 0.0], Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.5, 0.0, 0.5]], [0.0, 0.01, 50.0])
19:52:57	logging naive policy simulation 1 to ./sims/base_naive_22127_195255_run1.txt
19:52:57	estimating U using teacher 3 with beta 50.0
19:52:57	Estimated U: [10.0, -0.0, -10.0]
19:52:57	Estimated D: Any[[0.3125, 0.6875, 0.0], [0.6666666666666666, 0.0, 0.3333333333333333], [0.6875, 0.0, 0.3125]]
19:52:57	given U and D estimates, highest-reward arm is arm 3 with reward 3.75
19:52:57	logging naive policy simulation 2 to ./sims/base_naive_22127_195255_run2.txt
19:52:57	estimating U using teacher 3 with beta 50.0
19:52:57	Estimated U: [10.0, -0.0, -10.0]
19:52:57	Estimated D: Any[[0.35294117647058826, 0.6470588235294118, 0.0], [0.6060606060606061, 0.0, 0.3939393939393939], [0.64, 0.0, 0.36]]
19:52:57	given U and D estimates, highest-reward arm is arm 1 with reward 3.5294117647058827
19:52:57	logging naive policy simulation 3 to ./sims/base_naive_22127_195255_run3.txt
19:52:57	estimating U using teacher 3 with beta 50.0
19:52:57	Estimated U: [10.0, -0.0, -10.0]
19:52:57	Estimated D: Any[[0.3076923076923077, 0.6923076923076923, 0.0], [0.5510204081632653, 0.0, 0.4489795918367347], [0.5789473684210527, 0.0, 0.42105263157894735]]
19:52:57	given U and D estimates, highest-reward arm is arm 1 with reward 3.076923076923077
19:52:57	logging naive policy simulation 4 to ./sims/base_naive_22127_195255_run4.txt
19:52:57	estimating U using teacher 3 with beta 50.0
19:52:57	Estimated U: [10.0, -0.0, -10.0]
19:52:57	Estimated D: Any[[0.3880597014925373, 0.6119402985074627, 0.0], [0.5277777777777778, 0.0, 0.4722222222222222], [0.5416666666666666, 0.0, 0.4583333333333333]]
19:52:57	given U and D estimates, highest-reward arm is arm 1 with reward 3.8805970149253732
19:52:57	logging naive policy simulation 5 to ./sims/base_naive_22127_195255_run5.txt
19:52:57	estimating U using teacher 3 with beta 50.0
19:52:57	Estimated U: [10.0, -0.0, -10.0]
19:52:57	Estimated D: Any[[0.45121951219512196, 0.5487804878048781, 0.0], [0.4827586206896552, 0.0, 0.5172413793103449], [0.5555555555555556, 0.0, 0.4444444444444444]]
19:52:57	given U and D estimates, highest-reward arm is arm 1 with reward 4.51219512195122
19:52:57	logging naive policy simulation 6 to ./sims/base_naive_22127_195255_run6.txt
19:52:57	estimating U using teacher 3 with beta 50.0
19:52:57	Estimated U: [10.0, -0.0, -10.0]
19:52:57	Estimated D: Any[[0.4329896907216495, 0.5670103092783505, 0.0], [0.47619047619047616, 0.0, 0.5238095238095238], [0.4936708860759494, 0.0, 0.5063291139240507]]
19:52:57	given U and D estimates, highest-reward arm is arm 1 with reward 4.3298969072164954
19:52:57	logging naive policy simulation 7 to ./sims/base_naive_22127_195255_run7.txt
19:52:57	estimating U using teacher 3 with beta 50.0
19:52:57	Estimated U: [10.0, -0.0, -10.0]
19:52:57	Estimated D: Any[[0.4583333333333333, 0.5416666666666666, 0.0], [0.45454545454545453, 0.0, 0.5454545454545454], [0.45918367346938777, 0.0, 0.5408163265306123]]
19:52:57	given U and D estimates, highest-reward arm is arm 1 with reward 4.583333333333333
19:52:57	logging naive policy simulation 8 to ./sims/base_naive_22127_195255_run8.txt
19:52:57	estimating U using teacher 3 with beta 50.0
19:52:57	Estimated U: [10.0, -0.0, -10.0]
19:52:57	Estimated D: Any[[0.4642857142857143, 0.5357142857142857, 0.0], [0.463768115942029, 0.0, 0.5362318840579711], [0.4745762711864407, 0.0, 0.5254237288135594]]
19:52:57	given U and D estimates, highest-reward arm is arm 1 with reward 4.642857142857143
19:52:57	logging naive policy simulation 9 to ./sims/base_naive_22127_195255_run9.txt
19:52:57	estimating U using teacher 3 with beta 50.0
19:52:57	Estimated U: [10.0, -0.0, -10.0]
19:52:57	Estimated D: Any[[0.4805194805194805, 0.5194805194805194, 0.0], [0.5031446540880503, 0.0, 0.4968553459119497], [0.49612403100775193, 0.0, 0.5038759689922481]]
19:52:57	given U and D estimates, highest-reward arm is arm 1 with reward 4.805194805194805
19:52:57	logging naive policy simulation 10 to ./sims/base_naive_22127_195255_run10.txt
19:52:57	estimating U using teacher 3 with beta 50.0
19:52:57	Estimated U: [10.0, -0.0, -10.0]
19:52:57	Estimated D: Any[[0.4860335195530726, 0.5139664804469274, 0.0], [0.5172413793103449, 0.0, 0.4827586206896552], [0.5, 0.0, 0.5]]
19:52:57	given U and D estimates, highest-reward arm is arm 1 with reward 4.860335195530726
19:52:57	logging naive policy simulation 11 to ./sims/base_naive_22127_195255_run11.txt
19:52:57	estimating U using teacher 3 with beta 50.0
19:52:57	Estimated U: [10.0, -0.0, -10.0]
19:52:57	Estimated D: Any[[0.465, 0.535, 0.0], [0.5340314136125655, 0.0, 0.46596858638743455], [0.4782608695652174, 0.0, 0.5217391304347826]]
19:52:57	given U and D estimates, highest-reward arm is arm 1 with reward 4.65
19:52:57	logging naive policy simulation 12 to ./sims/base_naive_22127_195255_run12.txt
19:52:58	estimating U using teacher 3 with beta 50.0
19:52:58	Estimated U: [10.0, -0.0, -10.0]
19:52:58	Estimated D: Any[[0.46153846153846156, 0.5384615384615384, 0.0], [0.5120772946859904, 0.0, 0.48792270531400966], [0.4943181818181818, 0.0, 0.5056818181818182]]
19:52:58	given U and D estimates, highest-reward arm is arm 1 with reward 4.615384615384616
19:52:58	logging naive policy simulation 13 to ./sims/base_naive_22127_195255_run13.txt
19:52:58	estimating U using teacher 3 with beta 50.0
19:52:58	Estimated U: [10.0, -0.0, -10.0]
19:52:58	Estimated D: Any[[0.4703389830508475, 0.5296610169491526, 0.0], [0.5066666666666667, 0.0, 0.49333333333333335], [0.4896907216494845, 0.0, 0.5103092783505154]]
19:52:58	given U and D estimates, highest-reward arm is arm 1 with reward 4.703389830508475
19:52:58	logging naive policy simulation 14 to ./sims/base_naive_22127_195255_run14.txt
19:52:58	estimating U using teacher 3 with beta 50.0
19:52:58	Estimated U: [10.0, -0.0, -10.0]
19:52:58	Estimated D: Any[[0.47619047619047616, 0.5238095238095238, 0.0], [0.4939271255060729, 0.0, 0.5060728744939271], [0.5, 0.0, 0.5]]
19:52:58	given U and D estimates, highest-reward arm is arm 1 with reward 4.761904761904762
19:52:58	logging naive policy simulation 15 to ./sims/base_naive_22127_195255_run15.txt
19:52:58	estimating U using teacher 3 with beta 50.0
19:52:58	Estimated U: [10.0, -0.0, -10.0]
19:52:58	Estimated D: Any[[0.46715328467153283, 0.5328467153284672, 0.0], [0.48863636363636365, 0.0, 0.5113636363636364], [0.5022421524663677, 0.0, 0.4977578475336323]]
19:52:58	given U and D estimates, highest-reward arm is arm 1 with reward 4.671532846715328
19:52:58	logging naive policy simulation 16 to ./sims/base_naive_22127_195255_run16.txt
19:52:58	estimating U using teacher 3 with beta 50.0
19:52:58	Estimated U: [10.0, -0.0, -10.0]
19:52:58	Estimated D: Any[[0.4778156996587031, 0.5221843003412969, 0.0], [0.4894366197183099, 0.0, 0.5105633802816901], [0.5020746887966805, 0.0, 0.4979253112033195]]
19:52:58	given U and D estimates, highest-reward arm is arm 1 with reward 4.778156996587031
19:52:58	logging naive policy simulation 17 to ./sims/base_naive_22127_195255_run17.txt
19:52:58	estimating U using teacher 3 with beta 50.0
19:52:58	Estimated U: [10.0, -0.0, -10.0]
19:52:58	Estimated D: Any[[0.4870967741935484, 0.5129032258064516, 0.0], [0.48172757475083056, 0.0, 0.5182724252491694], [0.5019455252918288, 0.0, 0.4980544747081712]]
19:52:58	given U and D estimates, highest-reward arm is arm 1 with reward 4.870967741935484
19:52:58	logging naive policy simulation 18 to ./sims/base_naive_22127_195255_run18.txt
19:52:58	estimating U using teacher 3 with beta 50.0
19:52:58	Estimated U: [10.0, -0.0, -10.0]
19:52:58	Estimated D: Any[[0.4845679012345679, 0.5154320987654321, 0.0], [0.475, 0.0, 0.525], [0.5129151291512916, 0.0, 0.4870848708487085]]
19:52:58	given U and D estimates, highest-reward arm is arm 1 with reward 4.845679012345679
19:52:58	logging naive policy simulation 19 to ./sims/base_naive_22127_195255_run19.txt
19:52:58	estimating U using teacher 3 with beta 50.0
19:52:58	Estimated U: [10.0, -0.0, -10.0]
19:52:58	Estimated D: Any[[0.49117647058823527, 0.5088235294117647, 0.0], [0.48265895953757226, 0.0, 0.5173410404624278], [0.5053003533568905, 0.0, 0.49469964664310956]]
19:52:58	given U and D estimates, highest-reward arm is arm 1 with reward 4.911764705882352
19:52:58	logging naive policy simulation 20 to ./sims/base_naive_22127_195255_run20.txt
19:52:58	estimating U using teacher 3 with beta 50.0
19:52:58	Estimated U: [10.0, -0.0, -10.0]
19:52:58	Estimated D: Any[[0.4887640449438202, 0.5112359550561798, 0.0], [0.4821917808219178, 0.0, 0.5178082191780822], [0.5084745762711864, 0.0, 0.4915254237288136]]
19:52:58	given U and D estimates, highest-reward arm is arm 1 with reward 4.8876404494382015
19:52:58	logging naive policy simulation 21 to ./sims/base_naive_22127_195255_run21.txt
19:52:58	estimating U using teacher 3 with beta 50.0
19:52:58	Estimated U: [10.0, -0.0, -10.0]
19:52:58	Estimated D: Any[[0.477088948787062, 0.522911051212938, 0.0], [0.4754521963824289, 0.0, 0.524547803617571], [0.5064516129032258, 0.0, 0.4935483870967742]]
19:52:58	given U and D estimates, highest-reward arm is arm 1 with reward 4.77088948787062
19:52:58	logging naive policy simulation 22 to ./sims/base_naive_22127_195255_run22.txt
19:52:58	estimating U using teacher 3 with beta 50.0
19:52:58	Estimated U: [10.0, -0.0, -10.0]
19:52:58	Estimated D: Any[[0.47570332480818417, 0.5242966751918159, 0.0], [0.4688279301745636, 0.0, 0.5311720698254364], [0.5123456790123457, 0.0, 0.4876543209876543]]
19:52:58	given U and D estimates, highest-reward arm is arm 1 with reward 4.757033248081842
19:52:58	logging naive policy simulation 23 to ./sims/base_naive_22127_195255_run23.txt
19:52:58	estimating U using teacher 3 with beta 50.0
19:52:58	Estimated U: [10.0, -0.0, -10.0]
19:52:58	Estimated D: Any[[0.4742014742014742, 0.5257985257985258, 0.0], [0.4684466019417476, 0.0, 0.5315533980582524], [0.5086705202312138, 0.0, 0.4913294797687861]]
19:52:58	given U and D estimates, highest-reward arm is arm 1 with reward 4.742014742014742
19:52:58	logging naive policy simulation 24 to ./sims/base_naive_22127_195255_run24.txt
19:52:58	estimating U using teacher 3 with beta 50.0
19:52:58	Estimated U: [10.0, -0.0, -10.0]
19:52:58	Estimated D: Any[[0.476303317535545, 0.523696682464455, 0.0], [0.4720930232558139, 0.0, 0.5279069767441861], [0.5069252077562327, 0.0, 0.4930747922437673]]
19:52:58	given U and D estimates, highest-reward arm is arm 1 with reward 4.76303317535545
19:52:58	logging naive policy simulation 25 to ./sims/base_naive_22127_195255_run25.txt
19:52:58	estimating U using teacher 3 with beta 50.0
19:52:58	Estimated U: [10.0, -0.0, -10.0]
19:52:58	Estimated D: Any[[0.475, 0.525, 0.0], [0.4766146993318486, 0.0, 0.5233853006681515], [0.5, 0.0, 0.5]]
19:52:58	given U and D estimates, highest-reward arm is arm 1 with reward 4.75
19:52:58	ran 25 naive policy rollouts for 1000 timesteps each
19:52:58	Naive R: [40.0, 2295.0, 2295.0, 2287.5, 2287.5, 2287.5, 2307.5, 2300.0, 2285.0, 2312.5, 2302.5, 2302.5, 2287.5, 2290.0, 2305.0, 2297.5, 2292.5, 2285.0, 2290.0, 2290.0, 2287.5, 2300.0, 2290.0, 2287.5, 2295.0]
