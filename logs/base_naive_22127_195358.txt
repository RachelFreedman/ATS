19:53:58	Running experiment with ID base_naive_22127_195358
19:53:59	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  y: Float64 0.9
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 4426

19:53:59	will explore for first 100 timesteps
19:53:59	will estimate based on feedback from teacher 3 with beta 50.0
19:53:59	generated 27 utilities (each length 3 items)
19:53:59	generated 216 arm distribution sets (each shape 3 arms x 3 items)
19:53:59	generated 1 beta value sets (each length 3 teachers)
19:53:59	generated 5832 states
19:53:59	generated 6 actions
19:53:59	generated reward function
19:54:00	generated 21 observations
19:54:00	generated observation function
19:54:00	true state State([0.0, 10.0, 10.0], Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.0, 0.5], [0.5, 0.5, 0.0]], [0.0, 0.01, 50.0])
19:54:00	logging naive policy simulation 1 to ./sims/base_naive_22127_195358_run1.txt
19:54:00	estimating U using teacher 3 with beta 50.0
19:54:00	Estimated U: [10.0, -0.0, -10.0]
19:54:00	Estimated D: Any[[0.0, 0.625, 0.375], [0.6666666666666666, 0.0, 0.3333333333333333], [0.6153846153846154, 0.38461538461538464, 0.0]]
19:54:00	given U and D estimates, highest-reward arm is arm 3 with reward 6.153846153846154
19:54:00	logging naive policy simulation 2 to ./sims/base_naive_22127_195358_run2.txt
19:54:00	estimating U using teacher 3 with beta 50.0
19:54:00	Estimated U: [10.0, -0.0, -10.0]
19:54:00	Estimated D: Any[[0.0, 0.5714285714285714, 0.42857142857142855], [0.6, 0.0, 0.4], [0.5862068965517241, 0.41379310344827586, 0.0]]
19:54:00	given U and D estimates, highest-reward arm is arm 3 with reward 5.862068965517241
19:54:00	logging naive policy simulation 3 to ./sims/base_naive_22127_195358_run3.txt
19:54:00	estimating U using teacher 3 with beta 50.0
19:54:00	Estimated U: [10.0, -0.0, -10.0]
19:54:00	Estimated D: Any[[0.0, 0.5833333333333334, 0.4166666666666667], [0.4583333333333333, 0.0, 0.5416666666666666], [0.5869565217391305, 0.41304347826086957, 0.0]]
19:54:00	given U and D estimates, highest-reward arm is arm 3 with reward 5.869565217391305
19:54:00	logging naive policy simulation 4 to ./sims/base_naive_22127_195358_run4.txt
19:54:00	estimating U using teacher 3 with beta 50.0
19:54:00	Estimated U: [10.0, -0.0, -10.0]
19:54:00	Estimated D: Any[[0.0, 0.576271186440678, 0.423728813559322], [0.4084507042253521, 0.0, 0.5915492957746479], [0.5409836065573771, 0.45901639344262296, 0.0]]
19:54:00	given U and D estimates, highest-reward arm is arm 3 with reward 5.409836065573771
19:54:00	logging naive policy simulation 5 to ./sims/base_naive_22127_195358_run5.txt
19:54:00	estimating U using teacher 3 with beta 50.0
19:54:00	Estimated U: [10.0, -0.0, -10.0]
19:54:00	Estimated D: Any[[0.0, 0.5324675324675324, 0.4675324675324675], [0.40229885057471265, 0.0, 0.5977011494252874], [0.5394736842105263, 0.4605263157894737, 0.0]]
19:54:00	given U and D estimates, highest-reward arm is arm 3 with reward 5.394736842105264
19:54:00	logging naive policy simulation 6 to ./sims/base_naive_22127_195358_run6.txt
19:54:00	estimating U using teacher 3 with beta 50.0
19:54:00	Estimated U: [10.0, -0.0, -10.0]
19:54:00	Estimated D: Any[[0.0, 0.5494505494505495, 0.45054945054945056], [0.43564356435643564, 0.0, 0.5643564356435643], [0.5531914893617021, 0.44680851063829785, 0.0]]
19:54:00	given U and D estimates, highest-reward arm is arm 3 with reward 5.531914893617022
19:54:00	logging naive policy simulation 7 to ./sims/base_naive_22127_195358_run7.txt
19:54:00	estimating U using teacher 3 with beta 50.0
19:54:00	Estimated U: [10.0, -0.0, -10.0]
19:54:00	Estimated D: Any[[0.0, 0.5636363636363636, 0.43636363636363634], [0.45217391304347826, 0.0, 0.5478260869565217], [0.5652173913043478, 0.43478260869565216, 0.0]]
19:54:00	given U and D estimates, highest-reward arm is arm 3 with reward 5.652173913043478
19:54:00	logging naive policy simulation 8 to ./sims/base_naive_22127_195358_run8.txt
19:54:00	estimating U using teacher 3 with beta 50.0
19:54:00	Estimated U: [10.0, -0.0, -10.0]
19:54:00	Estimated D: Any[[0.0, 0.5714285714285714, 0.42857142857142855], [0.47368421052631576, 0.0, 0.5263157894736842], [0.5581395348837209, 0.4418604651162791, 0.0]]
19:54:00	given U and D estimates, highest-reward arm is arm 3 with reward 5.5813953488372094
19:54:01	logging naive policy simulation 9 to ./sims/base_naive_22127_195358_run9.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.5492957746478874, 0.4507042253521127], [0.4966887417218543, 0.0, 0.5033112582781457], [0.5466666666666666, 0.4533333333333333, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.466666666666667
19:54:01	logging naive policy simulation 10 to ./sims/base_naive_22127_195358_run10.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.5316455696202531, 0.46835443037974683], [0.48148148148148145, 0.0, 0.5185185185185185], [0.5562130177514792, 0.4437869822485207, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.562130177514792
19:54:01	logging naive policy simulation 11 to ./sims/base_naive_22127_195358_run11.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.5363128491620112, 0.46368715083798884], [0.4945652173913043, 0.0, 0.5054347826086957], [0.553763440860215, 0.44623655913978494, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.53763440860215
19:54:01	logging naive policy simulation 12 to ./sims/base_naive_22127_195358_run12.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.5230769230769231, 0.47692307692307695], [0.47738693467336685, 0.0, 0.5226130653266332], [0.5577889447236181, 0.44221105527638194, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.57788944723618
19:54:01	logging naive policy simulation 13 to ./sims/base_naive_22127_195358_run13.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.5138888888888888, 0.4861111111111111], [0.47368421052631576, 0.0, 0.5263157894736842], [0.5497630331753555, 0.45023696682464454, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.497630331753554
19:54:01	logging naive policy simulation 14 to ./sims/base_naive_22127_195358_run14.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.5278969957081545, 0.4721030042918455], [0.47555555555555556, 0.0, 0.5244444444444445], [0.5482456140350878, 0.4517543859649123, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.482456140350878
19:54:01	logging naive policy simulation 15 to ./sims/base_naive_22127_195358_run15.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.5119047619047619, 0.4880952380952381], [0.4834710743801653, 0.0, 0.5165289256198347], [0.5537190082644629, 0.4462809917355372, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.537190082644629
19:54:01	logging naive policy simulation 16 to ./sims/base_naive_22127_195358_run16.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.5147058823529411, 0.4852941176470588], [0.4846153846153846, 0.0, 0.5153846153846153], [0.5444015444015444, 0.4555984555984556, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.444015444015444
19:54:01	logging naive policy simulation 17 to ./sims/base_naive_22127_195358_run17.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.49829351535836175, 0.5017064846416383], [0.4927536231884058, 0.0, 0.5072463768115942], [0.5359712230215827, 0.46402877697841727, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.359712230215827
19:54:01	logging naive policy simulation 18 to ./sims/base_naive_22127_195358_run18.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.495114006514658, 0.504885993485342], [0.4865771812080537, 0.0, 0.5134228187919463], [0.5275862068965518, 0.4724137931034483, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.275862068965518
19:54:01	logging naive policy simulation 19 to ./sims/base_naive_22127_195358_run19.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.47678018575851394, 0.5232198142414861], [0.4827586206896552, 0.0, 0.5172413793103449], [0.5261437908496732, 0.4738562091503268, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.261437908496732
19:54:01	logging naive policy simulation 20 to ./sims/base_naive_22127_195358_run20.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.4746268656716418, 0.5253731343283582], [0.48059701492537316, 0.0, 0.5194029850746269], [0.5154320987654321, 0.4845679012345679, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.15432098765432
19:54:01	logging naive policy simulation 21 to ./sims/base_naive_22127_195358_run21.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.46839080459770116, 0.5316091954022989], [0.4742857142857143, 0.0, 0.5257142857142857], [0.5102040816326531, 0.4897959183673469, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.1020408163265305
19:54:01	logging naive policy simulation 22 to ./sims/base_naive_22127_195358_run22.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.4673913043478261, 0.532608695652174], [0.475, 0.0, 0.525], [0.5069637883008357, 0.49303621169916434, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.069637883008356
19:54:01	logging naive policy simulation 23 to ./sims/base_naive_22127_195358_run23.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.46842105263157896, 0.531578947368421], [0.4868421052631579, 0.0, 0.5131578947368421], [0.5052910052910053, 0.4947089947089947, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.052910052910054
19:54:01	logging naive policy simulation 24 to ./sims/base_naive_22127_195358_run24.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.4702842377260982, 0.5297157622739018], [0.48883374689826303, 0.0, 0.511166253101737], [0.5090439276485789, 0.4909560723514212, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.090439276485789
19:54:01	logging naive policy simulation 25 to ./sims/base_naive_22127_195358_run25.txt
19:54:01	estimating U using teacher 3 with beta 50.0
19:54:01	Estimated U: [10.0, -0.0, -10.0]
19:54:01	Estimated D: Any[[0.0, 0.4723618090452261, 0.5276381909547738], [0.48426150121065376, 0.0, 0.5157384987893463], [0.508557457212714, 0.49144254278728605, 0.0]]
19:54:01	given U and D estimates, highest-reward arm is arm 3 with reward 5.08557457212714
19:54:01	ran 25 naive policy rollouts for 1000 timesteps each
19:54:01	Naive R: [4850.0, 4770.0, 4830.0, 4800.0, 4835.0, 4800.0, 4865.0, 4820.0, 4855.0, 4810.0, 4905.0, 4800.0, 4820.0, 4835.0, 4845.0, 4875.0, 4885.0, 4810.0, 4845.0, 4790.0, 4800.0, 4830.0, 4815.0, 4730.0, 4770.0]
