19:53:43	Running experiment with ID base_naive_22127_195343
19:53:43	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  y: Float64 0.9
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 4151

19:53:43	will explore for first 100 timesteps
19:53:43	will estimate based on feedback from teacher 3 with beta 50.0
19:53:43	generated 27 utilities (each length 3 items)
19:53:44	generated 216 arm distribution sets (each shape 3 arms x 3 items)
19:53:44	generated 1 beta value sets (each length 3 teachers)
19:53:44	generated 5832 states
19:53:44	generated 6 actions
19:53:44	generated reward function
19:53:44	generated 21 observations
19:53:44	generated observation function
19:53:44	true state State([5.0, 0.0, 10.0], Array{Float64}[[0.5, 0.0, 0.5], [0.0, 0.5, 0.5], [0.5, 0.5, 0.0]], [0.0, 0.01, 50.0])
19:53:44	logging naive policy simulation 1 to ./sims/base_naive_22127_195343_run1.txt
19:53:44	estimating U using teacher 3 with beta 50.0
19:53:44	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.4666666666666667, 0.0, 0.5333333333333333], [0.0, 0.5333333333333333, 0.4666666666666667], [0.6, 0.4, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 6.0
19:53:45	logging naive policy simulation 2 to ./sims/base_naive_22127_195343_run2.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.45454545454545453, 0.0, 0.5454545454545454], [0.0, 0.6206896551724138, 0.3793103448275862], [0.4666666666666667, 0.5333333333333333, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.666666666666667
19:53:45	logging naive policy simulation 3 to ./sims/base_naive_22127_195343_run3.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.46808510638297873, 0.0, 0.5319148936170213], [0.0, 0.5869565217391305, 0.41304347826086957], [0.42857142857142855, 0.5714285714285714, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.285714285714286
19:53:45	logging naive policy simulation 4 to ./sims/base_naive_22127_195343_run4.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5166666666666667, 0.0, 0.48333333333333334], [0.0, 0.5714285714285714, 0.42857142857142855], [0.45161290322580644, 0.5483870967741935, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.516129032258064
19:53:45	logging naive policy simulation 5 to ./sims/base_naive_22127_195343_run5.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5256410256410257, 0.0, 0.47435897435897434], [0.0, 0.5512820512820513, 0.44871794871794873], [0.44871794871794873, 0.5512820512820513, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.487179487179487
19:53:45	logging naive policy simulation 6 to ./sims/base_naive_22127_195343_run6.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5, 0.0, 0.5], [0.0, 0.5698924731182796, 0.43010752688172044], [0.44086021505376344, 0.5591397849462365, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.408602150537634
19:53:45	logging naive policy simulation 7 to ./sims/base_naive_22127_195343_run7.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.525, 0.0, 0.475], [0.0, 0.5607476635514018, 0.4392523364485981], [0.43478260869565216, 0.5652173913043478, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.3478260869565215
19:53:45	logging naive policy simulation 8 to ./sims/base_naive_22127_195343_run8.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5323741007194245, 0.0, 0.4676258992805755], [0.0, 0.5555555555555556, 0.4444444444444444], [0.4166666666666667, 0.5833333333333334, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.166666666666667
19:53:45	logging naive policy simulation 9 to ./sims/base_naive_22127_195343_run9.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5128205128205128, 0.0, 0.48717948717948717], [0.0, 0.5352112676056338, 0.4647887323943662], [0.4315068493150685, 0.5684931506849316, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.315068493150685
19:53:45	logging naive policy simulation 10 to ./sims/base_naive_22127_195343_run10.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5144508670520231, 0.0, 0.48554913294797686], [0.0, 0.5324675324675324, 0.4675324675324675], [0.46107784431137727, 0.5389221556886228, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.610778443113773
19:53:45	logging naive policy simulation 11 to ./sims/base_naive_22127_195343_run11.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5026737967914439, 0.0, 0.49732620320855614], [0.0, 0.5028248587570622, 0.4971751412429379], [0.4594594594594595, 0.5405405405405406, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.594594594594595
19:53:45	logging naive policy simulation 12 to ./sims/base_naive_22127_195343_run12.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5174129353233831, 0.0, 0.48258706467661694], [0.0, 0.5024630541871922, 0.4975369458128079], [0.46464646464646464, 0.5353535353535354, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.646464646464646
19:53:45	logging naive policy simulation 13 to ./sims/base_naive_22127_195343_run13.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5324074074074074, 0.0, 0.4675925925925926], [0.0, 0.509090909090909, 0.4909090909090909], [0.5022421524663677, 0.4977578475336323, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 5.022421524663677
19:53:45	logging naive policy simulation 14 to ./sims/base_naive_22127_195343_run14.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5350877192982456, 0.0, 0.4649122807017544], [0.0, 0.49145299145299143, 0.5085470085470085], [0.4979757085020243, 0.5020242914979757, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.979757085020243
19:53:45	logging naive policy simulation 15 to ./sims/base_naive_22127_195343_run15.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5435684647302904, 0.0, 0.45643153526970953], [0.0, 0.47808764940239046, 0.5219123505976095], [0.4883720930232558, 0.5116279069767442, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.883720930232558
19:53:45	logging naive policy simulation 16 to ./sims/base_naive_22127_195343_run16.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5447470817120622, 0.0, 0.45525291828793774], [0.0, 0.4888888888888889, 0.5111111111111111], [0.4852941176470588, 0.5147058823529411, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.852941176470588
19:53:45	logging naive policy simulation 17 to ./sims/base_naive_22127_195343_run17.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5345454545454545, 0.0, 0.46545454545454545], [0.0, 0.4788732394366197, 0.5211267605633803], [0.4823943661971831, 0.5176056338028169, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.823943661971831
19:53:45	logging naive policy simulation 18 to ./sims/base_naive_22127_195343_run18.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5326460481099656, 0.0, 0.46735395189003437], [0.0, 0.4769736842105263, 0.5230263157894737], [0.49158249158249157, 0.5084175084175084, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.915824915824915
19:53:45	logging naive policy simulation 19 to ./sims/base_naive_22127_195343_run19.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5374592833876222, 0.0, 0.46254071661237783], [0.0, 0.484375, 0.515625], [0.4935897435897436, 0.5064102564102564, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.935897435897436
19:53:45	logging naive policy simulation 20 to ./sims/base_naive_22127_195343_run20.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5386996904024768, 0.0, 0.4613003095975232], [0.0, 0.4925373134328358, 0.5074626865671642], [0.49848024316109424, 0.5015197568389058, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.984802431610943
19:53:45	logging naive policy simulation 21 to ./sims/base_naive_22127_195343_run21.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5317919075144508, 0.0, 0.4682080924855491], [0.0, 0.48732394366197185, 0.5126760563380282], [0.49854227405247814, 0.5014577259475219, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.985422740524782
19:53:45	logging naive policy simulation 22 to ./sims/base_naive_22127_195343_run22.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5264623955431755, 0.0, 0.4735376044568245], [0.0, 0.4742547425474255, 0.5257452574525745], [0.4859550561797753, 0.5140449438202247, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.859550561797753
19:53:45	logging naive policy simulation 23 to ./sims/base_naive_22127_195343_run23.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5267379679144385, 0.0, 0.4732620320855615], [0.0, 0.4763157894736842, 0.5236842105263158], [0.48525469168900803, 0.514745308310992, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.8525469168900806
19:53:45	logging naive policy simulation 24 to ./sims/base_naive_22127_195343_run24.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.5204081632653061, 0.0, 0.47959183673469385], [0.0, 0.47103274559193953, 0.5289672544080605], [0.49226804123711343, 0.5077319587628866, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.922680412371134
19:53:45	logging naive policy simulation 25 to ./sims/base_naive_22127_195343_run25.txt
19:53:45	estimating U using teacher 3 with beta 50.0
19:53:45	Estimated U: [10.0, -0.0, -10.0]
19:53:45	Estimated D: Any[[0.513317191283293, 0.0, 0.48668280871670705], [0.0, 0.4676258992805755, 0.5323741007194245], [0.4925373134328358, 0.5074626865671642, 0.0]]
19:53:45	given U and D estimates, highest-reward arm is arm 3 with reward 4.925373134328358
19:53:45	ran 25 naive policy rollouts for 1000 timesteps each
19:53:45	Naive R: [2475.0, 2492.5, 2487.5, 2465.0, 2500.0, 2542.5, 2510.0, 2530.0, 2492.5, 2490.0, 2515.0, 2517.5, 2510.0, 2470.0, 2460.0, 2500.0, 2485.0, 2502.5, 2487.5, 2487.5, 2557.5, 2450.0, 2460.0, 2507.5, 2542.5]
