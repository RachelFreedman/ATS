19:52:40	Running experiment with ID base_naive_22127_195240
19:52:40	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  y: Float64 0.9
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 3446

19:52:40	will explore for first 100 timesteps
19:52:40	will estimate based on feedback from teacher 3 with beta 50.0
19:52:40	generated 27 utilities (each length 3 items)
19:52:40	generated 216 arm distribution sets (each shape 3 arms x 3 items)
19:52:40	generated 1 beta value sets (each length 3 teachers)
19:52:41	generated 5832 states
19:52:41	generated 6 actions
19:52:41	generated reward function
19:52:41	generated 21 observations
19:52:41	generated observation function
19:52:41	true state State([5.0, 10.0, 5.0], Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.0, 0.5], [0.5, 0.0, 0.5]], [0.0, 0.01, 50.0])
19:52:41	logging naive policy simulation 1 to ./sims/base_naive_22127_195240_run1.txt
19:52:41	estimating U using teacher 3 with beta 50.0
19:52:41	Estimated U: [10.0, -0.0, -10.0]
19:52:41	Estimated D: Any[[0.0, 0.6470588235294118, 0.35294117647058826], [0.5, 0.0, 0.5], [0.2857142857142857, 0.0, 0.7142857142857143]]
19:52:41	given U and D estimates, highest-reward arm is arm 2 with reward 0.0
19:52:41	logging naive policy simulation 2 to ./sims/base_naive_22127_195240_run2.txt
19:52:41	estimating U using teacher 3 with beta 50.0
19:52:41	Estimated U: [10.0, -0.0, -10.0]
19:52:41	Estimated D: Any[[0.0, 0.6363636363636364, 0.36363636363636365], [0.5, 0.0, 0.5], [0.36666666666666664, 0.0, 0.6333333333333333]]
19:52:41	given U and D estimates, highest-reward arm is arm 2 with reward 0.0
19:52:42	logging naive policy simulation 3 to ./sims/base_naive_22127_195240_run3.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5490196078431373, 0.45098039215686275], [0.4489795918367347, 0.0, 0.5510204081632653], [0.4423076923076923, 0.0, 0.5576923076923077]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward -1.020408163265306
19:52:42	logging naive policy simulation 4 to ./sims/base_naive_22127_195240_run4.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5230769230769231, 0.47692307692307695], [0.5135135135135135, 0.0, 0.4864864864864865], [0.45588235294117646, 0.0, 0.5441176470588235]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward 0.27027027027026995
19:52:42	logging naive policy simulation 5 to ./sims/base_naive_22127_195240_run5.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5394736842105263, 0.4605263157894737], [0.5, 0.0, 0.5], [0.4044943820224719, 0.0, 0.5955056179775281]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward 0.0
19:52:42	logging naive policy simulation 6 to ./sims/base_naive_22127_195240_run6.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5301204819277109, 0.46987951807228917], [0.48672566371681414, 0.0, 0.5132743362831859], [0.4166666666666667, 0.0, 0.5833333333333334]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward -0.265486725663717
19:52:42	logging naive policy simulation 7 to ./sims/base_naive_22127_195240_run7.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5196078431372549, 0.4803921568627451], [0.4806201550387597, 0.0, 0.5193798449612403], [0.392, 0.0, 0.608]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward -0.387596899224806
19:52:42	logging naive policy simulation 8 to ./sims/base_naive_22127_195240_run8.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5083333333333333, 0.49166666666666664], [0.4805194805194805, 0.0, 0.5194805194805194], [0.4225352112676056, 0.0, 0.5774647887323944]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward -0.3896103896103891
19:52:42	logging naive policy simulation 9 to ./sims/base_naive_22127_195240_run9.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5075757575757576, 0.49242424242424243], [0.4911242603550296, 0.0, 0.5088757396449705], [0.432258064516129, 0.0, 0.567741935483871]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward -0.17751479289940897
19:52:42	logging naive policy simulation 10 to ./sims/base_naive_22127_195240_run10.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5033112582781457, 0.4966887417218543], [0.4810810810810811, 0.0, 0.518918918918919], [0.44508670520231214, 0.0, 0.5549132947976878]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward -0.37837837837837807
19:52:42	logging naive policy simulation 11 to ./sims/base_naive_22127_195240_run11.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5089820359281437, 0.49101796407185627], [0.48, 0.0, 0.52], [0.4416243654822335, 0.0, 0.5583756345177665]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward -0.40000000000000036
19:52:42	logging naive policy simulation 12 to ./sims/base_naive_22127_195240_run12.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5319148936170213, 0.46808510638297873], [0.4811320754716981, 0.0, 0.5188679245283019], [0.4460093896713615, 0.0, 0.5539906103286385]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward -0.3773584905660381
19:52:42	logging naive policy simulation 13 to ./sims/base_naive_22127_195240_run13.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5432692307692307, 0.4567307692307692], [0.4892703862660944, 0.0, 0.5107296137339056], [0.44493392070484583, 0.0, 0.5550660792951542]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward -0.21459227467811126
19:52:42	logging naive policy simulation 14 to ./sims/base_naive_22127_195240_run14.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5244444444444445, 0.47555555555555556], [0.49606299212598426, 0.0, 0.5039370078740157], [0.45867768595041325, 0.0, 0.5413223140495868]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward -0.07874015748031482
19:52:42	logging naive policy simulation 15 to ./sims/base_naive_22127_195240_run15.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.51440329218107, 0.48559670781893005], [0.5018315018315018, 0.0, 0.4981684981684982], [0.46387832699619774, 0.0, 0.5361216730038023]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward 0.0366300366300365
19:52:42	logging naive policy simulation 16 to ./sims/base_naive_22127_195240_run16.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.515748031496063, 0.484251968503937], [0.506993006993007, 0.0, 0.493006993006993], [0.4574468085106383, 0.0, 0.5425531914893617]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward 0.1398601398601398
19:52:42	logging naive policy simulation 17 to ./sims/base_naive_22127_195240_run17.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5183823529411765, 0.48161764705882354], [0.5016611295681063, 0.0, 0.4983388704318937], [0.4589041095890411, 0.0, 0.541095890410959]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward 0.03322259136212613
19:52:42	logging naive policy simulation 18 to ./sims/base_naive_22127_195240_run18.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5187713310580204, 0.4812286689419795], [0.5015384615384615, 0.0, 0.49846153846153846], [0.4563106796116505, 0.0, 0.5436893203883495]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward 0.03076923076923077
19:52:42	logging naive policy simulation 19 to ./sims/base_naive_22127_195240_run19.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5111111111111111, 0.4888888888888889], [0.4970414201183432, 0.0, 0.5029585798816568], [0.45482866043613707, 0.0, 0.5451713395638629]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward -0.05917159763313662
19:52:42	logging naive policy simulation 20 to ./sims/base_naive_22127_195240_run20.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.50920245398773, 0.49079754601226994], [0.5028248587570622, 0.0, 0.4971751412429379], [0.4606413994169096, 0.0, 0.5393586005830904]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward 0.056497175141243194
19:52:42	logging naive policy simulation 21 to ./sims/base_naive_22127_195240_run21.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5087719298245614, 0.49122807017543857], [0.5080645161290323, 0.0, 0.49193548387096775], [0.46089385474860334, 0.0, 0.5391061452513967]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward 0.16129032258064457
19:52:42	logging naive policy simulation 22 to ./sims/base_naive_22127_195240_run22.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5138888888888888, 0.4861111111111111], [0.5025641025641026, 0.0, 0.49743589743589745], [0.46112600536193027, 0.0, 0.5388739946380697]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward 0.0512820512820511
19:52:42	logging naive policy simulation 23 to ./sims/base_naive_22127_195240_run23.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.510752688172043, 0.489247311827957], [0.49264705882352944, 0.0, 0.5073529411764706], [0.46851385390428213, 0.0, 0.5314861460957179]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward -0.14705882352941124
19:52:42	logging naive policy simulation 24 to ./sims/base_naive_22127_195240_run24.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5177664974619289, 0.48223350253807107], [0.49523809523809526, 0.0, 0.5047619047619047], [0.45652173913043476, 0.0, 0.5434782608695652]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward -0.0952380952380949
19:52:42	logging naive policy simulation 25 to ./sims/base_naive_22127_195240_run25.txt
19:52:42	estimating U using teacher 3 with beta 50.0
19:52:42	Estimated U: [10.0, -0.0, -10.0]
19:52:42	Estimated D: Any[[0.0, 0.5170731707317073, 0.48292682926829267], [0.4919908466819222, 0.0, 0.5080091533180778], [0.4608294930875576, 0.0, 0.5391705069124424]]
19:52:42	given U and D estimates, highest-reward arm is arm 2 with reward -0.16018306636155644
19:52:42	ran 25 naive policy rollouts for 1000 timesteps each
19:52:42	Naive R: [4777.5, 4770.0, 4840.0, 4810.0, 4797.5, 4732.5, 4807.5, 4845.0, 4730.0, 4812.5, 4815.0, 4797.5, 4825.0, 4807.5, 4835.0, 4742.5, 4760.0, 4862.5, 4790.0, 4772.5, 4785.0, 4800.0, 4800.0, 4810.0, 4805.0]
