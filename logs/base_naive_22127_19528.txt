19:52:08	Running experiment with ID base_naive_22127_19528
19:52:08	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  y: Float64 0.9
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 3204

19:52:08	will explore for first 100 timesteps
19:52:08	will estimate based on feedback from teacher 3 with beta 50.0
19:52:09	generated 27 utilities (each length 3 items)
19:52:09	generated 216 arm distribution sets (each shape 3 arms x 3 items)
19:52:09	generated 1 beta value sets (each length 3 teachers)
19:52:09	generated 5832 states
19:52:09	generated 6 actions
19:52:09	generated reward function
19:52:09	generated 21 observations
19:52:09	generated observation function
19:52:09	true state State([10.0, 10.0, 5.0], Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.5, 0.0, 0.5]], [0.0, 0.01, 50.0])
19:52:09	logging naive policy simulation 1 to ./sims/base_naive_22127_19528_run1.txt
19:52:09	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.5714285714285714, 0.42857142857142855, 0.0], [0.0, 0.5714285714285714, 0.42857142857142855], [0.47058823529411764, 0.0, 0.5294117647058824]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 5.7142857142857135
19:52:10	logging naive policy simulation 2 to ./sims/base_naive_22127_19528_run2.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.6153846153846154, 0.38461538461538464, 0.0], [0.0, 0.42424242424242425, 0.5757575757575758], [0.42857142857142855, 0.0, 0.5714285714285714]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 6.153846153846154
19:52:10	logging naive policy simulation 3 to ./sims/base_naive_22127_19528_run3.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.6226415094339622, 0.37735849056603776, 0.0], [0.0, 0.43478260869565216, 0.5652173913043478], [0.4444444444444444, 0.0, 0.5555555555555556]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 6.226415094339623
19:52:10	logging naive policy simulation 4 to ./sims/base_naive_22127_19528_run4.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.6029411764705882, 0.39705882352941174, 0.0], [0.0, 0.44776119402985076, 0.5522388059701493], [0.5068493150684932, 0.0, 0.4931506849315068]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 6.029411764705882
19:52:10	logging naive policy simulation 5 to ./sims/base_naive_22127_19528_run5.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.5647058823529412, 0.43529411764705883, 0.0], [0.0, 0.47058823529411764, 0.5294117647058824], [0.44565217391304346, 0.0, 0.5543478260869565]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 5.647058823529411
19:52:10	logging naive policy simulation 6 to ./sims/base_naive_22127_19528_run6.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.5588235294117647, 0.4411764705882353, 0.0], [0.0, 0.4752475247524752, 0.5247524752475248], [0.45132743362831856, 0.0, 0.5486725663716814]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 5.588235294117647
19:52:10	logging naive policy simulation 7 to ./sims/base_naive_22127_19528_run7.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.5470085470085471, 0.452991452991453, 0.0], [0.0, 0.48739495798319327, 0.5126050420168067], [0.48507462686567165, 0.0, 0.5149253731343284]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 5.47008547008547
19:52:10	logging naive policy simulation 8 to ./sims/base_naive_22127_19528_run8.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.5503875968992248, 0.4496124031007752, 0.0], [0.0, 0.47058823529411764, 0.5294117647058824], [0.4864864864864865, 0.0, 0.5135135135135135]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 5.503875968992248
19:52:10	logging naive policy simulation 9 to ./sims/base_naive_22127_19528_run9.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.5337837837837838, 0.46621621621621623, 0.0], [0.0, 0.46621621621621623, 0.5337837837837838], [0.49411764705882355, 0.0, 0.5058823529411764]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 5.337837837837838
19:52:10	logging naive policy simulation 10 to ./sims/base_naive_22127_19528_run10.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.514792899408284, 0.48520710059171596, 0.0], [0.0, 0.4601226993865031, 0.5398773006134969], [0.4891304347826087, 0.0, 0.5108695652173914]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 5.147928994082839
19:52:10	logging naive policy simulation 11 to ./sims/base_naive_22127_19528_run11.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.5081081081081081, 0.4918918918918919, 0.0], [0.0, 0.4581005586592179, 0.5418994413407822], [0.48514851485148514, 0.0, 0.5148514851485149]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 5.081081081081082
19:52:10	logging naive policy simulation 12 to ./sims/base_naive_22127_19528_run12.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.4975124378109453, 0.5024875621890548, 0.0], [0.0, 0.4752475247524752, 0.5247524752475248], [0.48598130841121495, 0.0, 0.514018691588785]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 4.975124378109453
19:52:10	logging naive policy simulation 13 to ./sims/base_naive_22127_19528_run13.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.5046728971962616, 0.4953271028037383, 0.0], [0.0, 0.4845814977973568, 0.5154185022026432], [0.4810126582278481, 0.0, 0.5189873417721519]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 5.046728971962616
19:52:10	logging naive policy simulation 14 to ./sims/base_naive_22127_19528_run14.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.49327354260089684, 0.5067264573991032, 0.0], [0.0, 0.508, 0.492], [0.4816326530612245, 0.0, 0.5183673469387755]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 4.932735426008969
19:52:10	logging naive policy simulation 15 to ./sims/base_naive_22127_19528_run15.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.48535564853556484, 0.5146443514644351, 0.0], [0.0, 0.5, 0.5], [0.4846153846153846, 0.0, 0.5153846153846153]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 4.853556485355648
19:52:10	logging naive policy simulation 16 to ./sims/base_naive_22127_19528_run16.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.4883720930232558, 0.5116279069767442, 0.0], [0.0, 0.5035714285714286, 0.49642857142857144], [0.4891304347826087, 0.0, 0.5108695652173914]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 4.883720930232558
19:52:10	logging naive policy simulation 17 to ./sims/base_naive_22127_19528_run17.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.4855072463768116, 0.5144927536231884, 0.0], [0.0, 0.4966216216216216, 0.5033783783783784], [0.4897260273972603, 0.0, 0.5102739726027398]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 4.855072463768116
19:52:10	logging naive policy simulation 18 to ./sims/base_naive_22127_19528_run18.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.49310344827586206, 0.506896551724138, 0.0], [0.0, 0.4935483870967742, 0.5064516129032258], [0.4919614147909968, 0.0, 0.5080385852090032]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 4.931034482758621
19:52:10	logging naive policy simulation 19 to ./sims/base_naive_22127_19528_run19.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.5033112582781457, 0.4966887417218543, 0.0], [0.0, 0.48632218844984804, 0.513677811550152], [0.49244712990936557, 0.0, 0.5075528700906344]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 5.033112582781457
19:52:10	logging naive policy simulation 20 to ./sims/base_naive_22127_19528_run20.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.49538461538461537, 0.5046153846153846, 0.0], [0.0, 0.4868035190615836, 0.5131964809384164], [0.4942857142857143, 0.0, 0.5057142857142857]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 4.953846153846154
19:52:10	logging naive policy simulation 21 to ./sims/base_naive_22127_19528_run21.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.4941860465116279, 0.5058139534883721, 0.0], [0.0, 0.48295454545454547, 0.5170454545454546], [0.5068493150684932, 0.0, 0.4931506849315068]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 4.941860465116279
19:52:10	logging naive policy simulation 22 to ./sims/base_naive_22127_19528_run22.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.4972067039106145, 0.5027932960893855, 0.0], [0.0, 0.488, 0.512], [0.5039164490861618, 0.0, 0.4960835509138381]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 4.972067039106145
19:52:10	logging naive policy simulation 23 to ./sims/base_naive_22127_19528_run23.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.4905149051490515, 0.5094850948509485, 0.0], [0.0, 0.4859335038363171, 0.5140664961636828], [0.5024875621890548, 0.0, 0.4975124378109453]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 4.905149051490515
19:52:10	logging naive policy simulation 24 to ./sims/base_naive_22127_19528_run24.txt
19:52:10	estimating U using teacher 3 with beta 50.0
19:52:10	Estimated U: [10.0, -0.0, -10.0]
19:52:10	Estimated D: Any[[0.5012919896640827, 0.49870801033591733, 0.0], [0.0, 0.48514851485148514, 0.5148514851485149], [0.4976190476190476, 0.0, 0.5023809523809524]]
19:52:10	given U and D estimates, highest-reward arm is arm 1 with reward 5.0129198966408275
19:52:11	logging naive policy simulation 25 to ./sims/base_naive_22127_19528_run25.txt
19:52:11	estimating U using teacher 3 with beta 50.0
19:52:11	Estimated U: [10.0, -0.0, -10.0]
19:52:11	Estimated D: Any[[0.5, 0.5, 0.0], [0.0, 0.49160671462829736, 0.5083932853717026], [0.5057736720554272, 0.0, 0.4942263279445728]]
19:52:11	given U and D estimates, highest-reward arm is arm 1 with reward 5.0
19:52:11	ran 25 naive policy rollouts for 1000 timesteps each
19:52:11	Naive R: [9372.5, 9527.5, 9380.0, 9450.0, 9447.5, 9447.5, 9442.5, 9352.5, 9445.0, 9427.5, 9415.0, 9422.5, 9490.0, 9322.5, 9362.5, 9445.0, 9420.0, 9387.5, 9412.5, 9462.5, 9385.0, 9447.5, 9372.5, 9412.5, 9385.0]
