{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "using POMDPs, QuickPOMDPs, POMDPModelTools, POMDPPolicies, Parameters, Random, Plots, LinearAlgebra, Serialization, StatsBase\n",
    "using POMDPTools, BasicPOMCP, D3Trees, GridInterpolations, POMCPOW, POMDPModels, Combinatorics, Dates, CSV, ParticleFilters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log (generic function with 1 method)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function log(s::String)\n",
    "    s_time = Dates.format(Dates.now(), \"HH:MM:SS\\t\") * s * \"\\n\"\n",
    "    open(\"./logs/\" * expID * \".txt\", \"a\") do file\n",
    "        write(file, s_time)\n",
    "    end\n",
    "    print(s_time)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1Ã—8 Matrix{Float64}:\n",
       " 3.0  3.0  0.9  7.0  7.0  5.0  1000.0  289506.0"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set these or use command line parameters\n",
    "args = [3 3 0.9 7 7 5 1000 289506]\n",
    "# julia ApproximatePOMDP.jl 3 3 0.9 7 7 5 1000 289506"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:46:42\tRunning experiment with ID base_naive_221122_134642\n"
     ]
    }
   ],
   "source": [
    "exp_name = \"base_naive_\"\n",
    "expID = exp_name * Dates.format(Dates.now(), \"yymd_HHMMS\")\n",
    "log(\"Running experiment with ID \" * expID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:46:44\tMyParameters\n",
      "  N: Int64 3\n",
      "  K: Int64 3\n",
      "  M: Int64 3\n",
      "  y: Float64 0.9\n",
      "  umax: Int64 10\n",
      "  u_grain: Int64 7\n",
      "  d_grain: Int64 7\n",
      "  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]\n",
      "  exp_iters: Int64 5\n",
      "  exp_steps: Int64 1000\n",
      "  s_index: Int64 289506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if @isdefined args\n",
    "    @with_kw struct MyParameters\n",
    "        N::Int = convert(Int64, args[1])         # size of item set\n",
    "        K::Int = convert(Int64, args[2])         # size of arm set\n",
    "        M::Int = 3                               # size of beta set\n",
    "        y::Float64 = args[3]                     # discount factor\n",
    "        umax::Real = 10                          # max utility\n",
    "        u_grain::Int = convert(Int64, args[4])   # granularity of utility approximation\n",
    "        d_grain::Int = convert(Int64, args[5])   # granularity of arm distribution approximation\n",
    "        beta::Array{Float64} = [0.0, 0.01, 50.0]      # teacher beta values\n",
    "        exp_iters::Int = convert(Int64, args[6]) # number of rollouts to run\n",
    "        exp_steps::Int = convert(Int64, args[7]) # number of timesteps per rollout\n",
    "        s_index::Int = convert(Int64, args[8])   # index of true state\n",
    "    end\n",
    "else\n",
    "    @with_kw struct MyParameters\n",
    "        N::Int = parse(Int64, ARGS[1])           # size of item set\n",
    "        K::Int = parse(Int64, ARGS[2])           # size of arm set\n",
    "        M::Int = 3                               # size of beta set\n",
    "        y::Float64 = parse(Float64, ARGS[3])     # discount factor\n",
    "        umax::Real = 10                          # max utility\n",
    "        u_grain::Int = parse(Int64, ARGS[4])     # granularity of utility approximation\n",
    "        d_grain::Int = parse(Int64, ARGS[5])     # granularity of arm distribution approximation\n",
    "        beta::Array{Float64} = [0.0, 0.01, 50.0]      # teacher beta values\n",
    "        exp_iters::Int = parse(Int64, ARGS[6])   # number of rollouts to run\n",
    "        exp_steps::Int = parse(Int64, ARGS[7])   # number of timesteps per rollout\n",
    "        s_index::Int = parse(Int64, ARGS[8])     # index of true state\n",
    "    end\n",
    "end\n",
    "\n",
    "params = MyParameters()\n",
    "log(string(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create POMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:46:45\tgenerated 343 utilities (each length 3 items)\n",
      "13:46:46\tgenerated 21952 arm distribution sets (each shape 3 arms x 3 items)\n",
      "13:46:46\tgenerated 1 beta value sets (each length 3 teachers)\n",
      "13:46:47\tgenerated 7529536 states\n",
      "13:46:47\tgenerated 6 actions\n",
      "13:46:47\tgenerated reward function\n",
      "13:46:47\tgenerated 21 observations\n",
      "13:46:47\tgenerated observation function\n"
     ]
    }
   ],
   "source": [
    "struct State\n",
    "    u::Array{Float64}         # list of N utility values for N items\n",
    "    d::Array{Array{Float64}}  # list of K arm distributions, each assigning probabilities to N items\n",
    "    b::Array{Float64}         # list of M beta values\n",
    "end\n",
    "\n",
    "# space of utility functions\n",
    "umin = 0\n",
    "grid_coor = fill(range(umin, params.umax, length=params.u_grain), params.N)\n",
    "U = RectangleGrid(grid_coor...)\n",
    "\n",
    "@assert length(U[1]) == params.N\n",
    "log(\"generated \" * string(length(U)) * \" utilities (each length \" * string(length(U[1])) * \" items)\")\n",
    "\n",
    "function generate_probability_distributions(N::Int, coor::Array{Float64}, S::Float64=1.0)\n",
    "    if S == 0\n",
    "        return [[0.0 for _ in 1:N]]\n",
    "    end\n",
    "    if N == 1\n",
    "        return [[float(S)]]\n",
    "    end\n",
    "    out = []\n",
    "    range = coor[1:findall(x -> isapprox(x, S, atol=1e-15), coor)[1]]\n",
    "    for k in range\n",
    "        subsolution = generate_probability_distributions(N - 1, coor, S - k)\n",
    "        for lst in subsolution\n",
    "            if typeof(lst[1]) != Float64\n",
    "                log(\"ERROR: lst \" * string(lst) * \" has type \" * string(typeof(lst[1])) * \". Must be Float64.\")\n",
    "            end\n",
    "            prepend!(lst, float(k))\n",
    "        end\n",
    "        out = vcat(out, subsolution)\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "# space of arm distributions\n",
    "coor = collect(range(0.0, 1.0, length=params.d_grain))\n",
    "simplex_list = generate_probability_distributions(params.N, coor)\n",
    "D_tuples = vec(collect(Base.product(fill(simplex_list, params.K)...)))\n",
    "D = [collect(d) for d in D_tuples]\n",
    "\n",
    "@assert length(D[1]) == params.K\n",
    "@assert length(D[1][1]) == params.N\n",
    "log(\"generated \" * string(length(D)) * \" arm distribution sets (each shape \" * string(length(D[1])) * \" arms x \" * string(length(D[1][1])) * \" items)\")\n",
    "\n",
    "# beta values\n",
    "B = [params.beta]\n",
    "\n",
    "# each beta value set must be length M\n",
    "@assert length(B[1]) == params.M\n",
    "log(\"generated \" * string(length(B)) * \" beta value sets (each length \" * string(length(B[1])) * \" teachers)\")\n",
    "\n",
    "# State space\n",
    "S = [[State(u, d, b) for u in U, d in D, b in B]...,]\n",
    "\n",
    "log(\"generated \" * string(length(S)) * \" states\")\n",
    "\n",
    "# Action space - actions are arm choices (K) or beta selections (M)\n",
    "struct Action\n",
    "    name::String      # valid names are {B,C} + index\n",
    "    isBeta::Bool      # true if 'B' action, false if 'C' action\n",
    "    index::Integer    # index of beta (if 'B' action) or arm choice (if 'C' action)\n",
    "end\n",
    "\n",
    "A = Array{Action}(undef, params.K + params.M)\n",
    "for i in 1:params.K+params.M\n",
    "    if i <= params.K\n",
    "        A[i] = Action(\"C\" * string(i), false, i)\n",
    "    else\n",
    "        A[i] = Action(\"B\" * string(i - params.K), true, i - params.K)\n",
    "    end\n",
    "end\n",
    "log(\"generated \" * string(length(A)) * \" actions\")\n",
    "\n",
    "# Reward function\n",
    "function R(s::State, a::Action)\n",
    "    # if beta selected, return 0\n",
    "    if a.isBeta\n",
    "        return 0\n",
    "        # if arm pulled, return that arm's avg utility\n",
    "    else\n",
    "        utilities = s.u\n",
    "        arm_dist = s.d[a.index]\n",
    "        return dot(utilities, arm_dist)\n",
    "    end\n",
    "end\n",
    "log(\"generated reward function\")\n",
    "\n",
    "# item space\n",
    "I = 1:params.N\n",
    "\n",
    "# preference space\n",
    "struct Preference\n",
    "    i0::Int    # first item to compare, in {1,2,...,N}\n",
    "    i1::Int    # second item to compare, in {1,2,...,N}\n",
    "    label::Int # feedback label, in {0,1}\n",
    "end\n",
    "\n",
    "P = [[Preference(i0, i1, label) for i0 in I, i1 in I, label in [0, 1]]...,]\n",
    "\n",
    "# observation space\n",
    "struct Observation\n",
    "    isItem::Bool    # true if item returned, false otherwise\n",
    "    i::Int          # item, if item returned\n",
    "    p::Preference   # preference, if preference returned\n",
    "end\n",
    "\n",
    "invalid_i = -1\n",
    "invalid_p = Preference(-1, -1, -1)\n",
    "I_obs = [Observation(true, i, invalid_p) for i in I]\n",
    "P_obs = [Observation(false, invalid_i, p) for p in P]\n",
    "omega = union(I_obs, P_obs)\n",
    "\n",
    "log(\"generated \" * string(length(omega)) * \" observations\")\n",
    "\n",
    "# unnormalized query profile (likelihood of querying 1,1; 2,1; 3,1; ... ; N,1; 1,2; 2,2; ... ; N,N)\n",
    "Q = ones(params.N * params.N)\n",
    "\n",
    "# preference probability (expected preference, or probability that preference=1)\n",
    "function Pr(p::Preference, s::State, b::Float64)\n",
    "    prob_pref_1 = exp(Float64(b) * s.u[p.i1]) / (exp(Float64(b) * s.u[p.i1]) + exp(Float64(b) * s.u[p.i0]))\n",
    "    if p.label == 1\n",
    "        return prob_pref_1\n",
    "    else\n",
    "        return 1.0 - prob_pref_1\n",
    "    end\n",
    "end\n",
    "\n",
    "function O(s::State, a::Action, sp::State)\n",
    "    # if B action, obs in P_obs\n",
    "    if a.isBeta\n",
    "        prob_of_pref = [Pr(o.p, s, s.b[a.index]) for o in P_obs]\n",
    "        prob_of_query = vcat(Q, Q)   # doubled because each query appears once for each label\n",
    "\n",
    "        # weight by querying profile to get dist\n",
    "        dist = [prob_of_pref[i] * prob_of_query[i] for i in 1:length(prob_of_pref)]\n",
    "        normalized_dist = dist / sum(dist)\n",
    "        return SparseCat(P_obs, normalized_dist)\n",
    "        # if C action, obs in I_obs\n",
    "    else\n",
    "        return SparseCat(I_obs, s.d[a.index])\n",
    "    end\n",
    "end\n",
    "\n",
    "log(\"generated observation function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:46:51\twill explore for first 100 timesteps\n"
     ]
    }
   ],
   "source": [
    "# baseline-specific parameters\n",
    "t_explore = 100\n",
    "\n",
    "log(\"will explore for first \"*string(t_explore)*\" timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pull_arm (generic function with 1 method)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function select_arm(K)\n",
    "    return rand(1:K)\n",
    "end\n",
    "\n",
    "function select_teacher(M)\n",
    "    # always query teacher 2 to use mid-range beta\n",
    "    return 2\n",
    "#     return rand(1:M)\n",
    "end\n",
    "\n",
    "function select_query(N)\n",
    "    return sort(sample(1:N, 2, replace=false))\n",
    "end\n",
    "\n",
    "function query_teacher(m, i1, i2, s)\n",
    "    b = s.b[m]\n",
    "    p1 = exp(Float64(b) * s.u[i1]) / (exp(Float64(b) * s.u[i1]) + exp(Float64(b) * s.u[i2]))\n",
    "    return sample(0:1, Weights([p1, 1-p1]))\n",
    "end\n",
    "    \n",
    "function pull_arm(k, s)\n",
    "    return sample(1:params.N, Weights(s.d[k]))\n",
    "end     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execute policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimate_u (generic function with 1 method)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_pref(b)\n",
    "    N = 3\n",
    "    u = [10, 4, 0]\n",
    "    query = sort(sample(1:N, 2, replace=false))\n",
    "    i1 = query[1]\n",
    "    i2 = query[2]\n",
    "    p1 = exp(Float64(b) * u[i1]) / (exp(Float64(b) * u[i1]) + exp(Float64(b) * u[i2]))\n",
    "    label = sample(0:1, Weights([p1, 1-p1]))\n",
    "    return Preference(i1, i2, label)\n",
    "end\n",
    "\n",
    "function est_P(a, o, M, N)\n",
    "    teach_prefs = zeros(M, N, N)\n",
    "    teach_pulls = zeros(M, N, N)\n",
    "    for s in 1:length(a)\n",
    "        if a[s].isBeta\n",
    "            index = a[s].index\n",
    "            i0, i1, label = o[s].p.i0, o[s].p.i1, o[s].p.label\n",
    "            teach_pulls[index, i0, i1] = teach_pulls[index, i0, i1] + 1\n",
    "            teach_prefs[index, i0, i1] = teach_prefs[index, i0, i1] + label\n",
    "        end\n",
    "    end\n",
    "\n",
    "    P_hat = zeros(M, N, N)\n",
    "    for index in 1:M\n",
    "        for i0 in 1:N-1\n",
    "            for i1 in i0+1:N\n",
    "                if teach_prefs[index, i0, i1] == 0\n",
    "                    println(\"WARNING: teacher \"*string(index)*\" never prefers item \"*string(i1)*\" to item \"*string(i0)*\", so setting preference probability at \"*string(eps))\n",
    "                    P_hat[index, i0, i1] = eps\n",
    "                else\n",
    "                    P_hat[index, i0, i1] = teach_prefs[index, i0, i1]/teach_pulls[index, i0, i1]\n",
    "                end\n",
    "                P_hat[index, i1, i0] = 1-P_hat[index, i0, i1]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    for i in 1:N\n",
    "        for m in 1:M\n",
    "            P_hat[m, i,i] = 0.5\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return P_hat\n",
    "end\n",
    "\n",
    "function calc_deltas(P_hat, b, N, t)\n",
    "    deltas = zeros(N, N)\n",
    "    for i in 1:N\n",
    "        for j in 1:N\n",
    "            deltas[j,i] = calc_delta(P_hat[t,i,j],b[t])\n",
    "            deltas[i,j] = -deltas[j,i]\n",
    "        end\n",
    "    end\n",
    "    return deltas\n",
    "end\n",
    "\n",
    "function calc_delta(p, b)\n",
    "    return (-1/b)*Base.log((1/p)-1)\n",
    "end\n",
    "\n",
    "function est_U(deltas, umax, N)\n",
    "    rnge = maximum(deltas)\n",
    "    result = findall(x->x==rnge, deltas)[1]\n",
    "    min_i = result[1][1]\n",
    "    max_i = result[2][1]\n",
    "    true_vals = zeros(N)\n",
    "    for i in 1:N\n",
    "        val = deltas[max_i,i]\n",
    "        true_vals[i] = -val*(umax/rnge)\n",
    "    end\n",
    "    \n",
    "    return true_vals\n",
    "end\n",
    "\n",
    "function estimate_u(a, o, teacher, M, N, b, umax)\n",
    "    P_hat = est_P(a, o, M, N)\n",
    "    println(\"P_hat\")\n",
    "    println(P_hat[2,:,:])\n",
    "    deltas = calc_deltas(P_hat, b, N, teacher)\n",
    "    println(\"deltas\")\n",
    "    println(deltas)\n",
    "    U_hat = est_U(deltas, umax, N)\n",
    "    return U_hat\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimate_d (generic function with 1 method)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# infer D\n",
    "function estimate_d(a, o, K, N)\n",
    "    items_returned = zeros((K, N))\n",
    "    for s in 1:length(a)\n",
    "        if !a[s].isBeta\n",
    "            items_returned[a[s].index, o[s].i] = items_returned[a[s].index, o[s].i] + 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    D_hat = []\n",
    "    for row_index in 1:size(items_returned, 1)\n",
    "        row = items_returned[row_index,:]\n",
    "        push!(D_hat, row/sum(row))\n",
    "    end\n",
    "\n",
    "    return D_hat\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calc_max_arm (generic function with 1 method)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calc expected U(arm)\n",
    "function calc_max_arm(u, d)\n",
    "    max_val = -999999999\n",
    "    max_arm = -999999999\n",
    "    \n",
    "    for i in 1:length(d)\n",
    "        val = dot(u, d[i])\n",
    "        if val > max_val\n",
    "            max_val = val\n",
    "            max_arm = i\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return max_arm, max_val\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:53:25\ttrue state State([10.0, 1.6666666666666667, 0.0], Array{Float64}[[0.0, 0.6666666666666666, 0.33333333333333337], [0.0, 0.3333333333333333, 0.6666666666666667], [0.0, 0.16666666666666666, 0.8333333333333334]], [0.0, 0.01, 50.0])\n",
      "13:53:25\tlogging naive policy simulation 1 to ./sims/base_naive_221122_134642_run1.txt\n",
      "13:53:25\testimating U using teacher 2 with beta 0.01\n",
      "WARNING: teacher 1 never prefers item 2 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 1 never prefers item 3 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 1 never prefers item 3 to item 2, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 2 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 3 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 3 to item 2, so setting preference probability at 1.0e-5\n",
      "P_hat\n",
      "[0.5 0.15384615384615385 0.5; 0.8461538461538461 0.5 0.4117647058823529; 0.5 0.5882352941176471 0.5]\n",
      "deltas\n",
      "[0.0 170.47480922384247 -0.0; -170.47480922384247 0.0 35.66749439387324; 0.0 -35.66749439387324 0.0]\n",
      "13:53:25\tEstimated U: [10.0, -0.0, -2.0922442768092457]\n",
      "13:53:25\tEstimated D: Any[[0.0, 0.5294117647058824, 0.47058823529411764], [0.0, 0.26666666666666666, 0.7333333333333333], [0.0, 0.16666666666666666, 0.8333333333333334]]\n",
      "13:53:25\tgiven U and D estimates, highest-reward arm is arm 1 with reward -0.9845855420278803\n",
      "13:53:25\tlogging naive policy simulation 2 to ./sims/base_naive_221122_134642_run2.txt\n",
      "13:53:25\testimating U using teacher 2 with beta 0.01\n",
      "WARNING: teacher 1 never prefers item 2 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 1 never prefers item 3 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 1 never prefers item 3 to item 2, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 2 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 3 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 3 to item 2, so setting preference probability at 1.0e-5\n",
      "P_hat\n",
      "[0.5 0.2631578947368421 0.5; 0.736842105263158 0.5 0.41379310344827586; 0.5 0.5862068965517242 0.5]\n",
      "deltas\n",
      "[0.0 102.96194171811588 -0.0; -102.96194171811588 0.0 34.83066942682162; 0.0 -34.83066942682162 0.0]\n",
      "13:53:25\tEstimated U: [10.0, -0.0, -3.3828683536465647]\n",
      "13:53:25\tEstimated D: Any[[0.0, 0.6176470588235294, 0.38235294117647056], [0.0, 0.36, 0.64], [0.0, 0.19444444444444445, 0.8055555555555556]]\n",
      "13:53:25\tgiven U and D estimates, highest-reward arm is arm 1 with reward -1.2934496646295688\n",
      "13:53:25\tlogging naive policy simulation 3 to ./sims/base_naive_221122_134642_run3.txt\n",
      "13:53:25\testimating U using teacher 2 with beta 0.01\n",
      "WARNING: teacher 1 never prefers item 2 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 1 never prefers item 3 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 1 never prefers item 3 to item 2, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 2 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 3 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 3 to item 2, so setting preference probability at 1.0e-5\n",
      "P_hat\n",
      "[0.5 0.36 0.49019607843137253; 0.64 0.5 0.5116279069767442; 0.5098039215686274 0.4883720930232558 0.5]\n",
      "deltas\n",
      "[0.0 57.53641449035618 3.9220713153281155; -57.53641449035618 0.0 -4.652001563489312; -3.9220713153281155 4.652001563489312 0.0]\n",
      "13:53:25\tEstimated U: [10.0, -0.0, 0.808531710690635]\n",
      "13:53:25\tEstimated D: Any[[0.0, 0.6271186440677966, 0.3728813559322034], [0.0, 0.35555555555555557, 0.6444444444444445], [0.0, 0.17307692307692307, 0.8269230769230769]]\n",
      "13:53:25\tgiven U and D estimates, highest-reward arm is arm 3 with reward 0.6685935299941789\n",
      "13:53:25\tlogging naive policy simulation 4 to ./sims/base_naive_221122_134642_run4.txt\n",
      "13:53:25\testimating U using teacher 2 with beta 0.01\n",
      "WARNING: teacher 1 never prefers item 2 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 1 never prefers item 3 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 1 never prefers item 3 to item 2, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 2 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 3 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 3 to item 2, so setting preference probability at 1.0e-5\n",
      "P_hat\n",
      "[0.5 0.3880597014925373 0.5147058823529411; 0.6119402985074627 0.5 0.4745762711864407; 0.4852941176470589 0.5254237288135593 0.5]\n",
      "deltas\n",
      "[0.0 45.54755286828256 -5.884050002293339; -45.54755286828256 0.0 10.178269430994199; 5.884050002293339 -10.178269430994199 0.0]\n",
      "13:53:25\tEstimated U: [10.0, -0.0, -2.2346468229431324]\n",
      "13:53:25\tEstimated D: Any[[0.0, 0.6375, 0.3625], [0.0, 0.328125, 0.671875], [0.0, 0.1774193548387097, 0.8225806451612904]]\n",
      "13:53:25\tgiven U and D estimates, highest-reward arm is arm 1 with reward -0.8100594733168854\n",
      "13:53:25\tlogging naive policy simulation 5 to ./sims/base_naive_221122_134642_run5.txt\n",
      "13:53:25\testimating U using teacher 2 with beta 0.01\n",
      "WARNING: teacher 1 never prefers item 2 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 1 never prefers item 3 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 1 never prefers item 3 to item 2, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 2 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 3 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 3 to item 2, so setting preference probability at 1.0e-5\n",
      "P_hat\n",
      "[0.5 0.4578313253012048 0.5212765957446809; 0.5421686746987953 0.5 0.5066666666666667; 0.4787234042553191 0.4933333333333333 0.5]\n",
      "deltas\n",
      "[0.0 16.90763300439344 -8.515780834030696; -16.90763300439344 0.0 -2.666824708216149; 8.515780834030696 2.666824708216149 0.0]\n",
      "13:53:25\tEstimated U: [10.0, -0.0, 1.5772903915782748]\n",
      "13:53:25\tEstimated D: Any[[0.0, 0.6451612903225806, 0.3548387096774194], [0.0, 0.35526315789473684, 0.6447368421052632], [0.0, 0.16455696202531644, 0.8354430379746836]]\n",
      "13:53:25\tgiven U and D estimates, highest-reward arm is arm 3 with reward 1.3177362765084322\n",
      "13:53:25\tlogging naive policy simulation 6 to ./sims/base_naive_221122_134642_run6.txt\n",
      "13:53:25\testimating U using teacher 2 with beta 0.01\n",
      "WARNING: teacher 1 never prefers item 2 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 1 never prefers item 3 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 1 never prefers item 3 to item 2, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 2 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 3 to item 1, so setting preference probability at 1.0e-5\n",
      "WARNING: teacher 3 never prefers item 3 to item 2, so setting preference probability at 1.0e-5\n",
      "P_hat\n",
      "[0.5 0.4639175257731959 0.4778761061946903; 0.5360824742268041 0.5 0.5274725274725275; 0.5221238938053097 0.4725274725274725 0.5]\n",
      "deltas\n",
      "[0.0 14.458122881110755 8.855339734144483; -14.458122881110755 0.0 -11.00008952143287; -8.855339734144483 11.00008952143287 0.0]\n",
      "13:53:25\tEstimated U: [10.0, -0.0, 7.608241824949671]\n",
      "13:53:25\tEstimated D: Any[[0.0, 0.6759259259259259, 0.32407407407407407], [0.0, 0.3870967741935484, 0.6129032258064516], [0.0, 0.17346938775510204, 0.826530612244898]]\n",
      "13:53:25\tgiven U and D estimates, highest-reward arm is arm 3 with reward 6.288444773682891\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "BoundsError: attempt to access 5-element Vector{Float64} at index [6]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 5-element Vector{Float64} at index [6]",
      "",
      "Stacktrace:",
      " [1] setindex!(A::Vector{Float64}, x::Float64, i1::Int64)",
      "   @ Base ./array.jl:966",
      " [2] top-level scope",
      "   @ In[273]:82",
      " [3] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "# query teachers and pull arms randomly for t_b timesteps\n",
    "# then query argmax arm for remaining timesteps\n",
    "true_state = S[params.s_index]\n",
    "log(\"true state \"*string(true_state))\n",
    "\n",
    "as = []\n",
    "os = []\n",
    "rs = []\n",
    "\n",
    "# estimate using teacher 2, since it has intermediate, reasonable beta\n",
    "teacher = 2\n",
    "random_R = zeros(params.exp_iters)\n",
    "for iter in 1:20#params.exp_iters\n",
    "    log(\"logging naive policy simulation \"*string(iter)*\" to \"*\"./sims/\"*expID*\"_run\"*string(iter)*\".txt\")\n",
    "    open(\"./sims/\"*expID*\"_run\"*string(iter)*\".txt\", \"w\") do file\n",
    "        write(file, string(true_state))\n",
    "    end\n",
    "    r_accum = 0.\n",
    "    for t in 1:t_explore\n",
    "        msg = \"\"\n",
    "        if rand(Bool)\n",
    "            # select arm\n",
    "            action = select_arm(params.K)\n",
    "            a = Action(\"C\"*string(action), false, action)\n",
    "            \n",
    "            # pull arm\n",
    "            item = pull_arm(a.index, true_state)\n",
    "            o = Observation(true, item, invalid_p)\n",
    "            r = R(true_state, a)\n",
    "            r_accum = r_accum + r\n",
    "            \n",
    "            push!(as, a)\n",
    "            push!(os, o)\n",
    "            push!(rs, r)\n",
    "            msg = \"\\n\"*string(t)*\",C,\"*a.name*\",i\"*string(o.i)*\",\"*string(r)\n",
    "        else\n",
    "            # select teacher\n",
    "            action = select_teacher(params.M)\n",
    "            a = Action(\"B\"*string(action), true, action)\n",
    "            \n",
    "            # query teacher\n",
    "            q = select_query(params.N)\n",
    "            label = query_teacher(a.index, q[1], q[2], true_state)\n",
    "            p = Preference(q[1], q[2], label)\n",
    "            o = Observation(false, invalid_i, p)\n",
    "            r = R(true_state, a)\n",
    "            r_accum = r_accum + r\n",
    "            \n",
    "            push!(as, a)\n",
    "            push!(os, o)\n",
    "            push!(rs, r)\n",
    "            msg = \"\\n\"*string(t)*\",B,\"*a.name*\",(i\"*string(o.p.i0)*\"-i\"*string(o.p.i1)*\";\"*string(o.p.label)*\"),\"*string(r)\n",
    "        end\n",
    "        \n",
    "        open(\"./sims/\"*expID*\"_run\"*string(iter)*\".txt\", \"a\") do file\n",
    "            write(file, msg)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    log(\"estimating U using teacher \"*string(teacher)*\" with beta \"*string(params.beta[teacher]))\n",
    "    \n",
    "    u_est = estimate_u(as, os, teacher, params.M, params.N, params.beta, params.umax)\n",
    "    d_est = estimate_d(as, os, params.K, params.N)\n",
    "    max_a, max_val = calc_max_arm(u_est, d_est)\n",
    "    \n",
    "    log(\"Estimated U: \"*string(u_est))\n",
    "    log(\"Estimated D: \"*string(d_est))\n",
    "    log(\"given U and D estimates, highest-reward arm is arm \"*string(max_a)*\" with reward \"*string(max_val))\n",
    "    \n",
    "    a = Action(\"C\"*string(max_a), false, max_a)\n",
    "    for t in t_explore+1:params.exp_steps\n",
    "        item = pull_arm(a.index, true_state)\n",
    "        o = Observation(true, item, invalid_p)\n",
    "        r = R(true_state, a)\n",
    "        r_accum = r_accum + r\n",
    "\n",
    "        msg = \"\\n\"*string(t)*\",C,\"*a.name*\",i\"*string(o.i)*\",\"*string(r)\n",
    "        open(\"./sims/\"*expID*\"_run\"*string(iter)*\".txt\", \"a\") do file\n",
    "            write(file, msg)\n",
    "        end\n",
    "    end\n",
    "    random_R[iter] = r_accum\n",
    "end\n",
    "\n",
    "log(\"ran \"*string(params.exp_iters)*\" naive policy rollouts for \"*string(params.exp_steps)*\" timesteps each\")\n",
    "log(\"Naive R: \"*string(random_R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random: Actions\n",
    "Selects actions uniformly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:23:23\tRunning experiment with ID base_rand_act_221114_172323\n"
     ]
    }
   ],
   "source": [
    "exp_name = \"base_rand_act_\"\n",
    "expID = exp_name * Dates.format(Dates.now(), \"yymd_HHMMS\")\n",
    "log(\"Running experiment with ID \" * expID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create POMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:23:25\tMyParameters\n",
      "  N: Int64 3\n",
      "  K: Int64 3\n",
      "  M: Int64 2\n",
      "  y: Float64 0.9\n",
      "  umax: Int64 10\n",
      "  u_grain: Int64 6\n",
      "  d_grain: Int64 6\n",
      "  beta: Array{Float64}((2,)) [0.01, 10.0]\n",
      "  exp_iters: Int64 2\n",
      "  exp_steps: Int64 1000\n",
      "  s_index: Int64 5003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if @isdefined args\n",
    "    @with_kw struct MyParameters\n",
    "        N::Int = convert(Int64, args[1])         # size of item set\n",
    "        K::Int = convert(Int64, args[2])         # size of arm set\n",
    "        M::Int = 2                               # size of beta set\n",
    "        y::Float64 = args[3]                     # discount factor\n",
    "        umax::Real = 10                          # max utility\n",
    "        u_grain::Int = convert(Int64, args[4])   # granularity of utility approximation\n",
    "        d_grain::Int = convert(Int64, args[5])   # granularity of arm distribution approximation\n",
    "        beta::Array{Float64} = [0.01, 10.0]      # teacher beta values\n",
    "        exp_iters::Int = convert(Int64, args[6]) # number of rollouts to run\n",
    "        exp_steps::Int = convert(Int64, args[7]) # number of timesteps per rollout\n",
    "        s_index::Int = convert(Int64, args[8])   # index of true state\n",
    "    end\n",
    "else\n",
    "    @with_kw struct MyParameters\n",
    "        N::Int = parse(Int64, ARGS[1])           # size of item set\n",
    "        K::Int = parse(Int64, ARGS[2])           # size of arm set\n",
    "        M::Int = 2                               # size of beta set\n",
    "        y::Float64 = parse(Float64, ARGS[3])     # discount factor\n",
    "        umax::Real = 10                          # max utility\n",
    "        u_grain::Int = parse(Int64, ARGS[4])     # granularity of utility approximation\n",
    "        d_grain::Int = parse(Int64, ARGS[5])     # granularity of arm distribution approximation\n",
    "        beta::Array{Float64} = [0.01, 10.0]      # teacher beta values\n",
    "        exp_iters::Int = parse(Int64, ARGS[6])   # number of rollouts to run\n",
    "        exp_steps::Int = parse(Int64, ARGS[7])   # number of timesteps per rollout\n",
    "        s_index::Int = parse(Int64, ARGS[8])     # index of true state\n",
    "    end\n",
    "end\n",
    "\n",
    "params = MyParameters()\n",
    "log(string(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:23:27\tgenerated 216 utilities (each length 3 items)\n",
      "17:23:27\tgenerated 9261 arm distribution sets (each shape 3 arms x 3 items)\n",
      "17:23:27\tgenerated 1 beta value sets (each length 2 teachers)\n",
      "17:23:28\tgenerated 2000376 states\n",
      "17:23:28\tgenerated 5 actions\n",
      "17:23:28\tgenerated transition function\n",
      "17:23:28\tgenerated reward function\n",
      "17:23:28\tgenerated 21 observations\n",
      "17:23:28\tgenerated observation function\n",
      "17:23:28\tcreated POMDP\n"
     ]
    }
   ],
   "source": [
    "struct State\n",
    "    u::Array{Float64}         # list of N utility values for N items\n",
    "    d::Array{Array{Float64}}  # list of K arm distributions, each assigning probabilities to N items\n",
    "    b::Array{Float64}         # list of M beta values\n",
    "end\n",
    "\n",
    "# space of utility functions\n",
    "umin = 0\n",
    "grid_coor = fill(range(umin, params.umax, length=params.u_grain), params.N)\n",
    "U = RectangleGrid(grid_coor...)\n",
    "\n",
    "@assert length(U[1]) == params.N\n",
    "log(\"generated \" * string(length(U)) * \" utilities (each length \" * string(length(U[1])) * \" items)\")\n",
    "\n",
    "function generate_probability_distributions(N::Int, coor::Array{Float64}, S::Float64=1.0)\n",
    "    if S == 0\n",
    "        return [[0.0 for _ in 1:N]]\n",
    "    end\n",
    "    if N == 1\n",
    "        return [[float(S)]]\n",
    "    end\n",
    "    out = []\n",
    "    range = coor[1:findall(x -> isapprox(x, S, atol=1e-15), coor)[1]]\n",
    "    for k in range\n",
    "        subsolution = generate_probability_distributions(N - 1, coor, S - k)\n",
    "        for lst in subsolution\n",
    "            if typeof(lst[1]) != Float64\n",
    "                log(\"ERROR: lst \" * string(lst) * \" has type \" * string(typeof(lst[1])) * \". Must be Float64.\")\n",
    "            end\n",
    "            prepend!(lst, float(k))\n",
    "        end\n",
    "        out = vcat(out, subsolution)\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "# space of arm distributions\n",
    "coor = collect(range(0.0, 1.0, length=params.d_grain))\n",
    "simplex_list = generate_probability_distributions(params.N, coor)\n",
    "D_tuples = vec(collect(Base.product(fill(simplex_list, params.K)...)))\n",
    "D = [collect(d) for d in D_tuples]\n",
    "\n",
    "@assert length(D[1]) == params.K\n",
    "@assert length(D[1][1]) == params.N\n",
    "log(\"generated \" * string(length(D)) * \" arm distribution sets (each shape \" * string(length(D[1])) * \" arms x \" * string(length(D[1][1])) * \" items)\")\n",
    "\n",
    "# beta values\n",
    "B = [params.beta]\n",
    "\n",
    "# each beta value set must be length M\n",
    "@assert length(B[1]) == params.M\n",
    "log(\"generated \" * string(length(B)) * \" beta value sets (each length \" * string(length(B[1])) * \" teachers)\")\n",
    "\n",
    "# State space\n",
    "S = [[State(u, d, b) for u in U, d in D, b in B]...,]\n",
    "\n",
    "log(\"generated \" * string(length(S)) * \" states\")\n",
    "\n",
    "# Action space - actions are arm choices (K) or beta selections (M)\n",
    "struct Action\n",
    "    name::String      # valid names are {B,C} + index\n",
    "    isBeta::Bool      # true if 'B' action, false if 'C' action\n",
    "    index::Integer    # index of beta (if 'B' action) or arm choice (if 'C' action)\n",
    "end\n",
    "\n",
    "A = Array{Action}(undef, params.K + params.M)\n",
    "for i in 1:params.K+params.M\n",
    "    if i <= params.K\n",
    "        A[i] = Action(\"C\" * string(i), false, i)\n",
    "    else\n",
    "        A[i] = Action(\"B\" * string(i - params.K), true, i - params.K)\n",
    "    end\n",
    "end\n",
    "log(\"generated \" * string(length(A)) * \" actions\")\n",
    "\n",
    "# Transition function\n",
    "function T(s::State, a::Action)\n",
    "    return SparseCat([s], [1.0])    # categorical distribution\n",
    "end\n",
    "log(\"generated transition function\")\n",
    "\n",
    "# Reward function\n",
    "function R(s::State, a::Action)\n",
    "    # if beta selected, return 0\n",
    "    if a.isBeta\n",
    "        return 0\n",
    "        # if arm pulled, return that arm's avg utility\n",
    "    else\n",
    "        utilities = s.u\n",
    "        arm_dist = s.d[a.index]\n",
    "        return dot(utilities, arm_dist)\n",
    "    end\n",
    "end\n",
    "log(\"generated reward function\")\n",
    "\n",
    "# item space\n",
    "I = 1:params.N\n",
    "\n",
    "# preference space\n",
    "struct Preference\n",
    "    i0::Int    # first item to compare, in {1,2,...,N}\n",
    "    i1::Int    # second item to compare, in {1,2,...,N}\n",
    "    label::Int # feedback label, in {0,1}\n",
    "end\n",
    "\n",
    "P = [[Preference(i0, i1, label) for i0 in I, i1 in I, label in [0, 1]]...,]\n",
    "\n",
    "# observation space\n",
    "struct Observation\n",
    "    isItem::Bool    # true if item returned, false otherwise\n",
    "    i::Int          # item, if item returned\n",
    "    p::Preference   # preference, if preference returned\n",
    "end\n",
    "\n",
    "invalid_i = -1\n",
    "invalid_p = Preference(-1, -1, -1)\n",
    "I_obs = [Observation(true, i, invalid_p) for i in I]\n",
    "P_obs = [Observation(false, invalid_i, p) for p in P]\n",
    "omega = union(I_obs, P_obs)\n",
    "\n",
    "log(\"generated \" * string(length(omega)) * \" observations\")\n",
    "\n",
    "# unnormalized query profile (likelihood of querying 1,1; 2,1; 3,1; ... ; N,1; 1,2; 2,2; ... ; N,N)\n",
    "Q = ones(params.N * params.N)\n",
    "\n",
    "# preference probability (expected preference, or probability that preference=1)\n",
    "function Pr(p::Preference, s::State, b::Float64)\n",
    "    prob_pref_1 = exp(Float64(b) * s.u[p.i1]) / (exp(Float64(b) * s.u[p.i1]) + exp(Float64(b) * s.u[p.i0]))\n",
    "    if p.label == 1\n",
    "        return prob_pref_1\n",
    "    else\n",
    "        return 1.0 - prob_pref_1\n",
    "    end\n",
    "end\n",
    "\n",
    "function O(s::State, a::Action, sp::State)\n",
    "    # if B action, obs in P_obs\n",
    "    if a.isBeta\n",
    "        prob_of_pref = [Pr(o.p, s, s.b[a.index]) for o in P_obs]\n",
    "        prob_of_query = vcat(Q, Q)   # doubled because each query appears once for each label\n",
    "\n",
    "        # weight by querying profile to get dist\n",
    "        dist = [prob_of_pref[i] * prob_of_query[i] for i in 1:length(prob_of_pref)]\n",
    "        normalized_dist = dist / sum(dist)\n",
    "        return SparseCat(P_obs, normalized_dist)\n",
    "        # if C action, obs in I_obs\n",
    "    else\n",
    "        return SparseCat(I_obs, s.d[a.index])\n",
    "    end\n",
    "end\n",
    "\n",
    "log(\"generated observation function\")\n",
    "\n",
    "# define POMDP\n",
    "abstract type MyPOMDP <: POMDP{State,Action,Observation} end\n",
    "pomdp = QuickPOMDP(MyPOMDP,\n",
    "    states=S,\n",
    "    actions=A,\n",
    "    observations=omega,\n",
    "    transition=T,\n",
    "    observation=O,\n",
    "    reward=R,\n",
    "    discount=params.y,\n",
    "    initialstate=S);\n",
    "\n",
    "log(\"created POMDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate random rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:23:31\ttrue state State([8.0, 10.0, 0.0], Array{Float64}[[0.0, 0.4, 0.6], [0.0, 0.2, 0.8], [0.0, 0.0, 1.0]], [0.01, 10.0])\n",
      "17:23:31\tlogging random action simulation 2 to ./sims/base_rand_act_221114_172323_run1.txt\n",
      "17:23:32\tlogging random action simulation 2 to ./sims/base_rand_act_221114_172323_run2.txt\n",
      "17:23:32\tran 2 random rollouts for 1000 timesteps each\n",
      "17:23:32\tRandom R: [1200.0, 1208.0]\n"
     ]
    }
   ],
   "source": [
    "prior = Uniform(S)\n",
    "sim = RolloutSimulator(max_steps=params.exp_steps)\n",
    "true_state = S[params.s_index]\n",
    "log(\"true state \"*string(true_state))\n",
    "\n",
    "random_R = zeros(params.exp_iters)\n",
    "for iter in 1:params.exp_iters\n",
    "    log(\"logging random action simulation \"*string(params.exp_iters)*\" to \"*\"./sims/\"*expID*\"_run\"*string(iter)*\".txt\")\n",
    "    t = 1\n",
    "    r_accum = 0.\n",
    "    policy = RandomPolicy(pomdp)\n",
    "    for (s, a, o, r) in stepthrough(pomdp, policy, updater(policy), Uniform(S), true_state, \"s,a,o,r\", max_steps=params.exp_steps)\n",
    "        r_accum = r_accum + r\n",
    "        if t == 1\n",
    "            open(\"./sims/\"*expID*\"_run\"*string(iter)*\".txt\", \"w\") do file\n",
    "                write(file, string(s))\n",
    "            end\n",
    "        end\n",
    "        if a.isBeta\n",
    "            msg = \"\\n\"*string(t)*\",B,\"*a.name*\",(i\"*string(o.p.i0)*\"-i\"*string(o.p.i1)*\";\"*string(o.p.label)*\"),\"*string(r)\n",
    "        else\n",
    "            msg = \"\\n\"*string(t)*\",C,\"*a.name*\",i\"*string(o.i)*\",\"*string(r)\n",
    "        end\n",
    "        open(\"./sims/\"*expID*\"_run\"*string(iter)*\".txt\", \"a\") do file\n",
    "            write(file, msg)\n",
    "        end\n",
    "        t = t + 1\n",
    "    end\n",
    "    random_R[iter] = r_accum\n",
    "end\n",
    "\n",
    "log(\"ran \"*string(params.exp_iters)*\" random rollouts for \"*string(params.exp_steps)*\" timesteps each\")\n",
    "log(\"Random R: \"*string(random_R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random: Teacher\n",
    "Can choose whether or not to query teacher, but can't choose which teacher to query. When queries teacher, teacher beta sampled uniformly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:23:34\tRunning experiment with ID base_rand_B_221114_172334\n"
     ]
    }
   ],
   "source": [
    "exp_name = \"base_rand_B_\"\n",
    "expID = exp_name * Dates.format(Dates.now(), \"yymd_HHMMS\")\n",
    "log(\"Running experiment with ID \" * expID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:23:40\tMyParameters\n",
      "  N: Int64 3\n",
      "  K: Int64 3\n",
      "  M: Int64 2\n",
      "  y: Float64 0.9\n",
      "  umax: Int64 10\n",
      "  u_grain: Int64 6\n",
      "  d_grain: Int64 6\n",
      "  beta: Array{Float64}((2,)) [0.01, 10.0]\n",
      "  exp_iters: Int64 2\n",
      "  exp_steps: Int64 1000\n",
      "  s_index: Int64 5003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if @isdefined args\n",
    "    @with_kw struct MyParameters\n",
    "        N::Int = convert(Int64, args[1])         # size of item set\n",
    "        K::Int = convert(Int64, args[2])         # size of arm set\n",
    "        M::Int = 2                               # size of beta set\n",
    "        y::Float64 = args[3]                     # discount factor\n",
    "        umax::Real = 10                          # max utility\n",
    "        u_grain::Int = convert(Int64, args[4])   # granularity of utility approximation\n",
    "        d_grain::Int = convert(Int64, args[5])   # granularity of arm distribution approximation\n",
    "        beta::Array{Float64} = [0.01, 10.0]      # teacher beta values\n",
    "        exp_iters::Int = convert(Int64, args[6]) # number of rollouts to run\n",
    "        exp_steps::Int = convert(Int64, args[7]) # number of timesteps per rollout\n",
    "        s_index::Int = convert(Int64, args[8])   # index of true state\n",
    "    end\n",
    "else\n",
    "    @with_kw struct MyParameters\n",
    "        N::Int = parse(Int64, ARGS[1])           # size of item set\n",
    "        K::Int = parse(Int64, ARGS[2])           # size of arm set\n",
    "        M::Int = 2                               # size of beta set\n",
    "        y::Float64 = parse(Float64, ARGS[3])     # discount factor\n",
    "        umax::Real = 10                          # max utility\n",
    "        u_grain::Int = parse(Int64, ARGS[4])     # granularity of utility approximation\n",
    "        d_grain::Int = parse(Int64, ARGS[5])     # granularity of arm distribution approximation\n",
    "        beta::Array{Float64} = [0.01, 10.0]      # teacher beta values\n",
    "        exp_iters::Int = parse(Int64, ARGS[6])   # number of rollouts to run\n",
    "        exp_steps::Int = parse(Int64, ARGS[7])   # number of timesteps per rollout\n",
    "        s_index::Int = parse(Int64, ARGS[8])     # index of true state\n",
    "    end\n",
    "end\n",
    "\n",
    "params = MyParameters()\n",
    "log(string(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:23:42\tgenerated 216 utilities (each length 3 items)\n",
      "17:23:42\tgenerated 9261 arm distribution sets (each shape 3 arms x 3 items)\n",
      "17:23:42\tgenerated 1 beta value sets (each length 2 teachers)\n",
      "17:23:43\tgenerated 2000376 states\n",
      "17:23:43\tgenerated 4 actions\n",
      "17:23:43\tgenerated transition function\n",
      "17:23:43\tgenerated reward function\n",
      "17:23:43\tgenerated 21 observations\n",
      "17:23:43\tgenerated observation function\n",
      "17:23:43\tcreated POMDP\n"
     ]
    }
   ],
   "source": [
    "struct State\n",
    "    u::Array{Float64}         # list of N utility values for N items\n",
    "    d::Array{Array{Float64}}  # list of K arm distributions, each assigning probabilities to N items\n",
    "    b::Array{Float64}         # list of M beta values\n",
    "end\n",
    "\n",
    "# space of utility functions\n",
    "umin = 0\n",
    "grid_coor = fill(range(umin, params.umax, length=params.u_grain), params.N)\n",
    "U = RectangleGrid(grid_coor...)\n",
    "\n",
    "@assert length(U[1]) == params.N\n",
    "log(\"generated \" * string(length(U)) * \" utilities (each length \" * string(length(U[1])) * \" items)\")\n",
    "\n",
    "function generate_probability_distributions(N::Int, coor::Array{Float64}, S::Float64=1.0)\n",
    "    if S == 0\n",
    "        return [[0.0 for _ in 1:N]]\n",
    "    end\n",
    "    if N == 1\n",
    "        return [[float(S)]]\n",
    "    end\n",
    "    out = []\n",
    "    range = coor[1:findall(x -> isapprox(x, S, atol=1e-15), coor)[1]]\n",
    "    for k in range\n",
    "        subsolution = generate_probability_distributions(N - 1, coor, S - k)\n",
    "        for lst in subsolution\n",
    "            if typeof(lst[1]) != Float64\n",
    "                log(\"ERROR: lst \" * string(lst) * \" has type \" * string(typeof(lst[1])) * \". Must be Float64.\")\n",
    "            end\n",
    "            prepend!(lst, float(k))\n",
    "        end\n",
    "        out = vcat(out, subsolution)\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "# space of arm distributions\n",
    "coor = collect(range(0.0, 1.0, length=params.d_grain))\n",
    "simplex_list = generate_probability_distributions(params.N, coor)\n",
    "D_tuples = vec(collect(Base.product(fill(simplex_list, params.K)...)))\n",
    "D = [collect(d) for d in D_tuples]\n",
    "\n",
    "@assert length(D[1]) == params.K\n",
    "@assert length(D[1][1]) == params.N\n",
    "log(\"generated \" * string(length(D)) * \" arm distribution sets (each shape \" * string(length(D[1])) * \" arms x \" * string(length(D[1][1])) * \" items)\")\n",
    "\n",
    "# beta values\n",
    "B = [params.beta]\n",
    "\n",
    "# each beta value set must be length M\n",
    "@assert length(B[1]) == params.M\n",
    "log(\"generated \" * string(length(B)) * \" beta value sets (each length \" * string(length(B[1])) * \" teachers)\")\n",
    "\n",
    "# State space\n",
    "S = [[State(u, d, b) for u in U, d in D, b in B]...,]\n",
    "\n",
    "log(\"generated \" * string(length(S)) * \" states\")\n",
    "\n",
    "# Action space - actions are arm choices (K) or beta selections (M)\n",
    "struct Action\n",
    "    name::String      # valid names are {B,C} + index\n",
    "    isBeta::Bool      # true if 'B' action, false if 'C' action\n",
    "    index::Integer    # index of beta (if 'B' action) or arm choice (if 'C' action)\n",
    "end\n",
    "\n",
    "A = Array{Action}(undef, params.K + 1)\n",
    "for i in 1:params.K+1\n",
    "    if i <= params.K\n",
    "        A[i] = Action(\"C\" * string(i), false, i)\n",
    "    else\n",
    "        # only *ONE* beta action\n",
    "        A[i] = Action(\"B\", true, 1)\n",
    "    end\n",
    "end\n",
    "log(\"generated \" * string(length(A)) * \" actions\")\n",
    "\n",
    "# Transition function\n",
    "function T(s::State, a::Action)\n",
    "    return SparseCat([s], [1.0])    # categorical distribution\n",
    "end\n",
    "log(\"generated transition function\")\n",
    "\n",
    "# Reward function\n",
    "function R(s::State, a::Action)\n",
    "    # if beta selected, return 0\n",
    "    if a.isBeta\n",
    "        return 0\n",
    "        # if arm pulled, return that arm's avg utility\n",
    "    else\n",
    "        utilities = s.u\n",
    "        arm_dist = s.d[a.index]\n",
    "        return dot(utilities, arm_dist)\n",
    "    end\n",
    "end\n",
    "log(\"generated reward function\")\n",
    "\n",
    "# item space\n",
    "I = 1:params.N\n",
    "\n",
    "# preference space\n",
    "struct Preference\n",
    "    i0::Int    # first item to compare, in {1,2,...,N}\n",
    "    i1::Int    # second item to compare, in {1,2,...,N}\n",
    "    label::Int # feedback label, in {0,1}\n",
    "end\n",
    "\n",
    "P = [[Preference(i0, i1, label) for i0 in I, i1 in I, label in [0, 1]]...,]\n",
    "\n",
    "# observation space\n",
    "struct Observation\n",
    "    isItem::Bool    # true if item returned, false otherwise\n",
    "    i::Int          # item, if item returned\n",
    "    p::Preference   # preference, if preference returned\n",
    "end\n",
    "\n",
    "invalid_i = -1\n",
    "invalid_p = Preference(-1, -1, -1)\n",
    "I_obs = [Observation(true, i, invalid_p) for i in I]\n",
    "P_obs = [Observation(false, invalid_i, p) for p in P]\n",
    "omega = union(I_obs, P_obs)\n",
    "\n",
    "log(\"generated \" * string(length(omega)) * \" observations\")\n",
    "\n",
    "# unnormalized query profile (likelihood of querying 1,1; 2,1; 3,1; ... ; N,1; 1,2; 2,2; ... ; N,N)\n",
    "Q = ones(params.N * params.N)\n",
    "\n",
    "# preference probability (expected preference, or probability that preference=1)\n",
    "function Pr(p::Preference, s::State, b::Float64)\n",
    "    prob_pref_1 = exp(Float64(b) * s.u[p.i1]) / (exp(Float64(b) * s.u[p.i1]) + exp(Float64(b) * s.u[p.i0]))\n",
    "    if p.label == 1\n",
    "        return prob_pref_1\n",
    "    else\n",
    "        return 1.0 - prob_pref_1\n",
    "    end\n",
    "end\n",
    "\n",
    "function O(s::State, a::Action, sp::State)\n",
    "    # if B action, obs in P_obs\n",
    "    if a.isBeta\n",
    "        # choose beta *RANDOMLY* from s.b\n",
    "        b = s.b[rand(1:end)]\n",
    "        prob_of_pref = [Pr(o.p, s, b) for o in P_obs]\n",
    "        prob_of_query = vcat(Q, Q)   # doubled because each query appears once for each label\n",
    "\n",
    "        # weight by querying profile to get dist\n",
    "        dist = [prob_of_pref[i] * prob_of_query[i] for i in 1:length(prob_of_pref)]\n",
    "        normalized_dist = dist / sum(dist)\n",
    "        return SparseCat(P_obs, normalized_dist)\n",
    "        # if C action, obs in I_obs\n",
    "    else\n",
    "        return SparseCat(I_obs, s.d[a.index])\n",
    "    end\n",
    "end\n",
    "\n",
    "log(\"generated observation function\")\n",
    "\n",
    "# define POMDP\n",
    "abstract type MyPOMDP <: POMDP{State,Action,Observation} end\n",
    "pomdp = QuickPOMDP(MyPOMDP,\n",
    "    states=S,\n",
    "    actions=A,\n",
    "    observations=omega,\n",
    "    transition=T,\n",
    "    observation=O,\n",
    "    reward=R,\n",
    "    discount=params.y,\n",
    "    initialstate=S);\n",
    "\n",
    "log(\"created POMDP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:23:47\tsolved POMDP\n",
      "17:23:47\ttrue state State([8.0, 10.0, 0.0], Array{Float64}[[0.0, 0.4, 0.6], [0.0, 0.2, 0.8], [0.0, 0.0, 1.0]], [0.01, 10.0])\n",
      "17:23:47\tlogging random teacher simulation 1 to ./sims/base_rand_B_221114_172334_run1.txt\n",
      "17:25:12\tlogging random teacher simulation 2 to ./sims/base_rand_B_221114_172334_run2.txt\n",
      "17:26:37\tran 2 random teacher rollouts for 1000 timesteps each\n",
      "17:26:37\tPOMCPOW + random teacher selection R: [1668.0, 2386.0]\n"
     ]
    }
   ],
   "source": [
    "solver = POMCPOWSolver()\n",
    "planner = solve(solver, pomdp);\n",
    "log(\"solved POMDP\")\n",
    "\n",
    "true_state = S[params.s_index]\n",
    "log(\"true state \"*string(true_state))\n",
    "\n",
    "POMCPOW_R = Array{Float64}(undef, params.exp_iters)\n",
    "beliefs = Array{Array{ParticleFilters.ParticleCollection{State}}}(undef, (params.exp_iters, params.exp_steps))\n",
    "for iter in 1:params.exp_iters\n",
    "    log(\"logging random teacher simulation \"*string(iter)*\" to \"*\"./sims/\"*expID*\"_run\"*string(iter)*\".txt\")\n",
    "    t = 1\n",
    "    r_accum = 0.\n",
    "    beliefs_iter = Array{ParticleFilters.ParticleCollection{State}}(undef, params.exp_steps)\n",
    "    for (s, a, o, r, b) in stepthrough(pomdp, planner, updater(planner), Uniform(S), true_state, \"s,a,o,r,b\", max_steps=params.exp_steps)\n",
    "        r_accum = r_accum + r\n",
    "        beliefs_iter[t] = b\n",
    "        if t == 1\n",
    "            open(\"./sims/\"*expID*\"_run\"*string(iter)*\".txt\", \"w\") do file\n",
    "                write(file, string(s))\n",
    "            end\n",
    "        end\n",
    "        if a.isBeta\n",
    "            msg = \"\\n\"*string(t)*\",B,\"*a.name*\",(i\"*string(o.p.i0)*\"-i\"*string(o.p.i1)*\";\"*string(o.p.label)*\"),\"*string(r)\n",
    "        else\n",
    "            msg = \"\\n\"*string(t)*\",C,\"*a.name*\",i\"*string(o.i)*\",\"*string(r)\n",
    "        end\n",
    "        open(\"./sims/\"*expID*\"_run\"*string(iter)*\".txt\", \"a\") do file\n",
    "            write(file, msg)\n",
    "        end\n",
    "        t = t + 1\n",
    "    end\n",
    "    beliefs[iter] = beliefs_iter\n",
    "    POMCPOW_R[iter] = r_accum\n",
    "end\n",
    "    \n",
    "log(\"ran \"*string(params.exp_iters)*\" random teacher rollouts for \"*string(params.exp_steps)*\" timesteps each\")\n",
    "log(\"POMCPOW + random teacher selection R: \"*string(POMCPOW_R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
