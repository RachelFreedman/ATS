01:46:02	Running experiment with ID naive_100_23420_01462
01:46:02	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 4589
  t_explore: Int64 100
  teacher: Int64 2
  seed: Int64 1

01:46:02	will explore for first 100 timesteps
01:46:02	will estimate based on feedback from teacher 2 with beta 0.01
01:46:02	generated 27 utilities (each length 3 items)
01:46:03	generated 216 arm distribution sets (each shape 3 arms x 3 items)
01:46:03	generated 1 beta value sets (each length 3 teachers)
01:46:04	generated 5832000 states, 5832 of which are potential start states
01:46:04	generated 6 actions
01:46:04	generated reward function
01:46:04	generated 21 observations
01:46:04	generated observation function
01:46:04	true state State(1000, [3.0, 8.0, 8.0], Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]], [0.0, 0.01, 50.0])
01:46:05	logging naive policy simulation 1 to ./sims/naive_100_23420_01462_run1.txt
01:46:05	estimating U using teacher 2 with beta 0.01
01:46:05	Estimated U: [8.0, -5.293289359658068, -2.0]
01:46:05	True U: [3.0, 8.0, 8.0]
01:46:05	Estimated D: Any[[0.0, 0.47058823529411764, 0.5294117647058824], [0.6111111111111112, 0.3888888888888889, 0.0], [0.23529411764705882, 0.7647058823529411, 0.0]]
01:46:05	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:05	given U and D estimates, highest-reward arm is arm 2 with reward 4.830387471244085
01:46:05	logging naive policy simulation 2 to ./sims/naive_100_23420_01462_run2.txt
01:46:05	estimating U using teacher 2 with beta 0.01
01:46:05	Estimated U: [8.0, -10.479969065549497, -2.0]
01:46:05	True U: [3.0, 8.0, 8.0]
01:46:05	Estimated D: Any[[0.0, 0.5128205128205128, 0.48717948717948717], [0.45161290322580644, 0.5483870967741935, 0.0], [0.38461538461538464, 0.6153846153846154, 0.0]]
01:46:05	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:05	given U and D estimates, highest-reward arm is arm 2 with reward -0.13417658433359525
01:46:05	logging naive policy simulation 3 to ./sims/naive_100_23420_01462_run3.txt
01:46:05	estimating U using teacher 2 with beta 0.01
01:46:05	Estimated U: [-3.051224840856423, -2.0, 8.0]
01:46:05	True U: [3.0, 8.0, 8.0]
01:46:05	Estimated D: Any[[0.0, 0.5172413793103449, 0.4827586206896552], [0.5531914893617021, 0.44680851063829785, 0.0], [0.4107142857142857, 0.5892857142857143, 0.0]]
01:46:05	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:05	given U and D estimates, highest-reward arm is arm 1 with reward 4.827586206896552
01:46:05	logging naive policy simulation 4 to ./sims/naive_100_23420_01462_run4.txt
01:46:05	estimating U using teacher 2 with beta 0.01
01:46:05	Estimated U: [-5.662242164839556, -2.0, 8.000000000000002]
01:46:05	True U: [3.0, 8.0, 8.0]
01:46:05	Estimated D: Any[[0.0, 0.5131578947368421, 0.4868421052631579], [0.5074626865671642, 0.4925373134328358, 0.0], [0.43661971830985913, 0.5633802816901409, 0.0]]
01:46:05	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:05	given U and D estimates, highest-reward arm is arm 1 with reward 4.86842105263158
01:46:05	logging naive policy simulation 5 to ./sims/naive_100_23420_01462_run5.txt
01:46:05	estimating U using teacher 2 with beta 0.01
01:46:05	Estimated U: [-3.2305041617314134, -2.0, 8.0]
01:46:05	True U: [3.0, 8.0, 8.0]
01:46:05	Estimated D: Any[[0.0, 0.5, 0.5], [0.46153846153846156, 0.5384615384615384, 0.0], [0.4691358024691358, 0.5308641975308642, 0.0]]
01:46:05	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:05	given U and D estimates, highest-reward arm is arm 1 with reward 5.0
01:46:05	logging naive policy simulation 6 to ./sims/naive_100_23420_01462_run6.txt
01:46:05	estimating U using teacher 2 with beta 0.01
01:46:05	Estimated U: [-2.0, -2.0, 8.0]
01:46:05	True U: [3.0, 8.0, 8.0]
01:46:05	Estimated D: Any[[0.0, 0.509090909090909, 0.4909090909090909], [0.47058823529411764, 0.5294117647058824, 0.0], [0.4895833333333333, 0.5104166666666666, 0.0]]
01:46:05	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:05	given U and D estimates, highest-reward arm is arm 1 with reward 4.909090909090909
01:46:05	logging naive policy simulation 7 to ./sims/naive_100_23420_01462_run7.txt
01:46:05	estimating U using teacher 2 with beta 0.01
01:46:05	Estimated U: [-2.5842254323719906, -2.0, 8.0]
01:46:05	True U: [3.0, 8.0, 8.0]
01:46:05	Estimated D: Any[[0.0, 0.5075757575757576, 0.49242424242424243], [0.4482758620689655, 0.5517241379310345, 0.0], [0.5229357798165137, 0.47706422018348627, 0.0]]
01:46:05	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:05	given U and D estimates, highest-reward arm is arm 1 with reward 4.924242424242424
01:46:05	logging naive policy simulation 8 to ./sims/naive_100_23420_01462_run8.txt
01:46:05	estimating U using teacher 2 with beta 0.01
01:46:05	Estimated U: [-5.262619350938867, -2.0, 8.0]
01:46:05	True U: [3.0, 8.0, 8.0]
01:46:05	Estimated D: Any[[0.0, 0.4900662251655629, 0.5099337748344371], [0.45112781954887216, 0.5488721804511278, 0.0], [0.52, 0.48, 0.0]]
01:46:05	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:05	given U and D estimates, highest-reward arm is arm 1 with reward 5.099337748344372
01:46:05	logging naive policy simulation 9 to ./sims/naive_100_23420_01462_run9.txt
01:46:05	estimating U using teacher 2 with beta 0.01
01:46:05	Estimated U: [-8.269738042937394, -2.0, 8.0]
01:46:05	True U: [3.0, 8.0, 8.0]
01:46:05	Estimated D: Any[[0.0, 0.5060240963855421, 0.4939759036144578], [0.4645161290322581, 0.535483870967742, 0.0], [0.5214285714285715, 0.4785714285714286, 0.0]]
01:46:05	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:05	given U and D estimates, highest-reward arm is arm 1 with reward 4.9397590361445785
01:46:05	logging naive policy simulation 10 to ./sims/naive_100_23420_01462_run10.txt
01:46:05	estimating U using teacher 2 with beta 0.01
01:46:05	Estimated U: [-3.9946979836276, -2.0, 8.0]
01:46:05	True U: [3.0, 8.0, 8.0]
01:46:05	Estimated D: Any[[0.0, 0.521978021978022, 0.47802197802197804], [0.48255813953488375, 0.5174418604651163, 0.0], [0.50625, 0.49375, 0.0]]
01:46:05	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:05	given U and D estimates, highest-reward arm is arm 1 with reward 4.78021978021978
01:46:05	logging naive policy simulation 11 to ./sims/naive_100_23420_01462_run11.txt
01:46:05	estimating U using teacher 2 with beta 0.01
01:46:05	Estimated U: [-0.11652445413449342, -2.0, 8.0]
01:46:05	True U: [3.0, 8.0, 8.0]
01:46:05	Estimated D: Any[[0.0, 0.525, 0.475], [0.4712041884816754, 0.5287958115183246, 0.0], [0.4887640449438202, 0.5112359550561798, 0.0]]
01:46:05	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:05	given U and D estimates, highest-reward arm is arm 1 with reward 4.75
01:46:05	logging naive policy simulation 12 to ./sims/naive_100_23420_01462_run12.txt
01:46:05	estimating U using teacher 2 with beta 0.01
01:46:05	Estimated U: [-3.0324742851907547, -2.0, 8.0]
01:46:05	True U: [3.0, 8.0, 8.0]
01:46:05	Estimated D: Any[[0.0, 0.536697247706422, 0.463302752293578], [0.45365853658536587, 0.5463414634146342, 0.0], [0.48205128205128206, 0.517948717948718, 0.0]]
01:46:05	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:05	given U and D estimates, highest-reward arm is arm 1 with reward 4.63302752293578
01:46:05	logging naive policy simulation 13 to ./sims/naive_100_23420_01462_run13.txt
01:46:05	estimating U using teacher 2 with beta 0.01
01:46:05	Estimated U: [-4.396385782609837, -2.0, 8.0]
01:46:05	True U: [3.0, 8.0, 8.0]
01:46:05	Estimated D: Any[[0.0, 0.5148936170212766, 0.4851063829787234], [0.4434389140271493, 0.5565610859728507, 0.0], [0.4928909952606635, 0.5071090047393365, 0.0]]
01:46:05	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:05	given U and D estimates, highest-reward arm is arm 1 with reward 4.8510638297872335
01:46:06	logging naive policy simulation 14 to ./sims/naive_100_23420_01462_run14.txt
01:46:06	estimating U using teacher 2 with beta 0.01
01:46:06	Estimated U: [-8.408182941281884, -2.0, 7.999999999999998]
01:46:06	True U: [3.0, 8.0, 8.0]
01:46:06	Estimated D: Any[[0.0, 0.5040650406504065, 0.4959349593495935], [0.4411764705882353, 0.5588235294117647, 0.0], [0.49107142857142855, 0.5089285714285714, 0.0]]
01:46:06	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:06	given U and D estimates, highest-reward arm is arm 1 with reward 4.959349593495934
01:46:06	logging naive policy simulation 15 to ./sims/naive_100_23420_01462_run15.txt
01:46:06	estimating U using teacher 2 with beta 0.01
01:46:06	Estimated U: [-7.052003136485241, -2.0, 8.0]
01:46:06	True U: [3.0, 8.0, 8.0]
01:46:06	Estimated D: Any[[0.0, 0.5038461538461538, 0.49615384615384617], [0.4362934362934363, 0.5637065637065637, 0.0], [0.49586776859504134, 0.5041322314049587, 0.0]]
01:46:06	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:06	given U and D estimates, highest-reward arm is arm 1 with reward 4.961538461538462
01:46:06	logging naive policy simulation 16 to ./sims/naive_100_23420_01462_run16.txt
01:46:06	estimating U using teacher 2 with beta 0.01
01:46:06	Estimated U: [-7.151553740982959, -2.0, 8.0]
01:46:06	True U: [3.0, 8.0, 8.0]
01:46:06	Estimated D: Any[[0.0, 0.5053763440860215, 0.4946236559139785], [0.44981412639405205, 0.550185873605948, 0.0], [0.4940239043824701, 0.5059760956175299, 0.0]]
01:46:06	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:06	given U and D estimates, highest-reward arm is arm 1 with reward 4.946236559139785
01:46:06	logging naive policy simulation 17 to ./sims/naive_100_23420_01462_run17.txt
01:46:06	estimating U using teacher 2 with beta 0.01
01:46:06	Estimated U: [-6.971402509992169, -2.0, 7.999999999999998]
01:46:06	True U: [3.0, 8.0, 8.0]
01:46:06	Estimated D: Any[[0.0, 0.5117056856187291, 0.4882943143812709], [0.4574468085106383, 0.5425531914893617, 0.0], [0.5055350553505535, 0.4944649446494465, 0.0]]
01:46:06	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:06	given U and D estimates, highest-reward arm is arm 1 with reward 4.882943143812708
01:46:06	logging naive policy simulation 18 to ./sims/naive_100_23420_01462_run18.txt
01:46:06	estimating U using teacher 2 with beta 0.01
01:46:06	Estimated U: [-5.898981102228494, -2.0, 8.0]
01:46:06	True U: [3.0, 8.0, 8.0]
01:46:06	Estimated D: Any[[0.0, 0.5062893081761006, 0.4937106918238994], [0.4542483660130719, 0.545751633986928, 0.0], [0.5106382978723404, 0.48936170212765956, 0.0]]
01:46:06	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:06	given U and D estimates, highest-reward arm is arm 1 with reward 4.937106918238994
01:46:06	logging naive policy simulation 19 to ./sims/naive_100_23420_01462_run19.txt
01:46:06	estimating U using teacher 2 with beta 0.01
01:46:06	Estimated U: [-7.943149895147513, -2.0, 8.0]
01:46:06	True U: [3.0, 8.0, 8.0]
01:46:06	Estimated D: Any[[0.0, 0.4955223880597015, 0.5044776119402985], [0.46153846153846156, 0.5384615384615384, 0.0], [0.5033783783783784, 0.4966216216216216, 0.0]]
01:46:06	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:06	given U and D estimates, highest-reward arm is arm 1 with reward 5.044776119402985
01:46:06	logging naive policy simulation 20 to ./sims/naive_100_23420_01462_run20.txt
01:46:06	estimating U using teacher 2 with beta 0.01
01:46:06	Estimated U: [-7.582413467959379, -2.0, 8.0]
01:46:06	True U: [3.0, 8.0, 8.0]
01:46:06	Estimated D: Any[[0.0, 0.5070028011204482, 0.49299719887955185], [0.4574780058651026, 0.5425219941348973, 0.0], [0.5064516129032258, 0.4935483870967742, 0.0]]
01:46:06	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:06	given U and D estimates, highest-reward arm is arm 1 with reward 4.9299719887955185
01:46:06	logging naive policy simulation 21 to ./sims/naive_100_23420_01462_run21.txt
01:46:06	estimating U using teacher 2 with beta 0.01
01:46:06	Estimated U: [-8.782110457827226, -2.0, 8.0]
01:46:06	True U: [3.0, 8.0, 8.0]
01:46:06	Estimated D: Any[[0.0, 0.5054054054054054, 0.4945945945945946], [0.4638888888888889, 0.5361111111111111, 0.0], [0.5015197568389058, 0.49848024316109424, 0.0]]
01:46:06	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:06	given U and D estimates, highest-reward arm is arm 1 with reward 4.945945945945946
01:46:06	logging naive policy simulation 22 to ./sims/naive_100_23420_01462_run22.txt
01:46:06	estimating U using teacher 2 with beta 0.01
01:46:06	Estimated U: [-10.418046464950736, -2.0, 7.999999999999998]
01:46:06	True U: [3.0, 8.0, 8.0]
01:46:06	Estimated D: Any[[0.0, 0.49740932642487046, 0.5025906735751295], [0.46296296296296297, 0.5370370370370371, 0.0], [0.5029585798816568, 0.4970414201183432, 0.0]]
01:46:06	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:06	given U and D estimates, highest-reward arm is arm 1 with reward 5.0259067357512945
01:46:06	logging naive policy simulation 23 to ./sims/naive_100_23420_01462_run23.txt
01:46:06	estimating U using teacher 2 with beta 0.01
01:46:06	Estimated U: [-11.076233453436966, -2.0, 8.0]
01:46:06	True U: [3.0, 8.0, 8.0]
01:46:06	Estimated D: Any[[0.0, 0.4975247524752475, 0.5024752475247525], [0.46683673469387754, 0.5331632653061225, 0.0], [0.5111731843575419, 0.4888268156424581, 0.0]]
01:46:06	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:06	given U and D estimates, highest-reward arm is arm 1 with reward 5.024752475247524
01:46:06	logging naive policy simulation 24 to ./sims/naive_100_23420_01462_run24.txt
01:46:06	estimating U using teacher 2 with beta 0.01
01:46:06	Estimated U: [-10.245975465020638, -2.0, 8.0]
01:46:06	True U: [3.0, 8.0, 8.0]
01:46:06	Estimated D: Any[[0.0, 0.5035799522673031, 0.4964200477326969], [0.4609756097560976, 0.5390243902439025, 0.0], [0.5026595744680851, 0.4973404255319149, 0.0]]
01:46:06	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:06	given U and D estimates, highest-reward arm is arm 1 with reward 4.964200477326969
01:46:06	logging naive policy simulation 25 to ./sims/naive_100_23420_01462_run25.txt
01:46:06	estimating U using teacher 2 with beta 0.01
01:46:06	Estimated U: [-9.284829999783579, -2.0, 8.0]
01:46:06	True U: [3.0, 8.0, 8.0]
01:46:06	Estimated D: Any[[0.0, 0.502283105022831, 0.4977168949771689], [0.45433255269320844, 0.5456674473067916, 0.0], [0.49370277078085645, 0.5062972292191436, 0.0]]
01:46:06	True D: Array{Float64}[[0.0, 0.5, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:46:06	given U and D estimates, highest-reward arm is arm 1 with reward 4.9771689497716896
01:46:06	Max R:		(avg 8000.0)	[8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0]
01:46:06	Random R:	(avg 3167.0)	[3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665, 3166.6666666666665]
01:46:06	Naive R:	(avg 7341.0)	[5278.5, 5318.5, 7533.5, 7536.5, 7499.0, 7503.0, 7524.5, 7533.5, 7523.5, 7531.5, 7547.5, 7514.5, 7512.0, 7453.0, 7526.5, 7456.5, 7541.5, 7544.5, 7517.5, 7541.0, 7513.0, 7476.5, 7531.0, 7518.0, 7561.0]
01:46:06	Normalized R:	(avg 0.92)	[0.6598125, 0.6648125, 0.9416875, 0.9420625, 0.937375, 0.937875, 0.9405625, 0.9416875, 0.9404375, 0.9414375, 0.9434375, 0.9393125, 0.939, 0.931625, 0.9408125, 0.9320625, 0.9426875, 0.9430625, 0.9396875, 0.942625, 0.939125, 0.9345625, 0.941375, 0.93975, 0.945125]
