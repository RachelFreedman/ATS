01:52:31	Running experiment with ID naive_200_23420_015231
01:52:31	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 1572
  t_explore: Int64 200
  teacher: Int64 2
  seed: Int64 1

01:52:31	will explore for first 200 timesteps
01:52:31	will estimate based on feedback from teacher 2 with beta 0.01
01:52:32	generated 27 utilities (each length 3 items)
01:52:32	generated 216 arm distribution sets (each shape 3 arms x 3 items)
01:52:32	generated 1 beta value sets (each length 3 teachers)
01:52:34	generated 5832000 states, 5832 of which are potential start states
01:52:34	generated 6 actions
01:52:34	generated reward function
01:52:34	generated 21 observations
01:52:34	generated observation function
01:52:34	true state State(1000, [8.0, 3.0, -2.0], Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]], [0.0, 0.01, 50.0])
01:52:34	logging naive policy simulation 1 to ./sims/naive_200_23420_015231_run1.txt
01:52:34	estimating U using teacher 2 with beta 0.01
01:52:34	Estimated U: [-2.0, -2.0, 8.0]
01:52:34	True U: [8.0, 3.0, -2.0]
01:52:34	Estimated D: Any[[0.45, 0.55, 0.0], [0.5, 0.0, 0.5], [0.0, 0.3939393939393939, 0.6060606060606061]]
01:52:34	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:34	given U and D estimates, highest-reward arm is arm 3 with reward 6.0606060606060606
01:52:34	logging naive policy simulation 2 to ./sims/naive_200_23420_015231_run2.txt
01:52:34	estimating U using teacher 2 with beta 0.01
01:52:34	Estimated U: [8.0, -9.581912670664986, -2.0]
01:52:34	True U: [8.0, 3.0, -2.0]
01:52:34	Estimated D: Any[[0.5, 0.5, 0.0], [0.45588235294117646, 0.0, 0.5441176470588235], [0.0, 0.4714285714285714, 0.5285714285714286]]
01:52:34	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:34	given U and D estimates, highest-reward arm is arm 2 with reward 4.5588235294117645
01:52:35	logging naive policy simulation 3 to ./sims/naive_200_23420_015231_run3.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -6.990719039811972, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.4918032786885246, 0.5081967213114754, 0.0], [0.5151515151515151, 0.0, 0.48484848484848486], [0.0, 0.49056603773584906, 0.5094339622641509]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 5.151515151515151
01:52:35	logging naive policy simulation 4 to ./sims/naive_200_23420_015231_run4.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -8.761991557248937, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.49635036496350365, 0.5036496350364964]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 5.0
01:52:35	logging naive policy simulation 5 to ./sims/naive_200_23420_015231_run5.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -6.142753880733841, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5, 0.5, 0.0], [0.48502994011976047, 0.0, 0.5149700598802395], [0.0, 0.5060240963855421, 0.4939759036144578]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.850299401197605
01:52:35	logging naive policy simulation 6 to ./sims/naive_200_23420_015231_run6.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -9.06356316848244, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5022026431718062, 0.4977973568281938, 0.0], [0.4816753926701571, 0.0, 0.518324607329843], [0.0, 0.4975609756097561, 0.5024390243902439]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.816753926701571
01:52:35	logging naive policy simulation 7 to ./sims/naive_200_23420_015231_run7.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -8.303410249200633, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.50187265917603, 0.49812734082397003, 0.0], [0.4669603524229075, 0.0, 0.5330396475770925], [0.0, 0.5043859649122807, 0.4956140350877193]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.669603524229075
01:52:35	logging naive policy simulation 8 to ./sims/naive_200_23420_015231_run8.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -9.557629800833787, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.4983388704318937, 0.5016611295681063, 0.0], [0.4674329501915709, 0.0, 0.5325670498084292], [0.0, 0.5135135135135135, 0.4864864864864865]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.674329501915709
01:52:35	logging naive policy simulation 9 to ./sims/naive_200_23420_015231_run9.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -4.273442591373591, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5060606060606061, 0.49393939393939396, 0.0], [0.47315436241610737, 0.0, 0.5268456375838926], [0.0, 0.503448275862069, 0.496551724137931]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.731543624161073
01:52:35	logging naive policy simulation 10 to ./sims/naive_200_23420_015231_run10.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -4.908369779229817, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5124653739612188, 0.48753462603878117, 0.0], [0.48493975903614456, 0.0, 0.5150602409638554], [0.0, 0.5061349693251533, 0.4938650306748466]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.849397590361446
01:52:35	logging naive policy simulation 11 to ./sims/naive_200_23420_015231_run11.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -4.12256094696167, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5062344139650873, 0.4937655860349127, 0.0], [0.4835164835164835, 0.0, 0.5164835164835165], [0.0, 0.49859943977591037, 0.5014005602240896]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.835164835164835
01:52:35	logging naive policy simulation 12 to ./sims/naive_200_23420_015231_run12.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -5.9933515831735775, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5160550458715596, 0.48394495412844035, 0.0], [0.4731457800511509, 0.0, 0.5268542199488491], [0.0, 0.493573264781491, 0.506426735218509]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.731457800511508
01:52:35	logging naive policy simulation 13 to ./sims/naive_200_23420_015231_run13.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -8.441144121640889, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5085106382978724, 0.49148936170212765, 0.0], [0.46462264150943394, 0.0, 0.535377358490566], [0.0, 0.49292452830188677, 0.5070754716981132]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.64622641509434
01:52:35	logging naive policy simulation 14 to ./sims/naive_200_23420_015231_run14.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -6.335068727161507, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5070707070707071, 0.49292929292929294, 0.0], [0.4593406593406593, 0.0, 0.5406593406593406], [0.0, 0.4934210526315789, 0.506578947368421]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.593406593406593
01:52:35	logging naive policy simulation 15 to ./sims/naive_200_23420_015231_run15.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -5.78331181318565, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5038167938931297, 0.4961832061068702, 0.0], [0.4626262626262626, 0.0, 0.5373737373737374], [0.0, 0.4989648033126294, 0.5010351966873706]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.626262626262626
01:52:35	logging naive policy simulation 16 to ./sims/naive_200_23420_015231_run16.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -7.969959203057153, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5062611806797853, 0.4937388193202147, 0.0], [0.4675572519083969, 0.0, 0.5324427480916031], [0.0, 0.501984126984127, 0.498015873015873]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.675572519083969
01:52:35	logging naive policy simulation 17 to ./sims/naive_200_23420_015231_run17.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -7.6334077662361555, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5092748735244519, 0.49072512647554806, 0.0], [0.4676258992805755, 0.0, 0.5323741007194245], [0.0, 0.507380073800738, 0.492619926199262]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.676258992805755
01:52:35	logging naive policy simulation 18 to ./sims/naive_200_23420_015231_run18.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -9.317467215336642, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5039494470774092, 0.4960505529225908, 0.0], [0.4672268907563025, 0.0, 0.5327731092436975], [0.0, 0.5035335689045937, 0.49646643109540634]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.6722689075630255
01:52:35	logging naive policy simulation 19 to ./sims/naive_200_23420_015231_run19.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -8.672506147505164, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5037369207772795, 0.4962630792227205, 0.0], [0.47310126582278483, 0.0, 0.5268987341772152], [0.0, 0.4983221476510067, 0.5016778523489933]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.731012658227848
01:52:35	logging naive policy simulation 20 to ./sims/naive_200_23420_015231_run20.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -8.705161829535967, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5133991537376587, 0.48660084626234135, 0.0], [0.4655688622754491, 0.0, 0.5344311377245509], [0.0, 0.4944, 0.5056]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.655688622754491
01:52:35	logging naive policy simulation 21 to ./sims/naive_200_23420_015231_run21.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [1.669859895820657, -2.0, 8.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5135869565217391, 0.48641304347826086, 0.0], [0.4738330975954738, 0.0, 0.5261669024045261], [0.0, 0.4923547400611621, 0.5076452599388379]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 7.000570106223366
01:52:35	logging naive policy simulation 22 to ./sims/naive_200_23420_015231_run22.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -7.545823601092308, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5090673575129534, 0.49093264248704666, 0.0], [0.47289972899729, 0.0, 0.5271002710027101], [0.0, 0.4963072378138848, 0.5036927621861153]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.7289972899729
01:52:35	logging naive policy simulation 23 to ./sims/naive_200_23420_015231_run23.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.0, -9.86277685183804, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5105590062111801, 0.4894409937888199, 0.0], [0.47600518806744485, 0.0, 0.5239948119325551], [0.0, 0.500697350069735, 0.499302649930265]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.760051880674449
01:52:35	logging naive policy simulation 24 to ./sims/naive_200_23420_015231_run24.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [8.000000000000002, -9.123637972725218, -2.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5148632580261593, 0.48513674197384066, 0.0], [0.4745341614906832, 0.0, 0.5254658385093167], [0.0, 0.4993288590604027, 0.5006711409395973]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 4.745341614906833
01:52:35	logging naive policy simulation 25 to ./sims/naive_200_23420_015231_run25.txt
01:52:35	estimating U using teacher 2 with beta 0.01
01:52:35	Estimated U: [4.616048270302191, -2.0, 8.0]
01:52:35	True U: [8.0, 3.0, -2.0]
01:52:35	Estimated D: Any[[0.5171232876712328, 0.4828767123287671, 0.0], [0.4738095238095238, 0.0, 0.5261904761904762], [0.0, 0.49165596919127086, 0.5083440308087291]]
01:52:35	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.5, 0.0, 0.5], [0.0, 0.5, 0.5]]
01:52:35	given U and D estimates, highest-reward arm is arm 2 with reward 8.396651442357467
01:52:35	Max R:		(avg 5500.0)	[5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0, 5500.0]
01:52:35	Random R:	(avg 1500.0)	[1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0, 1500.0]
01:52:35	Naive R:	(avg 2629.0)	[738.5, 2751.5, 2731.0, 2707.5, 2700.5, 2695.0, 2739.5, 2704.5, 2686.0, 2690.5, 2731.5, 2689.5, 2703.5, 2646.5, 2693.0, 2690.0, 2702.0, 2749.0, 2724.0, 2742.5, 2680.0, 2702.5, 2700.5, 2714.0, 2714.5]
01:52:35	Normalized R:	(avg 0.48)	[0.13427272727272727, 0.5002727272727273, 0.49654545454545457, 0.49227272727272725, 0.491, 0.49, 0.4980909090909091, 0.49172727272727274, 0.4883636363636364, 0.48918181818181816, 0.49663636363636365, 0.489, 0.49154545454545456, 0.48118181818181816, 0.48963636363636365, 0.4890909090909091, 0.49127272727272725, 0.49981818181818183, 0.49527272727272725, 0.49863636363636366, 0.48727272727272725, 0.4913636363636364, 0.491, 0.4934545454545455, 0.49354545454545456]
