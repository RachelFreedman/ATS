01:45:29	Running experiment with ID naive_100_23420_014529
01:45:30	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 1245
  t_explore: Int64 100
  teacher: Int64 2
  seed: Int64 1

01:45:30	will explore for first 100 timesteps
01:45:30	will estimate based on feedback from teacher 2 with beta 0.01
01:45:30	generated 27 utilities (each length 3 items)
01:45:30	generated 216 arm distribution sets (each shape 3 arms x 3 items)
01:45:30	generated 1 beta value sets (each length 3 teachers)
01:45:32	generated 5832000 states, 5832 of which are potential start states
01:45:32	generated 6 actions
01:45:32	generated reward function
01:45:32	generated 21 observations
01:45:32	generated observation function
01:45:32	true state State(1000, [8.0, -2.0, -2.0], Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]], [0.0, 0.01, 50.0])
01:45:32	logging naive policy simulation 1 to ./sims/naive_100_23420_014529_run1.txt
01:45:32	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [8.0, -5.293289359658068, -2.0]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.47058823529411764, 0.5294117647058824, 0.0], [0.0, 0.6111111111111112, 0.3888888888888889], [0.0, 0.23529411764705882, 0.7647058823529411]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 1 with reward 2.9623762213574936
01:45:33	logging naive policy simulation 2 to ./sims/naive_100_23420_014529_run2.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [8.0, -8.937190477897685, -2.0]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.5128205128205128, 0.48717948717948717, 0.0], [0.0, 0.45161290322580644, 0.5483870967741935], [0.0, 0.38461538461538464, 0.6153846153846154]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 1 with reward 1.748548228716512
01:45:33	logging naive policy simulation 3 to ./sims/naive_100_23420_014529_run3.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [8.0, -9.917444626245452, -2.0]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.5172413793103449, 0.4827586206896552, 0.0], [0.0, 0.5531914893617021, 0.44680851063829785], [0.0, 0.4107142857142857, 0.5892857142857143]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 1 with reward 1.3501991459504716
01:45:33	logging naive policy simulation 4 to ./sims/naive_100_23420_014529_run4.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [8.0, -11.675799747077447, -2.0]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.5131578947368421, 0.4868421052631579, 0.0], [0.0, 0.5074626865671642, 0.4925373134328358], [0.0, 0.43661971830985913, 0.5633802816901409]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 1 with reward 0.4209922283965063
01:45:33	logging naive policy simulation 5 to ./sims/naive_100_23420_014529_run5.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [8.0, -10.545011279465728, -2.0]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.5, 0.5, 0.0], [0.0, 0.46153846153846156, 0.5384615384615384], [0.0, 0.4691358024691358, 0.5308641975308642]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 1 with reward 0.7274943602671362
01:45:33	logging naive policy simulation 6 to ./sims/naive_100_23420_014529_run6.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [3.2808843523875506, -2.0, 8.0]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.509090909090909, 0.4909090909090909, 0.0], [0.0, 0.47058823529411764, 0.5294117647058824], [0.0, 0.4895833333333333, 0.5104166666666666]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 2 with reward 5.294117647058823
01:45:33	logging naive policy simulation 7 to ./sims/naive_100_23420_014529_run7.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [5.6251824315911065, -2.0, 8.0]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.5075757575757576, 0.49242424242424243, 0.0], [0.0, 0.4482758620689655, 0.5517241379310345], [0.0, 0.5229357798165137, 0.47706422018348627]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 2 with reward 5.517241379310345
01:45:33	logging naive policy simulation 8 to ./sims/naive_100_23420_014529_run8.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [4.539218079109481, -2.0, 8.0]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.4900662251655629, 0.5099337748344371, 0.0], [0.0, 0.45112781954887216, 0.5488721804511278], [0.0, 0.52, 0.48]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 2 with reward 5.488721804511279
01:45:33	logging naive policy simulation 9 to ./sims/naive_100_23420_014529_run9.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [8.0, -2.0, 5.422868882726087]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.5060240963855421, 0.4939759036144578, 0.0], [0.0, 0.4645161290322581, 0.535483870967742], [0.0, 0.5214285714285715, 0.4785714285714286]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 1 with reward 5.0602409638554215
01:45:33	logging naive policy simulation 10 to ./sims/naive_100_23420_014529_run10.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [8.0, -2.0, 5.88147831375022]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.521978021978022, 0.47802197802197804, 0.0], [0.0, 0.48255813953488375, 0.5174418604651163], [0.0, 0.50625, 0.49375]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 1 with reward 5.21978021978022
01:45:33	logging naive policy simulation 11 to ./sims/naive_100_23420_014529_run11.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [8.0, -2.0, 4.3301492498747285]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.525, 0.475, 0.0], [0.0, 0.4712041884816754, 0.5287958115183246], [0.0, 0.4887640449438202, 0.5112359550561798]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 1 with reward 5.25
01:45:33	logging naive policy simulation 12 to ./sims/naive_100_23420_014529_run12.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [8.0, -2.0, 6.028411082810798]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.536697247706422, 0.463302752293578, 0.0], [0.0, 0.45365853658536587, 0.5463414634146342], [0.0, 0.48205128205128206, 0.517948717948718]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 1 with reward 5.36697247706422
01:45:33	logging naive policy simulation 13 to ./sims/naive_100_23420_014529_run13.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [8.0, -2.0, 6.306009856258944]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.5148936170212766, 0.4851063829787234, 0.0], [0.0, 0.4434389140271493, 0.5565610859728507], [0.0, 0.4928909952606635, 0.5071090047393365]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 1 with reward 5.148936170212766
01:45:33	logging naive policy simulation 14 to ./sims/naive_100_23420_014529_run14.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [5.326516996748507, -2.0, 7.999999999999998]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.5040650406504065, 0.4959349593495935, 0.0], [0.0, 0.4411764705882353, 0.5588235294117647], [0.0, 0.49107142857142855, 0.5089285714285714]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 2 with reward 5.588235294117646
01:45:33	logging naive policy simulation 15 to ./sims/naive_100_23420_014529_run15.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [8.0, -2.0, 7.444508278437784]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.5038461538461538, 0.49615384615384617, 0.0], [0.0, 0.4362934362934363, 0.5637065637065637], [0.0, 0.49586776859504134, 0.5041322314049587]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 2 with reward 5.323931307536356
01:45:33	logging naive policy simulation 16 to ./sims/naive_100_23420_014529_run16.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [8.0, -2.0, 6.520916596587284]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.5053763440860215, 0.4946236559139785, 0.0], [0.0, 0.44981412639405205, 0.550185873605948], [0.0, 0.4940239043824701, 0.5059760956175299]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 1 with reward 5.053763440860215
01:45:33	logging naive policy simulation 17 to ./sims/naive_100_23420_014529_run17.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [6.808986898924838, -2.0, 7.999999999999998]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.5117056856187291, 0.4882943143812709, 0.0], [0.0, 0.4574468085106383, 0.5425531914893617], [0.0, 0.5055350553505535, 0.4944649446494465]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 2 with reward 5.425531914893615
01:45:33	logging naive policy simulation 18 to ./sims/naive_100_23420_014529_run18.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [5.511022912186409, -2.0, 8.0]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.5062893081761006, 0.4937106918238994, 0.0], [0.0, 0.4542483660130719, 0.545751633986928], [0.0, 0.5106382978723404, 0.48936170212765956]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 2 with reward 5.45751633986928
01:45:33	logging naive policy simulation 19 to ./sims/naive_100_23420_014529_run19.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [3.9431498951475143, -2.0, 8.0]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.4955223880597015, 0.5044776119402985, 0.0], [0.0, 0.46153846153846156, 0.5384615384615384], [0.0, 0.5033783783783784, 0.4966216216216216]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 2 with reward 5.384615384615384
01:45:33	logging naive policy simulation 20 to ./sims/naive_100_23420_014529_run20.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [4.7613001160372175, -2.0, 8.0]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.5070028011204482, 0.49299719887955185, 0.0], [0.0, 0.4574780058651026, 0.5425219941348973], [0.0, 0.5064516129032258, 0.4935483870967742]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 2 with reward 5.425219941348973
01:45:33	logging naive policy simulation 21 to ./sims/naive_100_23420_014529_run21.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [3.150490255732578, -2.0, 8.0]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.5054054054054054, 0.4945945945945946, 0.0], [0.0, 0.4638888888888889, 0.5361111111111111], [0.0, 0.5015197568389058, 0.49848024316109424]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 2 with reward 5.361111111111111
01:45:33	logging naive policy simulation 22 to ./sims/naive_100_23420_014529_run22.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [2.347047403396842, -2.0, 7.999999999999998]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.49740932642487046, 0.5025906735751295, 0.0], [0.0, 0.46296296296296297, 0.5370370370370371], [0.0, 0.5029585798816568, 0.4970414201183432]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 2 with reward 5.37037037037037
01:45:33	logging naive policy simulation 23 to ./sims/naive_100_23420_014529_run23.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [1.3962755120115138, -2.0, 8.0]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.4975247524752475, 0.5024752475247525, 0.0], [0.0, 0.46683673469387754, 0.5331632653061225], [0.0, 0.5111731843575419, 0.4888268156424581]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 2 with reward 5.331632653061225
01:45:33	logging naive policy simulation 24 to ./sims/naive_100_23420_014529_run24.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [1.842170827240984, -2.0, 8.0]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.5035799522673031, 0.4964200477326969, 0.0], [0.0, 0.4609756097560976, 0.5390243902439025], [0.0, 0.5026595744680851, 0.4973404255319149]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 2 with reward 5.390243902439025
01:45:33	logging naive policy simulation 25 to ./sims/naive_100_23420_014529_run25.txt
01:45:33	estimating U using teacher 2 with beta 0.01
01:45:33	Estimated U: [3.039709923673069, -2.0, 8.0]
01:45:33	True U: [8.0, -2.0, -2.0]
01:45:33	Estimated D: Any[[0.502283105022831, 0.4977168949771689, 0.0], [0.0, 0.45433255269320844, 0.5456674473067916], [0.0, 0.49370277078085645, 0.5062972292191436]]
01:45:33	True D: Array{Float64}[[0.5, 0.5, 0.0], [0.0, 0.5, 0.5], [0.0, 0.5, 0.5]]
01:45:33	given U and D estimates, highest-reward arm is arm 2 with reward 5.456674473067916
01:45:34	Max R:		(avg 3000.0)	[3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0, 3000.0]
01:45:34	Random R:	(avg -167.0)	[-166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666, -166.66666666666666]
01:45:34	Naive R:	(avg 167.0)	[2681.0, 2696.0, 2691.0, 2684.0, 2674.0, -1792.0, -1788.0, -1809.0, 2671.0, 2674.0, 2680.0, 2692.0, 2687.0, -1827.0, -1836.0, 2719.0, -1806.0, -1813.0, -1815.0, -1794.0, -1837.0, -1806.0, -1814.0, -1827.0, -1819.0]
01:45:34	Normalized R:	(avg 0.06)	[0.8936666666666667, 0.8986666666666666, 0.897, 0.8946666666666667, 0.8913333333333333, -0.5973333333333334, -0.596, -0.603, 0.8903333333333333, 0.8913333333333333, 0.8933333333333333, 0.8973333333333333, 0.8956666666666667, -0.609, -0.612, 0.9063333333333333, -0.602, -0.6043333333333333, -0.605, -0.598, -0.6123333333333333, -0.602, -0.6046666666666667, -0.609, -0.6063333333333333]
