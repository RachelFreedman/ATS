01:41:07	Running experiment with ID naive_50_23420_01417
01:41:07	MyParameters
  N: Int64 3
  K: Int64 3
  M: Int64 3
  umax: Int64 10
  u_grain: Int64 3
  d_grain: Int64 3
  beta: Array{Float64}((3,)) [0.0, 0.01, 50.0]
  exp_iters: Int64 25
  exp_steps: Int64 1000
  s_index: Int64 4638
  t_explore: Int64 50
  teacher: Int64 2
  seed: Int64 1

01:41:07	will explore for first 50 timesteps
01:41:07	will estimate based on feedback from teacher 2 with beta 0.01
01:41:07	generated 27 utilities (each length 3 items)
01:41:08	generated 216 arm distribution sets (each shape 3 arms x 3 items)
01:41:08	generated 1 beta value sets (each length 3 teachers)
01:41:09	generated 5832000 states, 5832 of which are potential start states
01:41:09	generated 6 actions
01:41:09	generated reward function
01:41:09	generated 21 observations
01:41:09	generated observation function
01:41:10	true state State(1000, [8.0, -2.0, 8.0], Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]], [0.0, 0.01, 50.0])
01:41:10	logging naive policy simulation 1 to ./sims/naive_50_23420_01417_run1.txt
01:41:10	estimating U using teacher 2 with beta 0.01
01:41:10	Estimated U: [8.0, -2.0, -4.6303440583379345]
01:41:10	True U: [8.0, -2.0, 8.0]
01:41:10	Estimated D: Any[[0.5, 0.0, 0.5], [0.5714285714285714, 0.42857142857142855, 0.0], [0.1111111111111111, 0.8888888888888888, 0.0]]
01:41:10	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:10	given U and D estimates, highest-reward arm is arm 2 with reward 5.7142857142857135
01:41:10	logging naive policy simulation 2 to ./sims/naive_50_23420_01417_run2.txt
01:41:10	estimating U using teacher 2 with beta 0.01
01:41:10	Estimated U: [8.0, -5.547556456757271, -2.0]
01:41:10	True U: [8.0, -2.0, 8.0]
01:41:10	Estimated D: Any[[0.5294117647058824, 0.0, 0.47058823529411764], [0.42857142857142855, 0.5714285714285714, 0.0], [0.1875, 0.8125, 0.0]]
01:41:10	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:10	given U and D estimates, highest-reward arm is arm 1 with reward 5.294117647058823
01:41:10	logging naive policy simulation 3 to ./sims/naive_50_23420_01417_run3.txt
01:41:10	estimating U using teacher 2 with beta 0.01
01:41:10	Estimated U: [7.596029104518028, -2.0, 8.0]
01:41:10	True U: [8.0, -2.0, 8.0]
01:41:10	Estimated D: Any[[0.5, 0.0, 0.5], [0.5217391304347826, 0.4782608695652174, 0.0], [0.3076923076923077, 0.6923076923076923, 0.0]]
01:41:10	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:10	given U and D estimates, highest-reward arm is arm 1 with reward 9.798014552259014
01:41:10	logging naive policy simulation 4 to ./sims/naive_50_23420_01417_run4.txt
01:41:10	estimating U using teacher 2 with beta 0.01
01:41:10	Estimated U: [8.0, -2.0, 4.466756827670869]
01:41:10	True U: [8.0, -2.0, 8.0]
01:41:10	Estimated D: Any[[0.5161290322580645, 0.0, 0.4838709677419355], [0.45714285714285713, 0.5428571428571428, 0.0], [0.3333333333333333, 0.6666666666666666, 0.0]]
01:41:10	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:10	given U and D estimates, highest-reward arm is arm 1 with reward 8.290366206937517
01:41:10	logging naive policy simulation 5 to ./sims/naive_50_23420_01417_run5.txt
01:41:10	estimating U using teacher 2 with beta 0.01
01:41:10	Estimated U: [8.0, -2.0, -0.5920506926192135]
01:41:10	True U: [8.0, -2.0, 8.0]
01:41:10	Estimated D: Any[[0.5142857142857142, 0.0, 0.4857142857142857], [0.42, 0.58, 0.0], [0.40540540540540543, 0.5945945945945946, 0.0]]
01:41:10	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:10	given U and D estimates, highest-reward arm is arm 1 with reward 5.826718235013525
01:41:10	logging naive policy simulation 6 to ./sims/naive_50_23420_01417_run6.txt
01:41:10	estimating U using teacher 2 with beta 0.01
01:41:10	Estimated U: [6.919148737617363, -2.0, 8.0]
01:41:10	True U: [8.0, -2.0, 8.0]
01:41:10	Estimated D: Any[[0.5227272727272727, 0.0, 0.4772727272727273], [0.4642857142857143, 0.5357142857142857, 0.0], [0.4666666666666667, 0.5333333333333333, 0.0]]
01:41:10	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:10	given U and D estimates, highest-reward arm is arm 1 with reward 9.435009567390894
01:41:10	logging naive policy simulation 7 to ./sims/naive_50_23420_01417_run7.txt
01:41:10	estimating U using teacher 2 with beta 0.01
01:41:10	Estimated U: [7.625083839464722, -2.0, 8.0]
01:41:10	True U: [8.0, -2.0, 8.0]
01:41:10	Estimated D: Any[[0.49122807017543857, 0.0, 0.5087719298245614], [0.4426229508196721, 0.5573770491803278, 0.0], [0.5294117647058824, 0.47058823529411764, 0.0]]
01:41:10	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:10	given U and D estimates, highest-reward arm is arm 1 with reward 9.815830657982671
01:41:10	logging naive policy simulation 8 to ./sims/naive_50_23420_01417_run8.txt
01:41:10	estimating U using teacher 2 with beta 0.01
01:41:10	Estimated U: [7.241753190991284, -2.0, 8.0]
01:41:10	True U: [8.0, -2.0, 8.0]
01:41:10	Estimated D: Any[[0.4696969696969697, 0.0, 0.5303030303030303], [0.43661971830985913, 0.5633802816901409, 0.0], [0.5084745762711864, 0.4915254237288136, 0.0]]
01:41:10	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:10	given U and D estimates, highest-reward arm is arm 1 with reward 9.64385377152621
01:41:10	logging naive policy simulation 9 to ./sims/naive_50_23420_01417_run9.txt
01:41:10	estimating U using teacher 2 with beta 0.01
01:41:10	Estimated U: [8.0, -2.0, 6.526409809555755]
01:41:10	True U: [8.0, -2.0, 8.0]
01:41:10	Estimated D: Any[[0.5068493150684932, 0.0, 0.4931506849315068], [0.44047619047619047, 0.5595238095238095, 0.0], [0.5151515151515151, 0.48484848484848486, 0.0]]
01:41:10	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:10	given U and D estimates, highest-reward arm is arm 1 with reward 9.27329798827407
01:41:10	logging naive policy simulation 10 to ./sims/naive_50_23420_01417_run10.txt
01:41:10	estimating U using teacher 2 with beta 0.01
01:41:10	Estimated U: [5.943811190627975, -2.0, 8.0]
01:41:10	True U: [8.0, -2.0, 8.0]
01:41:10	Estimated D: Any[[0.5119047619047619, 0.0, 0.4880952380952381], [0.4444444444444444, 0.5555555555555556, 0.0], [0.52, 0.48, 0.0]]
01:41:10	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:10	given U and D estimates, highest-reward arm is arm 1 with reward 8.947427157107176
01:41:10	logging naive policy simulation 11 to ./sims/naive_50_23420_01417_run11.txt
01:41:10	estimating U using teacher 2 with beta 0.01
01:41:10	Estimated U: [4.616264045346701, -2.0, 8.0]
01:41:10	True U: [8.0, -2.0, 8.0]
01:41:10	Estimated D: Any[[0.5319148936170213, 0.0, 0.46808510638297873], [0.42857142857142855, 0.5714285714285714, 0.0], [0.4883720930232558, 0.5116279069767442, 0.0]]
01:41:10	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:10	given U and D estimates, highest-reward arm is arm 1 with reward 8.200140449652501
01:41:10	logging naive policy simulation 12 to ./sims/naive_50_23420_01417_run12.txt
01:41:10	estimating U using teacher 2 with beta 0.01
01:41:10	Estimated U: [3.5825581003455467, -2.0, 8.0]
01:41:10	True U: [8.0, -2.0, 8.0]
01:41:10	Estimated D: Any[[0.5490196078431373, 0.0, 0.45098039215686275], [0.4056603773584906, 0.5943396226415094, 0.0], [0.4731182795698925, 0.5268817204301075, 0.0]]
01:41:10	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:10	given U and D estimates, highest-reward arm is arm 1 with reward 7.574737780581869
01:41:11	logging naive policy simulation 13 to ./sims/naive_50_23420_01417_run13.txt
01:41:11	estimating U using teacher 2 with beta 0.01
01:41:11	Estimated U: [6.534298106863208, -2.0, 8.0]
01:41:11	True U: [8.0, -2.0, 8.0]
01:41:11	Estimated D: Any[[0.5225225225225225, 0.0, 0.4774774774774775], [0.3793103448275862, 0.6206896551724138, 0.0], [0.4854368932038835, 0.5145631067961165, 0.0]]
01:41:11	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:11	given U and D estimates, highest-reward arm is arm 1 with reward 9.234137749532128
01:41:11	logging naive policy simulation 14 to ./sims/naive_50_23420_01417_run14.txt
01:41:11	estimating U using teacher 2 with beta 0.01
01:41:11	Estimated U: [3.6409728173766123, -2.0, 8.0]
01:41:11	True U: [8.0, -2.0, 8.0]
01:41:11	Estimated D: Any[[0.5130434782608696, 0.0, 0.48695652173913045], [0.3790322580645161, 0.6209677419354839, 0.0], [0.4954128440366973, 0.5045871559633027, 0.0]]
01:41:11	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:11	given U and D estimates, highest-reward arm is arm 1 with reward 7.763629532393219
01:41:11	logging naive policy simulation 15 to ./sims/naive_50_23420_01417_run15.txt
01:41:11	estimating U using teacher 2 with beta 0.01
01:41:11	Estimated U: [3.9509434391798894, -2.0, 8.0]
01:41:11	True U: [8.0, -2.0, 8.0]
01:41:11	Estimated D: Any[[0.5161290322580645, 0.0, 0.4838709677419355], [0.3897058823529412, 0.6102941176470589, 0.0], [0.49586776859504134, 0.5041322314049587, 0.0]]
01:41:11	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:11	given U and D estimates, highest-reward arm is arm 1 with reward 7.910164355705749
01:41:11	logging naive policy simulation 16 to ./sims/naive_50_23420_01417_run16.txt
01:41:11	estimating U using teacher 2 with beta 0.01
01:41:11	Estimated U: [3.725213397675372, -2.0, 8.0]
01:41:11	True U: [8.0, -2.0, 8.0]
01:41:11	Estimated D: Any[[0.5147058823529411, 0.0, 0.4852941176470588], [0.4084507042253521, 0.5915492957746479, 0.0], [0.4921875, 0.5078125, 0.0]]
01:41:11	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:11	given U and D estimates, highest-reward arm is arm 1 with reward 7.799742189979971
01:41:11	logging naive policy simulation 17 to ./sims/naive_50_23420_01417_run17.txt
01:41:11	estimating U using teacher 2 with beta 0.01
01:41:11	Estimated U: [2.258864591592986, -2.0, 8.0]
01:41:11	True U: [8.0, -2.0, 8.0]
01:41:11	Estimated D: Any[[0.5244755244755245, 0.0, 0.4755244755244755], [0.4217687074829932, 0.5782312925170068, 0.0], [0.5104895104895105, 0.48951048951048953, 0.0]]
01:41:11	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:11	given U and D estimates, highest-reward arm is arm 1 with reward 6.988914995590727
01:41:11	logging naive policy simulation 18 to ./sims/naive_50_23420_01417_run18.txt
01:41:11	estimating U using teacher 2 with beta 0.01
01:41:11	Estimated U: [2.077635732779358, -2.0, 8.0]
01:41:11	True U: [8.0, -2.0, 8.0]
01:41:11	Estimated D: Any[[0.5194805194805194, 0.0, 0.4805194805194805], [0.41875, 0.58125, 0.0], [0.5102040816326531, 0.4897959183673469, 0.0]]
01:41:11	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:11	given U and D estimates, highest-reward arm is arm 1 with reward 6.923447133911354
01:41:11	logging naive policy simulation 19 to ./sims/naive_50_23420_01417_run19.txt
01:41:11	estimating U using teacher 2 with beta 0.01
01:41:11	Estimated U: [0.4591295040078722, -2.0, 8.0]
01:41:11	True U: [8.0, -2.0, 8.0]
01:41:11	Estimated D: Any[[0.5093167701863354, 0.0, 0.4906832298136646], [0.4294117647058823, 0.5705882352941176, 0.0], [0.506578947368421, 0.4934210526315789, 0.0]]
01:41:11	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:11	given U and D estimates, highest-reward arm is arm 1 with reward 6.15930819458786
01:41:11	logging naive policy simulation 20 to ./sims/naive_50_23420_01417_run20.txt
01:41:11	estimating U using teacher 2 with beta 0.01
01:41:11	Estimated U: [1.756731479155972, -2.0, 8.0]
01:41:11	True U: [8.0, -2.0, 8.0]
01:41:11	Estimated D: Any[[0.5146198830409356, 0.0, 0.4853801169590643], [0.423728813559322, 0.576271186440678, 0.0], [0.5094339622641509, 0.49056603773584906, 0.0]]
01:41:11	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:11	given U and D estimates, highest-reward arm is arm 1 with reward 6.787089884010091
01:41:11	logging naive policy simulation 21 to ./sims/naive_50_23420_01417_run21.txt
01:41:11	estimating U using teacher 2 with beta 0.01
01:41:11	Estimated U: [0.6885272221521168, -2.0, 8.0]
01:41:11	True U: [8.0, -2.0, 8.0]
01:41:11	Estimated D: Any[[0.5083798882681564, 0.0, 0.49162011173184356], [0.44021739130434784, 0.5597826086956522, 0.0], [0.5117647058823529, 0.48823529411764705, 0.0]]
01:41:11	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:11	given U and D estimates, highest-reward arm is arm 1 with reward 6.282994286122025
01:41:11	logging naive policy simulation 22 to ./sims/naive_50_23420_01417_run22.txt
01:41:11	estimating U using teacher 2 with beta 0.01
01:41:11	Estimated U: [0.040098079165438616, -2.0, 8.0]
01:41:11	True U: [8.0, -2.0, 8.0]
01:41:11	Estimated D: Any[[0.4973544973544973, 0.0, 0.5026455026455027], [0.44559585492227977, 0.5544041450777202, 0.0], [0.5229885057471264, 0.47701149425287354, 0.0]]
01:41:11	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:11	given U and D estimates, highest-reward arm is arm 1 with reward 6.041106981172229
01:41:11	logging naive policy simulation 23 to ./sims/naive_50_23420_01417_run23.txt
01:41:11	estimating U using teacher 2 with beta 0.01
01:41:11	Estimated U: [-0.1902606650516414, -2.0, 8.000000000000002]
01:41:11	True U: [8.0, -2.0, 8.0]
01:41:11	Estimated D: Any[[0.49246231155778897, 0.0, 0.507537688442211], [0.4504950495049505, 0.5495049504950495, 0.0], [0.5251396648044693, 0.4748603351955307, 0.0]]
01:41:11	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:11	given U and D estimates, highest-reward arm is arm 1 with reward 5.966605300627836
01:41:11	logging naive policy simulation 24 to ./sims/naive_50_23420_01417_run24.txt
01:41:11	estimating U using teacher 2 with beta 0.01
01:41:11	Estimated U: [0.8618629151356196, -2.0, 8.0]
01:41:11	True U: [8.0, -2.0, 8.0]
01:41:11	Estimated D: Any[[0.5096153846153846, 0.0, 0.49038461538461536], [0.44549763033175355, 0.5545023696682464, 0.0], [0.5185185185185185, 0.48148148148148145, 0.0]]
01:41:11	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:11	given U and D estimates, highest-reward arm is arm 1 with reward 6.3622955240594985
01:41:11	logging naive policy simulation 25 to ./sims/naive_50_23420_01417_run25.txt
01:41:11	estimating U using teacher 2 with beta 0.01
01:41:11	Estimated U: [0.9447759102737723, -2.0, 8.0]
01:41:11	True U: [8.0, -2.0, 8.0]
01:41:11	Estimated D: Any[[0.509090909090909, 0.0, 0.4909090909090909], [0.4429223744292237, 0.5570776255707762, 0.0], [0.5124378109452736, 0.48756218905472637, 0.0]]
01:41:11	True D: Array{Float64}[[0.5, 0.0, 0.5], [0.5, 0.5, 0.0], [0.5, 0.5, 0.0]]
01:41:11	given U and D estimates, highest-reward arm is arm 1 with reward 6.408249554321193
01:41:11	Max R:		(avg 8000.0)	[8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0, 8000.0]
01:41:11	Random R:	(avg 2333.0)	[2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335, 2333.3333333333335]
01:41:11	Naive R:	(avg 7531.0)	[2946.0, 7730.0, 7697.0, 7729.0, 7689.0, 7714.0, 7737.0, 7726.0, 7716.0, 7733.0, 7737.0, 7709.0, 7732.0, 7674.0, 7744.0, 7735.0, 7716.0, 7739.0, 7701.0, 7722.0, 7718.0, 7719.0, 7722.0, 7729.0, 7756.0]
01:41:11	Normalized R:	(avg 0.94)	[0.36825, 0.96625, 0.962125, 0.966125, 0.961125, 0.96425, 0.967125, 0.96575, 0.9645, 0.966625, 0.967125, 0.963625, 0.9665, 0.95925, 0.968, 0.966875, 0.9645, 0.967375, 0.962625, 0.96525, 0.96475, 0.964875, 0.96525, 0.966125, 0.9695]
